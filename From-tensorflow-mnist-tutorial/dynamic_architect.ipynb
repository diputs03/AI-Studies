{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diputs03/AI-Studies/blob/main/From-tensorflow-mnist-tutorial/dynamic_architect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DynamicNet(nn.Module):\n",
        "    def __init__(self, add_intermediate=lambda: False):\n",
        "        super(DynamicNet, self).__init__()\n",
        "        self.add_intermediate = add_intermediate\n",
        "        # Common input layer: from 784 to 256 neurons\n",
        "        self.input_layer = nn.Linear(784, 256)\n",
        "\n",
        "        # Optional intermediate layer: from 256 to 256 neurons\n",
        "        if self.add_intermediate:\n",
        "            self.intermediate_layer = nn.Linear(256, 256)\n",
        "\n",
        "        # Output layer: from 256 to 10 neurons\n",
        "        self.output_layer = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten input if necessary (assuming x is [batch_size, 28, 28])\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.input_layer(x))\n",
        "        if self.add_intermediate:\n",
        "            x = F.relu(self.intermediate_layer(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Condition: whether to add an intermediate layer\n",
        "def insert_extra_layer():\n",
        "  return True  # Change this flag as needed\n",
        "\n",
        "# Create the model with the desired architecture\n",
        "model = DynamicNet(add_intermediate=insert_extra_layer)\n",
        "\n",
        "# Print model structure\n",
        "print(model)\n",
        "\n",
        "# Dummy input for testing (e.g., a batch of 64 MNIST images)\n",
        "dummy_input = torch.randn(64, 1, 28, 28)\n",
        "output = model(dummy_input)\n",
        "print(\"Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPWHQr-iMfDw",
        "outputId": "bbeb2475-c248-45d8-951b-b96dc08e08ca"
      },
      "id": "kPWHQr-iMfDw",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DynamicNet(\n",
            "  (input_layer): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (intermediate_layer): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (output_layer): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n",
            "Output shape: torch.Size([64, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FlexibleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FlexibleNet, self).__init__()\n",
        "        self.layers = nn.ModuleList([nn.Linear(784, 256), nn.Linear(256, 10)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # Dynamically use each layer\n",
        "        for layer in self.layers:\n",
        "            x = F.relu(layer(x))\n",
        "        return x\n",
        "\n",
        "    def insert_layer(self, index, layer):\n",
        "        self.layers.insert(index, layer)\n",
        "\n",
        "# Example: Inserting a new layer between the first and second layers\n",
        "net = FlexibleNet()\n",
        "print(\"Before inserting:\", net)\n",
        "net.insert_layer(1, nn.Linear(256, 256))\n",
        "print(\"After inserting:\", net)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAmHTciVN6nE",
        "outputId": "dc3d37e0-092f-4740-daab-67c1c1136139"
      },
      "id": "EAmHTciVN6nE",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before inserting: FlexibleNet(\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "    (1): Linear(in_features=256, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "After inserting: FlexibleNet(\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
            "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MaskedLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(MaskedLinear, self).__init__()\n",
        "        # Standard linear layer parameters.\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "        # Create a mask of ones (initially, all connections are enabled)\n",
        "        self.register_buffer('mask', torch.ones(out_features, in_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the mask to the weight matrix.\n",
        "        masked_weight = self.linear.weight * self.mask\n",
        "        # Use the masked weight in the linear operation.\n",
        "        return F.linear(x, masked_weight, self.linear.bias)\n",
        "\n",
        "    def update_mask(self, new_mask):\n",
        "        \"\"\"\n",
        "        Update the connectivity mask.\n",
        "\n",
        "        Args:\n",
        "            new_mask (torch.Tensor): A tensor with shape (out_features, in_features)\n",
        "                                     containing 0s and 1s.\n",
        "        \"\"\"\n",
        "        self.mask.copy_(new_mask)\n",
        "\n",
        "# Example usage:\n",
        "# Create a masked linear layer for a 784->10 mapping.\n",
        "masked_layer = MaskedLinear(784, 10)\n",
        "\n",
        "# Suppose you want to disable the connection from input neuron 5 to output neuron 3:\n",
        "# Get the current mask, update the specified entry, and then update the mask.\n",
        "current_mask = masked_layer.mask.clone()\n",
        "current_mask[3, 5] = 0  # 0 disables the connection.\n",
        "masked_layer.update_mask(current_mask)\n",
        "\n",
        "# Now, in a model, you can use MaskedLinear in place of nn.Linear.\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.fc = MaskedLinear(784, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "# Dummy data for demonstration (e.g., one MNIST image flattened)\n",
        "dummy_input = torch.randn(1, 1, 28, 28)\n",
        "model = CustomModel()\n",
        "print(model)\n",
        "output = model(dummy_input)\n",
        "print(\"Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cblm52-5EYk",
        "outputId": "2bb5b94f-6cf5-4356-91cb-50aff0d036df"
      },
      "id": "0cblm52-5EYk",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomModel(\n",
            "  (fc): MaskedLinear(\n",
            "    (linear): Linear(in_features=784, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Output: tensor([[-0.2138, -0.5612, -0.0726,  0.0615, -0.9300,  0.4176,  0.3547, -0.2046,\n",
            "          0.5302,  0.1620]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ExtraNeuronModel(nn.Module):\n",
        "    def __init__(self, add_extra=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            add_extra (bool): Whether to include an extra neuron in the network.\n",
        "        \"\"\"\n",
        "        super(ExtraNeuronModel, self).__init__()\n",
        "        self.add_extra = add_extra\n",
        "\n",
        "        # Base pathway: from 784 inputs to 10 outputs.\n",
        "        self.base_layer = nn.Linear(784, 10)\n",
        "\n",
        "        if self.add_extra:\n",
        "            # Extra neuron computed from the same input.\n",
        "            self.extra_neuron = nn.Linear(784, 1)\n",
        "            # A combination layer that merges the original 10 outputs with the extra neuron.\n",
        "            # It takes 10 + 1 = 11 features and produces 10 outputs.\n",
        "            self.combination_layer = nn.Linear(11, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input (assuming x is [batch_size, 28, 28])\n",
        "        x_flat = x.view(x.size(0), -1)\n",
        "        base_out = self.base_layer(x_flat)  # shape: [batch_size, 10]\n",
        "\n",
        "        if self.add_extra:\n",
        "            # Compute extra neuron output\n",
        "            extra_out = self.extra_neuron(x_flat)  # shape: [batch_size, 1]\n",
        "            # Concatenate the base outputs with the extra neuron output\n",
        "            combined = torch.cat([base_out, extra_out], dim=1)  # shape: [batch_size, 11]\n",
        "            # Produce final output via the combination layer\n",
        "            final_out = self.combination_layer(combined)\n",
        "            return final_out\n",
        "        else:\n",
        "            return base_out\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Create input data (e.g., a batch of 64 MNIST images of size 28x28)\n",
        "dummy_input = torch.randn(64, 1, 28, 28)\n",
        "\n",
        "# Model without the extra neuron (standard 784 -> 10)\n",
        "model_standard = ExtraNeuronModel(add_extra=False)\n",
        "output_standard = model_standard(dummy_input)\n",
        "print(\"Output shape (without extra neuron):\", output_standard.shape)\n",
        "\n",
        "# Model with an extra neuron added\n",
        "model_extra = ExtraNeuronModel(add_extra=True)\n",
        "output_extra = model_extra(dummy_input)\n",
        "print(\"Output shape (with extra neuron):\", output_extra.shape)\n",
        "print(model_extra)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ycil2O25t7m",
        "outputId": "13326178-7f5d-4d59-b3b3-bc04233a50f9"
      },
      "id": "3Ycil2O25t7m",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape (without extra neuron): torch.Size([64, 10])\n",
            "Output shape (with extra neuron): torch.Size([64, 10])\n",
            "ExtraNeuronModel(\n",
            "  (base_layer): Linear(in_features=784, out_features=10, bias=True)\n",
            "  (extra_neuron): Linear(in_features=784, out_features=1, bias=True)\n",
            "  (combination_layer): Linear(in_features=11, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Here is mine\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "u5E0U3bi-Uke"
      },
      "id": "u5E0U3bi-Uke",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "  def relu(params, x):\n",
        "    assert(len(params)!=2)\n",
        "    if x>=0: return params[0]+params[1]*x\n",
        "    else: return params[0]\n",
        "  def sigmoid(params, x):\n",
        "    return 1/(1+np.e**(-params[0]-params[1]*x))\n",
        "  def linear(params, x):\n",
        "    assert(len(params)!=2)\n",
        "    return params[0]+params[1]*x\n",
        "  def polynomial(params, x):\n",
        "    ret=0\n",
        "    for i in range(len(params)):\n",
        "      ret+=params[i]*(x**i)\n",
        "    return ret\n",
        "\n",
        "\n",
        "\n",
        "  params=[0,0]\n",
        "  next=[]\n",
        "  name=''\n",
        "  def __init__(self, name, next):\n",
        "    self.name=name\n",
        "    self.next=next\n",
        "  def forward(self, next):\n",
        "    self.next+=next\n",
        "  def __str__(self):\n",
        "    ret=str(self.name)+'('\n",
        "    for n in self.next:\n",
        "      ret+=str(n)\n",
        "    return ret+')'"
      ],
      "metadata": {
        "id": "3UzBHSou6ikc"
      },
      "id": "3UzBHSou6ikc",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=Neuron(name='a', next=[])\n",
        "b=Neuron(name='b', next=[])\n",
        "c=Neuron(name='c', next=[])\n",
        "a.forward([b])\n",
        "a.forward([c])\n",
        "for n in b.next:\n",
        "  print(n)\n",
        "print(a)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jnT-Gk2S0DJ",
        "outputId": "cf02277c-c454-4b34-e6ab-c9eab92cfa70"
      },
      "id": "1jnT-Gk2S0DJ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a(b()c())\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}