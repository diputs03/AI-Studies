{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diputs03/AI-Studies/blob/main/Creating_network/test_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Aiming a Dynaimic Graph-structured NeuronNetwork\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import deque\n",
        "import sys\n",
        "import time\n",
        "\n",
        "def progressbar(it, prefix=\"\", size=60, out=sys.stdout): # Python3.6+\n",
        "    count = len(it)\n",
        "    start = time.time() # time estimate start\n",
        "    def show(j):\n",
        "        x = int(size*j/count)\n",
        "        # time estimate calculation and string\n",
        "        remaining = ((time.time() - start) / j) * (count - j)\n",
        "        mins, sec = divmod(remaining, 60) # limited to minutes\n",
        "        time_str = f\"{int(mins):02}:{sec:03.1f}\"\n",
        "        print(f\"{prefix}[{u'█'*x}{('.'*(size-x))}] {j}/{count} Est wait {time_str}\", end='\\r', file=out, flush=True)\n",
        "    show(0.1) # avoid div/0\n",
        "    for i, item in enumerate(it):\n",
        "        yield item\n",
        "        show(i+1)\n",
        "    print(\"\\n\", flush=True, file=out)\n",
        "\n",
        "\"\"\"\n",
        "Activation function in this case in \\tanh, thus\n",
        "\\dfrac{d\\tanh(x)}{dx}=1-\\tanh^2(x)\n",
        "however, for other activation funtions\n",
        "\\dfrac{d\\sigma(x)}{dx}=\\sigma(x)\\cdot\\left\\big(1-\\sigma(x)\\right\\big)\n",
        "\\dfrac{d\\mathop{\\mathrm{ReLu}}(x)}{dx}=\\begin{cases}1&x\\ge0\\\\0&\\text{else}\\end{cases}\n",
        "Loss is the Euclidean loss\n",
        "\\dfrac{d\\L}\n",
        "\"\"\"\n",
        "\n",
        "class Model:\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.idcnt = 0\n",
        "    self.prev, self.next = {}, {}\n",
        "    self.neurons = set()\n",
        "\n",
        "    self.Input_layer = [self.idcnt+i for i in range(input_size)]\n",
        "    self.neurons.update([self.idcnt+i for i in range(input_size)])\n",
        "    self.idcnt+=input_size\n",
        "\n",
        "    self.Output_layer = [self.idcnt+o for o in range(output_size)]\n",
        "    self.neurons.update([self.idcnt+o for o in range(output_size)])\n",
        "    self.idcnt+=output_size\n",
        "\n",
        "    for i in self.Input_layer: self.next[i], self.prev[i] = self.Output_layer.copy(), []\n",
        "    for o in self.Output_layer: self.prev[o], self.next[o] = self.Input_layer.copy(), []\n",
        "\n",
        "    self.weight = {}\n",
        "    self.weight_gsum, self.weight_gsqr = {}, {}\n",
        "    for u in self.Input_layer:\n",
        "      for v in self.Output_layer:\n",
        "        self.weight[(u,v)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_gsum[(u,v)], self.weight_gsqr[(u,v)] = 0, 0\n",
        "\n",
        "    self.bias = {}\n",
        "    self.bias_gsum, self.bias_gsqr = {}, {}\n",
        "    for i in self.neurons:\n",
        "      self.bias[i] = np.random.uniform(-0.1, 0.1)\n",
        "      self.bias_gsum[i], self.bias_gsqr[i] = 0, 0\n",
        "\n",
        "  def addLayer(self, mid_size, UP, DOWN):\n",
        "    Mid_layer = [self.idcnt+m for m in range(mid_size)]\n",
        "    self.neurons.update([self.idcnt+m for m in range(mid_size)])\n",
        "    self.idcnt+=mid_size\n",
        "\n",
        "    for m in Mid_layer:\n",
        "      self.bias[m] = np.random.uniform(-0.1, 0.1)\n",
        "      self.bias_gsum[m], self.bias_gsqr[m] = 0, 0\n",
        "\n",
        "      self.prev[m] = UP.copy()\n",
        "      for u in UP:\n",
        "        self.weight[(u,m)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_gsum[(u,m)], self.weight_gsqr[(u,m)] = 0, 0\n",
        "\n",
        "      self.next[m] = DOWN.copy()\n",
        "      for v in DOWN:\n",
        "        self.weight[(m,v)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_gsum[(m,v)], self.weight_gsqr[(m,v)] = 0, 0\n",
        "\n",
        "    for u in UP:\n",
        "      self.next[u] = Mid_layer.copy()\n",
        "    for v in DOWN:\n",
        "      self.prev[v] = Mid_layer.copy()\n",
        "\n",
        "    for u in UP:\n",
        "      for v in DOWN:\n",
        "        self.weight.pop((u,v))\n",
        "        self.weight_gsum.pop((u,v))\n",
        "        self.weight_gsqr.pop((u,v))\n",
        "    return Mid_layer\n",
        "\n",
        "  def addNode(self, u, v):\n",
        "    n = self.idcnt\n",
        "    self.idcnt += 1\n",
        "    self.neurons.add(n)\n",
        "    self.next[n], self.prev[n] = [], []\n",
        "    self.bias[n] = np.random.uniform(-0.1, 0.1)\n",
        "    self.bias_gsum[n], self.bias_gsqr[n] = 0, 0\n",
        "\n",
        "    self.next[u].append(n)\n",
        "    self.prev[n].append(u)\n",
        "    self.next[n].append(v)\n",
        "    self.prev[v].append(n)\n",
        "    self.weight[(u,n)] = np.random.uniform(-0.1, 0.1)\n",
        "    self.weight_gsum[(u,n)], self.weight_gsqr[(u,n)] = 0, 0\n",
        "    self.weight[(n,v)] = np.random.uniform(-0.1, 0.1)\n",
        "    self.weight_gsum[(n,v)], self.weight_gsqr[(n,v)] = 0, 0\n",
        "\n",
        "  def __forward(self, X, batch_size):\n",
        "    assert X.shape == (batch_size,len(self.Input_layer)), \\\n",
        "      f\"X.shape={X.shape}, where {(batch_size,len(self.Input_layer))} is expected\"\n",
        "    a = {q: np.zeros(batch_size) for q in self.neurons}\n",
        "\n",
        "    for i, n in enumerate(self.Input_layer):\n",
        "      a[n] = X[:, i].copy()\n",
        "\n",
        "    q = deque()\n",
        "    for i in self.Input_layer:\n",
        "      q.append(i)\n",
        "\n",
        "    cnt = {q: 0 for q in self.neurons}\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      a[c] = np.tanh(a[c] + self.bias[c])\n",
        "      for n in self.next[c]:\n",
        "        a[n] = a[n] + a[c] * self.weight[(c,n)]\n",
        "        cnt[n] += 1\n",
        "        if cnt[n] == len(self.prev[n]):\n",
        "          q.append(n)\n",
        "    return a\n",
        "\n",
        "  def evaluate(self, X):\n",
        "    a = self.__forward(X, len(X))\n",
        "    return np.array([a[o] for o in self.Output_layer]).T\n",
        "\n",
        "  def __backward(self, X, Y, batch_size, learning_rate, dsum, dsqr):\n",
        "    assert X.shape == (batch_size,len(self.Input_layer)), \\\n",
        "      f\"X.shape={X.shape}, where {(batch_size,len(self.Input_layer))} is expected\"\n",
        "    assert Y.shape == (batch_size,len(self.Output_layer)), \\\n",
        "      f\"X.shape={Y.shape}, where {(batch_size,len(self.Output_layer))} is expected\"\n",
        "    a = self.__forward(X, batch_size)\n",
        "\n",
        "    db, dw = {}, {}\n",
        "\n",
        "    par_a = {q: np.zeros(batch_size) for q in self.neurons}\n",
        "    for o, n in enumerate(self.Output_layer):\n",
        "      par_a[n] = 2 * (a[n] - Y[:, o])\n",
        "\n",
        "    q = deque()\n",
        "    for o in self.Output_layer:\n",
        "      q.append(o)\n",
        "\n",
        "    cnt = {q: 0 for q in self.neurons}\n",
        "\n",
        "    msg = []\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      par_b = par_a[c] * (1-a[c]**2)\n",
        "\n",
        "      gbias = par_b\n",
        "      self.bias_gsum[c] = (1-dsum)*np.sum(gbias)/batch_size + dsum*self.bias_gsum[c]\n",
        "      self.bias_gsqr[c] = (1-dsqr)*np.sum(gbias**2)/batch_size + dsqr*self.bias_gsqr[c]\n",
        "      db[c] = -learning_rate * self.bias_gsum[c] / (self.bias_gsqr[c]**(1/2)+1)\n",
        "\n",
        "      for p in self.prev[c]:\n",
        "        par_a[p] += par_a[c] * (1-a[c]**2) * self.weight[(p,c)]\n",
        "        gweight = par_a[c] * (1-a[c]**2) * a[p]\n",
        "        self.weight_gsum[(p,c)] = \\\n",
        "         (1-dsum)*np.sum(gweight)/batch_size + dsum*self.weight_gsum[(p,c)]\n",
        "        self.weight_gsqr[(p,c)] = \\\n",
        "         (1-dsqr)*np.sum(gweight**2)/batch_size + dsqr*self.weight_gsqr[(p,c)]\n",
        "        dw[(p,c)] = \\\n",
        "         -learning_rate * self.weight_gsum[(p,c)] / (self.weight_gsqr[(p,c)]**(1/2)+1)\n",
        "\n",
        "        if np.std(gweight, axis=0)/np.mean(gweight, axis=0) > 1000 and len(self.neurons) < 20:\n",
        "          msg.append((p,c))\n",
        "\n",
        "        cnt[p] += 1\n",
        "        if cnt[p] == len(self.next[p]):\n",
        "          q.append(p)\n",
        "\n",
        "    return dw, db, msg\n",
        "\n",
        "  def train(self, x, y, batch_size, epochs, learning_rate, verbose=True):\n",
        "    assert len(x) == len(y)\n",
        "    l = len(x)\n",
        "    Loss = [0 for epoch in range(epochs)]\n",
        "    for epoch in progressbar(range(epochs)):\n",
        "      X, Y = x.copy(), y.copy()\n",
        "      data=[(X[_], Y[_]) for _ in range(l)]\n",
        "      random.shuffle(data)\n",
        "      for _ in range(l):\n",
        "        X[_],Y[_]=data[_]\n",
        "      loss = 0\n",
        "      for batch in range(int(l / batch_size)):\n",
        "        L, R = batch * batch_size, (batch + 1) * batch_size\n",
        "        x_split, y_split = X[L:R], Y[L:R]\n",
        "        dw, db, msg = self.__backward(\n",
        "          x_split, y_split, batch_size, learning_rate,\n",
        "          .9, .9\n",
        "        )\n",
        "        for w in self.weight:\n",
        "            self.weight[w] += dw[w]\n",
        "        for p in self.neurons:\n",
        "            self.bias[p] += db[p]\n",
        "        for (u,v) in msg:\n",
        "          self.addNode(u,v)\n",
        "        output = self.evaluate(x_split)\n",
        "        loss += np.sum(((y_split-output) ** 2), axis=(0,1))\n",
        "      loss = ((loss) ** 0.5) / (int(l / batch_size) * batch_size)\n",
        "      if verbose:\n",
        "        print(f\"Epoch {epoch}/{epochs}, Loss:{loss}\")\n",
        "      Loss[epoch] = loss\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot([i for i in range(epochs)],Loss,'+',linewidth=2)\n",
        "    return fig, ax"
      ],
      "metadata": {
        "id": "YUliU5Cx-oyU",
        "cellView": "form"
      },
      "id": "YUliU5Cx-oyU",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "Y=np.array([[0],[1],[1],[0]])\n",
        "mod=Model(2, 1)\n",
        "mid1=mod.addLayer(4, mod.Input_layer, mod.Output_layer)\n",
        "mod.addLayer(4, mid1, mod.Output_layer)\n",
        "mod.evaluate(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfiQiE5l8XOr",
        "outputId": "0301150b-32e2-4cf4-eaed-c3840fa0948d"
      },
      "id": "KfiQiE5l8XOr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.04023416],\n",
              "       [0.04032494],\n",
              "       [0.03865762],\n",
              "       [0.03874732]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod.train(X, Y, 4, 10000, 0.01, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "aC2n_sC_8xnP",
        "outputId": "1b21a319-1bc6-4249-95cc-85faa285f477"
      },
      "id": "aC2n_sC_8xnP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tqmd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-d1f9ed56303c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-69-b8ec763484ce>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, batch_size, epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0mLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Loading…\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m       \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tqmd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X, Y)\n",
        "mod.evaluate(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz2Q6eSC82cI",
        "outputId": "934454d6-d0de-4108-f582-646463e4dcd0"
      },
      "id": "Iz2Q6eSC82cI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]] [[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.04320133],\n",
              "       [-0.04445075],\n",
              "       [-0.04314593],\n",
              "       [-0.04438858]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Flatten images to 1D vector of 784 features (28*28)\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "def test(model, X, Y, batch_size):\n",
        "  k = int(len(X)/batch_size)\n",
        "  for i in range(k):\n",
        "    Y_hat=model.evaluate(X[i*batch_size:(i+1)*batch_size])\n",
        "    wrong=0\n",
        "    for j in range(batch_size):\n",
        "      max1,max2,id1,id2=-999,-999,-1,-1\n",
        "      for l in range(10):\n",
        "        if max1 < Y_hat[j][l]:\n",
        "          max1,id1=Y_hat[j][l],l\n",
        "        if max2 < Y[i*batch_size+j][l]:\n",
        "          max2,id2=Y[i*batch_size+j][l],l\n",
        "      if id1 != id2: wrong+=1\n",
        "    print(f\"batch: {i}, accuracy: {(batch_size-wrong)/batch_size*100}%\")\n",
        "mod2 = Model(784, 10)\n",
        "#mod2.addLayer(32, mod2.Input_layer, mod2.Output_layer)"
      ],
      "metadata": {
        "id": "lcOYUuYAjXPG",
        "collapsed": true,
        "outputId": "8b7fcde7-a1e3-46ef-f907-5c2e6012a950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lcOYUuYAjXPG",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(mod2.Input_layer))\n",
        "fig, ax = mod2.train(x_train, y_train, 32, 5, 0.01)\n",
        "plt.show()\n",
        "\"\"\"\n",
        "for i in range(1,9):\n",
        "  plt.subplot(330+i)\n",
        "  plt.imshow(x_test[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
        "print(mod2.evaluate(np.array([x_test[5]])))\n",
        "print(y_test[5])\n",
        "\"\"\"\n",
        "test(mod2, x_test, y_test, 500)"
      ],
      "metadata": {
        "id": "ClmATYl3Qcb7",
        "outputId": "f48d73dd-fc18-4622-dd34-b6bc212a0f9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ClmATYl3Qcb7",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "784\n",
            "[█...........................................................] 0.1/5 Est wait 00:0.0\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(mod2, x_test, y_test, 5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhe7AJ7MHY5E",
        "outputId": "1d5d836f-88a5-49fb-8be8-a65c458c43fe"
      },
      "id": "Bhe7AJ7MHY5E",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch: 0, accuracy: 77.5%\n",
            "batch: 1, accuracy: 86.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set seed such that we always get the same dataset\n",
        "# (this is a good idea in general)\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_dataset(num_sequences=2**8):\n",
        "    \"\"\"\n",
        "    Generates a number of sequences as our dataset.\n",
        "\n",
        "    Args:\n",
        "     `num_sequences`: the number of sequences to be generated.\n",
        "\n",
        "    Returns a list of sequences.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "\n",
        "    for _ in range(num_sequences):\n",
        "        num_tokens = np.random.randint(1, 12)\n",
        "        sample = ['a'] * num_tokens + ['b'] * num_tokens + ['EOS']\n",
        "        samples.append(sample)\n",
        "\n",
        "    return samples\n",
        "\n",
        "\n",
        "sequences = generate_dataset()\n",
        "\n",
        "print('A single sample from the generated dataset:')\n",
        "print(sequences[0])"
      ],
      "metadata": {
        "id": "zNgFA_bYWkjp",
        "outputId": "b295e8a6-d9da-49a1-e968-a77ecb0dce07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zNgFA_bYWkjp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A single sample from the generated dataset:\n",
            "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def sequences_to_dicts(sequences):\n",
        "    \"\"\"\n",
        "    Creates word_to_idx and idx_to_word dictionaries for a list of sequences.\n",
        "    \"\"\"\n",
        "    # A bit of Python-magic to flatten a nested list\n",
        "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "    # Flatten the dataset\n",
        "    all_words = flatten(sequences)\n",
        "\n",
        "    # Count number of word occurences\n",
        "    word_count = defaultdict(int)\n",
        "    for word in flatten(sequences):\n",
        "        word_count[word] += 1\n",
        "\n",
        "    # Sort by frequency\n",
        "    word_count = sorted(list(word_count.items()), key=lambda l: -l[1])\n",
        "\n",
        "    # Create a list of all unique words\n",
        "    unique_words = [item[0] for item in word_count]\n",
        "\n",
        "    # Add UNK token to list of words\n",
        "    unique_words.append('UNK')\n",
        "\n",
        "    # Count number of sequences and number of unique words\n",
        "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
        "\n",
        "    # Create dictionaries so that we can go from word to index and back\n",
        "    # If a word is not in our vocabulary, we assign it to token 'UNK'\n",
        "    word_to_idx = defaultdict(lambda: vocab_size-1)\n",
        "    idx_to_word = defaultdict(lambda: 'UNK')\n",
        "\n",
        "    # Fill dictionaries\n",
        "    for idx, word in enumerate(unique_words):\n",
        "        # YOUR CODE HERE!\n",
        "        word_to_idx[word] = idx\n",
        "        idx_to_word[idx] = word\n",
        "\n",
        "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
        "\n",
        "\n",
        "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)\n",
        "\n",
        "print(f'We have {num_sequences} sentences and {len(word_to_idx)} unique tokens in our dataset (including UNK).\\n')\n",
        "print('The index of \\'b\\' is', word_to_idx['b'])\n",
        "print(f'The word corresponding to index 2 is \\'{idx_to_word[2]}\\'')\n",
        "\n",
        "assert idx_to_word[word_to_idx['b']] == 'b', \\\n",
        "    'Consistency error: something went wrong in the conversion.'"
      ],
      "metadata": {
        "id": "WL3UJobeWWNx",
        "outputId": "6de34448-5cf2-40e6-d459-95da2a7cea2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WL3UJobeWWNx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 256 sentences and 4 unique tokens in our dataset (including UNK).\n",
            "\n",
            "The index of 'b' is 1\n",
            "The word corresponding to index 2 is 'EOS'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod3=Model(1,1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "6dLrehaNQFcu",
        "outputId": "41b22c57-22bf-4c43-a44c-7dc36f4ed6b4"
      },
      "id": "6dLrehaNQFcu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-28b6613b5aa7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmod3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}