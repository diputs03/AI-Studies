{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diputs03/AI-Studies/blob/main/Creating_network/dymamic_architect_rebuilt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Aiming a Dynaimic Graph-structured NeuronNetwork\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "6mXjwpToZTeV"
      },
      "id": "6mXjwpToZTeV",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Activation function in this case in tanh\n",
        "Loss is the Euclidean loss\n",
        "\"\"\"\n",
        "class Model:\n",
        "  class Neuron:\n",
        "    def __init__(self, name, prev, next):\n",
        "      self.name, self.prev, self.next = name, prev, next\n",
        "      self.bias = np.random.uniform(-0.1, 0.1)\n",
        "\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.Input_layer = [Model.Neuron(f\"input{i}\", [], []) for i in range(input_size)]\n",
        "    self.Output_layer = [Model.Neuron(f\"output{o}\", [], []) for o in range(output_size)]\n",
        "    for i in range(input_size): self.Input_layer[i].next = self.Output_layer\n",
        "    for o in range(output_size): self.Output_layer[o].prev = self.Input_layer\n",
        "    self.weight = {}\n",
        "    for u in self.Input_layer:\n",
        "      for v in self.Output_layer:\n",
        "        self.weight[(u,v)] = np.random.uniform(-0.1, 0.1)\n",
        "    self.all_neurons = self.Input_layer + self.Output_layer\n",
        "\n",
        "  def forward(self, X, batch_size):\n",
        "    assert X.shape == (batch_size,len(self.Input_layer))\n",
        "    a = {q: np.zeros(batch_size) for q in self.all_neurons}\n",
        "\n",
        "    for i, n in enumerate(self.Input_layer):\n",
        "      a[n] = X[:, i]\n",
        "\n",
        "    q = deque()\n",
        "    for i in self.Input_layer:\n",
        "      q.append(i)\n",
        "\n",
        "    cnt = {q: 0 for q in self.all_neurons}\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      a[c] = np.tanh(a[c] + c.bias)\n",
        "      for n in c.next:\n",
        "        a[n] = a[n] + a[c] * self.weight[(c,n)]\n",
        "        cnt[n] += 1\n",
        "        if cnt[n] == len(n.prev):\n",
        "          q.append(n)\n",
        "    return a\n",
        "\n",
        "  def eval(self, X):\n",
        "    a = self.forward(X, len(X))\n",
        "    return np.array([a[o] for o in self.Output_layer]).T\n",
        "\n",
        "  def backward(self, X, Y, batch_size, learning_rate):\n",
        "    assert X.shape == (batch_size,len(self.Input_layer))\n",
        "    assert Y.shape == (batch_size,len(self.Output_layer))\n",
        "    a = self.forward(X, batch_size)\n",
        "    par_a = {q: np.zeros(batch_size) for q in self.all_neurons}\n",
        "    for o, n in enumerate(self.Output_layer):\n",
        "      par_a[n] = 2 * (a[n] - Y[:, o])\n",
        "\n",
        "    q = deque()\n",
        "    for o in self.Output_layer:\n",
        "      q.append(o)\n",
        "\n",
        "    cnt = {q: 0 for q in self.all_neurons}\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      par_b = par_a[c] * (1-a[c]**2)\n",
        "      c.bias -= np.mean(par_b) * learning_rate\n",
        "      for p in c.prev:\n",
        "        par_a[p] += par_a[c] * (1-a[c]**2) * self.weight[(p,c)]\n",
        "        par_w_pc = par_a[c] * (1-a[c]**2) * a[p]\n",
        "        self.weight[(p,c)] -= np.mean(par_w_pc) * learning_rate\n",
        "        cnt[p] += 1\n",
        "        if cnt[p] == len(p.next):\n",
        "          q.append(p)\n",
        "\n",
        "  def addLayer(self, mid_size, UP, DOWN):\n",
        "    Mid_layer = [Model.Neuron(f\"mid{o}\", [], []) for o in range(mid_size)]\n",
        "    for m, mid in enumerate(Mid_layer):\n",
        "      Mid_layer[m].prev = UP\n",
        "      for u in UP:\n",
        "        self.weight[(u,mid)] = np.random.uniform(-0.1, 0.1)\n",
        "      Mid_layer[m].next = DOWN\n",
        "      for v in DOWN:\n",
        "        self.weight[(mid,v)] = np.random.uniform(-0.1, 0.1)\n",
        "    for u in UP:\n",
        "      u.next = Mid_layer\n",
        "    for v in DOWN:\n",
        "      v.prev = Mid_layer\n",
        "    for u in UP:\n",
        "      for v in DOWN:\n",
        "        self.weight.pop((u, v))\n",
        "    self.all_neurons += Mid_layer\n",
        "    return Mid_layer\n",
        "\n",
        "  def train(self, X, Y, batch_size, epochs, learning_rate):\n",
        "    l = len(X)\n",
        "    for epoch in range(epochs):\n",
        "      data=[(X[_], Y[_]) for _ in range(len(X))]\n",
        "      random.shuffle(data)\n",
        "      for _ in range(len(X)):\n",
        "        X[_],Y[_]=data[_]\n",
        "      loss = 0\n",
        "      for batch in range(int(l / batch_size)):\n",
        "        L, R = batch * batch_size, (batch + 1) * batch_size\n",
        "        x_train, y_train = X[L:R], Y[L:R]\n",
        "        self.backward(x_train, y_train, batch_size, learning_rate)\n",
        "        output = self.eval(x_train)\n",
        "        loss += np.sum(((y_train - output) ** 2), axis=(0,1))\n",
        "      loss = ((loss) ** 0.5) / (int(l / batch_size) * batch_size)\n",
        "      print(f\"Epoch {epoch}/{epochs}, Loss:{loss}\")"
      ],
      "metadata": {
        "id": "YUliU5Cx-oyU"
      },
      "id": "YUliU5Cx-oyU",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "Y=np.array([[0],[1],[1],[0]])\n",
        "mod=Model(2, 1)\n",
        "mid1=mod.addLayer(4, mod.Input_layer, mod.Output_layer)\n",
        "mod.addLayer(4, mid1, mod.Output_layer)\n",
        "mod.eval(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfiQiE5l8XOr",
        "outputId": "5ec01112-8e4d-4659-d482-67c38fe0d128"
      },
      "id": "KfiQiE5l8XOr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.05312619],\n",
              "       [0.05317977],\n",
              "       [0.05292535],\n",
              "       [0.0529803 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod.train(X, Y, 4, 500, 0.1)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC2n_sC_8xnP",
        "outputId": "2f8f8310-d852-4771-c6ff-5c9704712a8d"
      },
      "id": "aC2n_sC_8xnP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss:0.011048444954117331\n",
            "Epoch 1, Loss:0.011042339979862315\n",
            "Epoch 2, Loss:0.011036244496605507\n",
            "Epoch 3, Loss:0.011030158480130281\n",
            "Epoch 4, Loss:0.011024081906306049\n",
            "Epoch 5, Loss:0.011018014751087657\n",
            "Epoch 6, Loss:0.011011956990515048\n",
            "Epoch 7, Loss:0.011005908600712877\n",
            "Epoch 8, Loss:0.010999869557890118\n",
            "Epoch 9, Loss:0.01099383983833968\n",
            "Epoch 10, Loss:0.010987819418438183\n",
            "Epoch 11, Loss:0.010981808274645233\n",
            "Epoch 12, Loss:0.010975806383503484\n",
            "Epoch 13, Loss:0.0109698137216378\n",
            "Epoch 14, Loss:0.010963830265755314\n",
            "Epoch 15, Loss:0.010957855992644705\n",
            "Epoch 16, Loss:0.010951890879176196\n",
            "Epoch 17, Loss:0.01094593490230078\n",
            "Epoch 18, Loss:0.010939988039050164\n",
            "Epoch 19, Loss:0.010934050266536322\n",
            "Epoch 20, Loss:0.01092812156195111\n",
            "Epoch 21, Loss:0.010922201902565876\n",
            "Epoch 22, Loss:0.010916291265731182\n",
            "Epoch 23, Loss:0.010910389628876416\n",
            "Epoch 24, Loss:0.010904496969509458\n",
            "Epoch 25, Loss:0.01089861326521624\n",
            "Epoch 26, Loss:0.010892738493660625\n",
            "Epoch 27, Loss:0.010886872632583755\n",
            "Epoch 28, Loss:0.010881015659803928\n",
            "Epoch 29, Loss:0.010875167553216214\n",
            "Epoch 30, Loss:0.010869328290792124\n",
            "Epoch 31, Loss:0.010863497850579051\n",
            "Epoch 32, Loss:0.010857676210700384\n",
            "Epoch 33, Loss:0.010851863349354784\n",
            "Epoch 34, Loss:0.010846059244815909\n",
            "Epoch 35, Loss:0.010840263875432299\n",
            "Epoch 36, Loss:0.01083447721962677\n",
            "Epoch 37, Loss:0.010828699255896357\n",
            "Epoch 38, Loss:0.0108229299628117\n",
            "Epoch 39, Loss:0.010817169319016995\n",
            "Epoch 40, Loss:0.010811417303229542\n",
            "Epoch 41, Loss:0.01080567389423931\n",
            "Epoch 42, Loss:0.010799939070908815\n",
            "Epoch 43, Loss:0.010794212812172799\n",
            "Epoch 44, Loss:0.01078849509703772\n",
            "Epoch 45, Loss:0.010782785904581593\n",
            "Epoch 46, Loss:0.010777085213953763\n",
            "Epoch 47, Loss:0.010771393004374297\n",
            "Epoch 48, Loss:0.010765709255133926\n",
            "Epoch 49, Loss:0.010760033945593717\n",
            "Epoch 50, Loss:0.010754367055184677\n",
            "Epoch 51, Loss:0.010748708563407539\n",
            "Epoch 52, Loss:0.01074305844983231\n",
            "Epoch 53, Loss:0.010737416694098112\n",
            "Epoch 54, Loss:0.010731783275912954\n",
            "Epoch 55, Loss:0.010726158175053124\n",
            "Epoch 56, Loss:0.010720541371363301\n",
            "Epoch 57, Loss:0.010714932844755831\n",
            "Epoch 58, Loss:0.01070933257521095\n",
            "Epoch 59, Loss:0.010703740542775897\n",
            "Epoch 60, Loss:0.010698156727565078\n",
            "Epoch 61, Loss:0.010692581109759625\n",
            "Epoch 62, Loss:0.010687013669607115\n",
            "Epoch 63, Loss:0.010681454387421241\n",
            "Epoch 64, Loss:0.01067590324358153\n",
            "Epoch 65, Loss:0.010670360218533181\n",
            "Epoch 66, Loss:0.010664825292786728\n",
            "Epoch 67, Loss:0.010659298446917653\n",
            "Epoch 68, Loss:0.010653779661566165\n",
            "Epoch 69, Loss:0.010648268917437032\n",
            "Epoch 70, Loss:0.010642766195299247\n",
            "Epoch 71, Loss:0.010637271475985588\n",
            "Epoch 72, Loss:0.010631784740392614\n",
            "Epoch 73, Loss:0.010626305969480276\n",
            "Epoch 74, Loss:0.010620835144271479\n",
            "Epoch 75, Loss:0.01061537224585224\n",
            "Epoch 76, Loss:0.010609917255370915\n",
            "Epoch 77, Loss:0.010604470154038259\n",
            "Epoch 78, Loss:0.01059903092312714\n",
            "Epoch 79, Loss:0.010593599543972154\n",
            "Epoch 80, Loss:0.010588175997969462\n",
            "Epoch 81, Loss:0.010582760266576452\n",
            "Epoch 82, Loss:0.010577352331311574\n",
            "Epoch 83, Loss:0.010571952173753951\n",
            "Epoch 84, Loss:0.010566559775543272\n",
            "Epoch 85, Loss:0.010561175118379451\n",
            "Epoch 86, Loss:0.010555798184022469\n",
            "Epoch 87, Loss:0.010550428954291868\n",
            "Epoch 88, Loss:0.010545067411066922\n",
            "Epoch 89, Loss:0.010539713536285855\n",
            "Epoch 90, Loss:0.01053436731194618\n",
            "Epoch 91, Loss:0.010529028720103917\n",
            "Epoch 92, Loss:0.010523697742873761\n",
            "Epoch 93, Loss:0.010518374362428473\n",
            "Epoch 94, Loss:0.0105130585609991\n",
            "Epoch 95, Loss:0.010507750320874143\n",
            "Epoch 96, Loss:0.01050244962439989\n",
            "Epoch 97, Loss:0.010497156453979861\n",
            "Epoch 98, Loss:0.010491870792074476\n",
            "Epoch 99, Loss:0.010486592621201107\n",
            "Epoch 100, Loss:0.01048132192393375\n",
            "Epoch 101, Loss:0.010476058682902634\n",
            "Epoch 102, Loss:0.010470802880794217\n",
            "Epoch 103, Loss:0.010465554500350749\n",
            "Epoch 104, Loss:0.010460313524370102\n",
            "Epoch 105, Loss:0.010455079935705658\n",
            "Epoch 106, Loss:0.010449853717265978\n",
            "Epoch 107, Loss:0.010444634852014578\n",
            "Epoch 108, Loss:0.01043942332296971\n",
            "Epoch 109, Loss:0.01043421911320408\n",
            "Epoch 110, Loss:0.01042902220584474\n",
            "Epoch 111, Loss:0.010423832584072867\n",
            "Epoch 112, Loss:0.010418650231123372\n",
            "Epoch 113, Loss:0.0104134751302849\n",
            "Epoch 114, Loss:0.01040830726489933\n",
            "Epoch 115, Loss:0.010403146618361947\n",
            "Epoch 116, Loss:0.010397993174120879\n",
            "Epoch 117, Loss:0.010392846915676934\n",
            "Epoch 118, Loss:0.010387707826583659\n",
            "Epoch 119, Loss:0.010382575890446777\n",
            "Epoch 120, Loss:0.010377451090924139\n",
            "Epoch 121, Loss:0.010372333411725554\n",
            "Epoch 122, Loss:0.010367222836612455\n",
            "Epoch 123, Loss:0.010362119349397736\n",
            "Epoch 124, Loss:0.010357022933945637\n",
            "Epoch 125, Loss:0.010351933574171412\n",
            "Epoch 126, Loss:0.010346851254041167\n",
            "Epoch 127, Loss:0.010341775957571742\n",
            "Epoch 128, Loss:0.010336707668830214\n",
            "Epoch 129, Loss:0.010331646371934122\n",
            "Epoch 130, Loss:0.010326592051050909\n",
            "Epoch 131, Loss:0.010321544690397921\n",
            "Epoch 132, Loss:0.010316504274242078\n",
            "Epoch 133, Loss:0.010311470786899812\n",
            "Epoch 134, Loss:0.010306444212736687\n",
            "Epoch 135, Loss:0.010301424536167336\n",
            "Epoch 136, Loss:0.010296411741655354\n",
            "Epoch 137, Loss:0.01029140581371285\n",
            "Epoch 138, Loss:0.0102864067369004\n",
            "Epoch 139, Loss:0.010281414495826868\n",
            "Epoch 140, Loss:0.01027642907514914\n",
            "Epoch 141, Loss:0.010271450459571951\n",
            "Epoch 142, Loss:0.010266478633847886\n",
            "Epoch 143, Loss:0.010261513582776827\n",
            "Epoch 144, Loss:0.01025655529120606\n",
            "Epoch 145, Loss:0.010251603744029836\n",
            "Epoch 146, Loss:0.010246658926189589\n",
            "Epoch 147, Loss:0.010241720822673217\n",
            "Epoch 148, Loss:0.010236789418515357\n",
            "Epoch 149, Loss:0.010231864698796944\n",
            "Epoch 150, Loss:0.010226946648645097\n",
            "Epoch 151, Loss:0.010222035253232897\n",
            "Epoch 152, Loss:0.010217130497779392\n",
            "Epoch 153, Loss:0.010212232367549035\n",
            "Epoch 154, Loss:0.010207340847851909\n",
            "Epoch 155, Loss:0.010202455924043341\n",
            "Epoch 156, Loss:0.010197577581523731\n",
            "Epoch 157, Loss:0.010192705805738389\n",
            "Epoch 158, Loss:0.010187840582177365\n",
            "Epoch 159, Loss:0.010182981896375343\n",
            "Epoch 160, Loss:0.010178129733911356\n",
            "Epoch 161, Loss:0.010173284080408629\n",
            "Epoch 162, Loss:0.010168444921534403\n",
            "Epoch 163, Loss:0.010163612242999942\n",
            "Epoch 164, Loss:0.01015878603056003\n",
            "Epoch 165, Loss:0.010153966270013026\n",
            "Epoch 166, Loss:0.010149152947200812\n",
            "Epoch 167, Loss:0.01014434604800818\n",
            "Epoch 168, Loss:0.010139545558363161\n",
            "Epoch 169, Loss:0.010134751464236536\n",
            "Epoch 170, Loss:0.010129963751641824\n",
            "Epoch 171, Loss:0.010125182406635014\n",
            "Epoch 172, Loss:0.010120407415314503\n",
            "Epoch 173, Loss:0.010115638763820767\n",
            "Epoch 174, Loss:0.010110876438336469\n",
            "Epoch 175, Loss:0.010106120425085963\n",
            "Epoch 176, Loss:0.010101370710335456\n",
            "Epoch 177, Loss:0.010096627280392623\n",
            "Epoch 178, Loss:0.010091890121606495\n",
            "Epoch 179, Loss:0.010087159220367292\n",
            "Epoch 180, Loss:0.010082434563106424\n",
            "Epoch 181, Loss:0.010077716136296044\n",
            "Epoch 182, Loss:0.01007300392644922\n",
            "Epoch 183, Loss:0.010068297920119373\n",
            "Epoch 184, Loss:0.010063598103900562\n",
            "Epoch 185, Loss:0.010058904464426976\n",
            "Epoch 186, Loss:0.010054216988373055\n",
            "Epoch 187, Loss:0.01004953566245309\n",
            "Epoch 188, Loss:0.010044860473421235\n",
            "Epoch 189, Loss:0.010040191408071275\n",
            "Epoch 190, Loss:0.010035528453236464\n",
            "Epoch 191, Loss:0.010030871595789526\n",
            "Epoch 192, Loss:0.010026220822642318\n",
            "Epoch 193, Loss:0.010021576120745723\n",
            "Epoch 194, Loss:0.010016937477089538\n",
            "Epoch 195, Loss:0.010012304878702305\n",
            "Epoch 196, Loss:0.010007678312651314\n",
            "Epoch 197, Loss:0.0100030577660421\n",
            "Epoch 198, Loss:0.009998443226018672\n",
            "Epoch 199, Loss:0.009993834679763114\n",
            "Epoch 200, Loss:0.009989232114495594\n",
            "Epoch 201, Loss:0.009984635517474139\n",
            "Epoch 202, Loss:0.009980044875994465\n",
            "Epoch 203, Loss:0.009975460177389983\n",
            "Epoch 204, Loss:0.009970881409031407\n",
            "Epoch 205, Loss:0.00996630855832692\n",
            "Epoch 206, Loss:0.009961741612721786\n",
            "Epoch 207, Loss:0.009957180559698298\n",
            "Epoch 208, Loss:0.009952625386775604\n",
            "Epoch 209, Loss:0.009948076081509716\n",
            "Epoch 210, Loss:0.009943532631493057\n",
            "Epoch 211, Loss:0.009938995024354802\n",
            "Epoch 212, Loss:0.00993446324776015\n",
            "Epoch 213, Loss:0.009929937289410773\n",
            "Epoch 214, Loss:0.009925417137044201\n",
            "Epoch 215, Loss:0.009920902778433993\n",
            "Epoch 216, Loss:0.009916394201389456\n",
            "Epoch 217, Loss:0.009911891393755534\n",
            "Epoch 218, Loss:0.009907394343412868\n",
            "Epoch 219, Loss:0.009902903038277233\n",
            "Epoch 220, Loss:0.009898417466299825\n",
            "Epoch 221, Loss:0.009893937615466863\n",
            "Epoch 222, Loss:0.009889463473799764\n",
            "Epoch 223, Loss:0.009884995029354526\n",
            "Epoch 224, Loss:0.00988053227022212\n",
            "Epoch 225, Loss:0.009876075184527933\n",
            "Epoch 226, Loss:0.009871623760432047\n",
            "Epoch 227, Loss:0.009867177986128683\n",
            "Epoch 228, Loss:0.00986273784984637\n",
            "Epoch 229, Loss:0.009858303339847774\n",
            "Epoch 230, Loss:0.009853874444429478\n",
            "Epoch 231, Loss:0.009849451151921929\n",
            "Epoch 232, Loss:0.009845033450689211\n",
            "Epoch 233, Loss:0.009840621329129107\n",
            "Epoch 234, Loss:0.009836214775672869\n",
            "Epoch 235, Loss:0.009831813778785004\n",
            "Epoch 236, Loss:0.009827418326963322\n",
            "Epoch 237, Loss:0.009823028408738665\n",
            "Epoch 238, Loss:0.009818644012674904\n",
            "Epoch 239, Loss:0.009814265127368715\n",
            "Epoch 240, Loss:0.009809891741449583\n",
            "Epoch 241, Loss:0.009805523843579578\n",
            "Epoch 242, Loss:0.009801161422453186\n",
            "Epoch 243, Loss:0.00979680446679737\n",
            "Epoch 244, Loss:0.009792452965371345\n",
            "Epoch 245, Loss:0.009788106906966354\n",
            "Epoch 246, Loss:0.009783766280405837\n",
            "Epoch 247, Loss:0.009779431074544986\n",
            "Epoch 248, Loss:0.009775101278270804\n",
            "Epoch 249, Loss:0.009770776880502041\n",
            "Epoch 250, Loss:0.009766457870188983\n",
            "Epoch 251, Loss:0.009762144236313275\n",
            "Epoch 252, Loss:0.009757835967887984\n",
            "Epoch 253, Loss:0.00975353305395732\n",
            "Epoch 254, Loss:0.00974923548359663\n",
            "Epoch 255, Loss:0.009744943245912241\n",
            "Epoch 256, Loss:0.009740656330041397\n",
            "Epoch 257, Loss:0.00973637472515193\n",
            "Epoch 258, Loss:0.009732098420442592\n",
            "Epoch 259, Loss:0.009727827405142386\n",
            "Epoch 260, Loss:0.009723561668511016\n",
            "Epoch 261, Loss:0.00971930119983827\n",
            "Epoch 262, Loss:0.00971504598844436\n",
            "Epoch 263, Loss:0.009710796023679358\n",
            "Epoch 264, Loss:0.009706551294923538\n",
            "Epoch 265, Loss:0.009702311791586926\n",
            "Epoch 266, Loss:0.009698077503109415\n",
            "Epoch 267, Loss:0.009693848418960482\n",
            "Epoch 268, Loss:0.009689624528639196\n",
            "Epoch 269, Loss:0.009685405821674098\n",
            "Epoch 270, Loss:0.009681192287623097\n",
            "Epoch 271, Loss:0.009676983916073245\n",
            "Epoch 272, Loss:0.009672780696640904\n",
            "Epoch 273, Loss:0.009668582618971244\n",
            "Epoch 274, Loss:0.009664389672738638\n",
            "Epoch 275, Loss:0.009660201847645988\n",
            "Epoch 276, Loss:0.009656019133425175\n",
            "Epoch 277, Loss:0.009651841519836557\n",
            "Epoch 278, Loss:0.009647668996669023\n",
            "Epoch 279, Loss:0.009643501553739942\n",
            "Epoch 280, Loss:0.00963933918089499\n",
            "Epoch 281, Loss:0.009635181868007991\n",
            "Epoch 282, Loss:0.009631029604980967\n",
            "Epoch 283, Loss:0.009626882381743862\n",
            "Epoch 284, Loss:0.009622740188254653\n",
            "Epoch 285, Loss:0.009618603014499016\n",
            "Epoch 286, Loss:0.009614470850490386\n",
            "Epoch 287, Loss:0.009610343686269895\n",
            "Epoch 288, Loss:0.009606221511906043\n",
            "Epoch 289, Loss:0.00960210431749492\n",
            "Epoch 290, Loss:0.009597992093159868\n",
            "Epoch 291, Loss:0.00959388482905138\n",
            "Epoch 292, Loss:0.009589782515347256\n",
            "Epoch 293, Loss:0.009585685142252165\n",
            "Epoch 294, Loss:0.009581592699997865\n",
            "Epoch 295, Loss:0.009577505178842871\n",
            "Epoch 296, Loss:0.009573422569072398\n",
            "Epoch 297, Loss:0.00956934486099853\n",
            "Epoch 298, Loss:0.009565272044959716\n",
            "Epoch 299, Loss:0.009561204111320942\n",
            "Epoch 300, Loss:0.009557141050473617\n",
            "Epoch 301, Loss:0.009553082852835404\n",
            "Epoch 302, Loss:0.009549029508850171\n",
            "Epoch 303, Loss:0.009544981008987818\n",
            "Epoch 304, Loss:0.009540937343744338\n",
            "Epoch 305, Loss:0.0095368985036417\n",
            "Epoch 306, Loss:0.009532864479227576\n",
            "Epoch 307, Loss:0.009528835261075452\n",
            "Epoch 308, Loss:0.009524810839784514\n",
            "Epoch 309, Loss:0.009520791205979318\n",
            "Epoch 310, Loss:0.009516776350310119\n",
            "Epoch 311, Loss:0.009512766263452488\n",
            "Epoch 312, Loss:0.009508760936107197\n",
            "Epoch 313, Loss:0.009504760359000275\n",
            "Epoch 314, Loss:0.00950076452288301\n",
            "Epoch 315, Loss:0.009496773418531501\n",
            "Epoch 316, Loss:0.009492787036746941\n",
            "Epoch 317, Loss:0.009488805368355335\n",
            "Epoch 318, Loss:0.009484828404207502\n",
            "Epoch 319, Loss:0.009480856135178906\n",
            "Epoch 320, Loss:0.00947688855216966\n",
            "Epoch 321, Loss:0.009472925646104302\n",
            "Epoch 322, Loss:0.009468967407931905\n",
            "Epoch 323, Loss:0.009465013828625801\n",
            "Epoch 324, Loss:0.009461064899183802\n",
            "Epoch 325, Loss:0.009457120610627534\n",
            "Epoch 326, Loss:0.009453180954003104\n",
            "Epoch 327, Loss:0.009449245920380434\n",
            "Epoch 328, Loss:0.009445315500853368\n",
            "Epoch 329, Loss:0.009441389686539625\n",
            "Epoch 330, Loss:0.00943746846858079\n",
            "Epoch 331, Loss:0.009433551838141985\n",
            "Epoch 332, Loss:0.009429639786412092\n",
            "Epoch 333, Loss:0.009425732304603363\n",
            "Epoch 334, Loss:0.009421829383951696\n",
            "Epoch 335, Loss:0.009417931015716141\n",
            "Epoch 336, Loss:0.009414037191179225\n",
            "Epoch 337, Loss:0.009410147901646515\n",
            "Epoch 338, Loss:0.00940626313844678\n",
            "Epoch 339, Loss:0.009402382892931882\n",
            "Epoch 340, Loss:0.00939850715647661\n",
            "Epoch 341, Loss:0.009394635920478622\n",
            "Epoch 342, Loss:0.009390769176358449\n",
            "Epoch 343, Loss:0.009386906915559323\n",
            "Epoch 344, Loss:0.009383049129547127\n",
            "Epoch 345, Loss:0.009379195809810392\n",
            "Epoch 346, Loss:0.00937534694786002\n",
            "Epoch 347, Loss:0.009371502535229559\n",
            "Epoch 348, Loss:0.00936766256347471\n",
            "Epoch 349, Loss:0.009363827024173488\n",
            "Epoch 350, Loss:0.009359995908926222\n",
            "Epoch 351, Loss:0.009356169209355336\n",
            "Epoch 352, Loss:0.009352346917105181\n",
            "Epoch 353, Loss:0.009348529023842311\n",
            "Epoch 354, Loss:0.009344715521254932\n",
            "Epoch 355, Loss:0.009340906401053228\n",
            "Epoch 356, Loss:0.009337101654969204\n",
            "Epoch 357, Loss:0.0093333012747564\n",
            "Epoch 358, Loss:0.009329505252190062\n",
            "Epoch 359, Loss:0.00932571357906703\n",
            "Epoch 360, Loss:0.009321926247205403\n",
            "Epoch 361, Loss:0.009318143248444877\n",
            "Epoch 362, Loss:0.009314364574646461\n",
            "Epoch 363, Loss:0.00931059021769226\n",
            "Epoch 364, Loss:0.009306820169485752\n",
            "Epoch 365, Loss:0.009303054421951396\n",
            "Epoch 366, Loss:0.009299292967034745\n",
            "Epoch 367, Loss:0.009295535796702337\n",
            "Epoch 368, Loss:0.009291782902941637\n",
            "Epoch 369, Loss:0.00928803427776076\n",
            "Epoch 370, Loss:0.009284289913188859\n",
            "Epoch 371, Loss:0.009280549801275517\n",
            "Epoch 372, Loss:0.009276813934091188\n",
            "Epoch 373, Loss:0.009273082303726643\n",
            "Epoch 374, Loss:0.009269354902293359\n",
            "Epoch 375, Loss:0.009265631721923018\n",
            "Epoch 376, Loss:0.009261912754767898\n",
            "Epoch 377, Loss:0.009258197993000317\n",
            "Epoch 378, Loss:0.009254487428813019\n",
            "Epoch 379, Loss:0.009250781054418677\n",
            "Epoch 380, Loss:0.00924707886205034\n",
            "Epoch 381, Loss:0.009243380843960758\n",
            "Epoch 382, Loss:0.00923968699242289\n",
            "Epoch 383, Loss:0.009235997299729404\n",
            "Epoch 384, Loss:0.009232311758192946\n",
            "Epoch 385, Loss:0.009228630360145706\n",
            "Epoch 386, Loss:0.009224953097939803\n",
            "Epoch 387, Loss:0.009221279963946838\n",
            "Epoch 388, Loss:0.009217610950557953\n",
            "Epoch 389, Loss:0.009213946050183885\n",
            "Epoch 390, Loss:0.009210285255254747\n",
            "Epoch 391, Loss:0.009206628558220076\n",
            "Epoch 392, Loss:0.009202975951548564\n",
            "Epoch 393, Loss:0.009199327427728332\n",
            "Epoch 394, Loss:0.00919568297926654\n",
            "Epoch 395, Loss:0.00919204259868961\n",
            "Epoch 396, Loss:0.009188406278542838\n",
            "Epoch 397, Loss:0.009184774011390673\n",
            "Epoch 398, Loss:0.009181145789816436\n",
            "Epoch 399, Loss:0.009177521606422268\n",
            "Epoch 400, Loss:0.009173901453829125\n",
            "Epoch 401, Loss:0.009170285324676841\n",
            "Epoch 402, Loss:0.009166673211623733\n",
            "Epoch 403, Loss:0.00916306510734688\n",
            "Epoch 404, Loss:0.009159461004541902\n",
            "Epoch 405, Loss:0.009155860895922963\n",
            "Epoch 406, Loss:0.009152264774222485\n",
            "Epoch 407, Loss:0.00914867263219149\n",
            "Epoch 408, Loss:0.009145084462599262\n",
            "Epoch 409, Loss:0.009141500258233293\n",
            "Epoch 410, Loss:0.009137920011899336\n",
            "Epoch 411, Loss:0.00913434371642119\n",
            "Epoch 412, Loss:0.00913077136464091\n",
            "Epoch 413, Loss:0.00912720294941849\n",
            "Epoch 414, Loss:0.0091236384636319\n",
            "Epoch 415, Loss:0.009120077900176937\n",
            "Epoch 416, Loss:0.009116521251967435\n",
            "Epoch 417, Loss:0.009112968511934938\n",
            "Epoch 418, Loss:0.00910941967302864\n",
            "Epoch 419, Loss:0.009105874728215557\n",
            "Epoch 420, Loss:0.0091023336704803\n",
            "Epoch 421, Loss:0.009098796492825023\n",
            "Epoch 422, Loss:0.009095263188269365\n",
            "Epoch 423, Loss:0.00909173374985045\n",
            "Epoch 424, Loss:0.009088208170622947\n",
            "Epoch 425, Loss:0.009084686443658563\n",
            "Epoch 426, Loss:0.009081168562046604\n",
            "Epoch 427, Loss:0.009077654518893419\n",
            "Epoch 428, Loss:0.009074144307322674\n",
            "Epoch 429, Loss:0.009070637920474967\n",
            "Epoch 430, Loss:0.009067135351508271\n",
            "Epoch 431, Loss:0.009063636593597213\n",
            "Epoch 432, Loss:0.009060141639933683\n",
            "Epoch 433, Loss:0.00905665048372628\n",
            "Epoch 434, Loss:0.009053163118200647\n",
            "Epoch 435, Loss:0.009049679536599076\n",
            "Epoch 436, Loss:0.009046199732180571\n",
            "Epoch 437, Loss:0.009042723698221067\n",
            "Epoch 438, Loss:0.009039251428012929\n",
            "Epoch 439, Loss:0.00903578291486512\n",
            "Epoch 440, Loss:0.009032318152103258\n",
            "Epoch 441, Loss:0.00902885713306939\n",
            "Epoch 442, Loss:0.00902539985112194\n",
            "Epoch 443, Loss:0.009021946299635764\n",
            "Epoch 444, Loss:0.009018496472002091\n",
            "Epoch 445, Loss:0.009015050361628363\n",
            "Epoch 446, Loss:0.009011607961938228\n",
            "Epoch 447, Loss:0.009008169266371602\n",
            "Epoch 448, Loss:0.00900473426838444\n",
            "Epoch 449, Loss:0.009001302961448798\n",
            "Epoch 450, Loss:0.008997875339052719\n",
            "Epoch 451, Loss:0.008994451394700342\n",
            "Epoch 452, Loss:0.008991031121911575\n",
            "Epoch 453, Loss:0.008987614514222309\n",
            "Epoch 454, Loss:0.008984201565184258\n",
            "Epoch 455, Loss:0.008980792268364735\n",
            "Epoch 456, Loss:0.008977386617346983\n",
            "Epoch 457, Loss:0.008973984605729901\n",
            "Epoch 458, Loss:0.00897058622712782\n",
            "Epoch 459, Loss:0.008967191475170944\n",
            "Epoch 460, Loss:0.008963800343504691\n",
            "Epoch 461, Loss:0.008960412825790243\n",
            "Epoch 462, Loss:0.00895702891570399\n",
            "Epoch 463, Loss:0.008953648606937814\n",
            "Epoch 464, Loss:0.008950271893198969\n",
            "Epoch 465, Loss:0.008946898768209921\n",
            "Epoch 466, Loss:0.008943529225708402\n",
            "Epoch 467, Loss:0.008940163259447242\n",
            "Epoch 468, Loss:0.00893680086319465\n",
            "Epoch 469, Loss:0.008933442030733707\n",
            "Epoch 470, Loss:0.008930086755862654\n",
            "Epoch 471, Loss:0.008926735032394717\n",
            "Epoch 472, Loss:0.008923386854158055\n",
            "Epoch 473, Loss:0.00892004221499576\n",
            "Epoch 474, Loss:0.008916701108765801\n",
            "Epoch 475, Loss:0.008913363529340912\n",
            "Epoch 476, Loss:0.00891002947060876\n",
            "Epoch 477, Loss:0.008906698926471501\n",
            "Epoch 478, Loss:0.008903371890846168\n",
            "Epoch 479, Loss:0.00890004835766428\n",
            "Epoch 480, Loss:0.008896728320872127\n",
            "Epoch 481, Loss:0.008893411774430426\n",
            "Epoch 482, Loss:0.008890098712314443\n",
            "Epoch 483, Loss:0.008886789128513817\n",
            "Epoch 484, Loss:0.008883483017032678\n",
            "Epoch 485, Loss:0.008880180371889534\n",
            "Epoch 486, Loss:0.008876881187117214\n",
            "Epoch 487, Loss:0.008873585456762756\n",
            "Epoch 488, Loss:0.008870293174887522\n",
            "Epoch 489, Loss:0.008867004335567086\n",
            "Epoch 490, Loss:0.008863718932891063\n",
            "Epoch 491, Loss:0.00886043696096328\n",
            "Epoch 492, Loss:0.0088571584139015\n",
            "Epoch 493, Loss:0.008853883285837694\n",
            "Epoch 494, Loss:0.00885061157091771\n",
            "Epoch 495, Loss:0.008847343263301277\n",
            "Epoch 496, Loss:0.00884407835716211\n",
            "Epoch 497, Loss:0.008840816846687749\n",
            "Epoch 498, Loss:0.008837558726079497\n",
            "Epoch 499, Loss:0.008834303989552483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X, Y)\n",
        "mod.eval(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz2Q6eSC82cI",
        "outputId": "f670d299-705e-4dbb-ed07-6b41fb796c71"
      },
      "id": "Iz2Q6eSC82cI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]] [[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.98233139],\n",
              "       [0.98233139],\n",
              "       [0.98233139],\n",
              "       [0.98233139]])"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Flatten images to 1D vector of 784 features (28*28)\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "def test(model, X, Y, batch_size):\n",
        "  k = int(len(X)/batch_size)\n",
        "  for i in range(k):\n",
        "    Y_hat=model.eval(X[i*batch_size:(i+1)*batch_size])\n",
        "    wrong=0\n",
        "    for j in range(batch_size):\n",
        "      max1,max2,id1,id2=-999,-999,-1,-1\n",
        "      for l in range(10):\n",
        "        if max1 < Y_hat[j][l]:\n",
        "          max1,id1=Y_hat[j][l],l\n",
        "        if max2 < Y[i*batch_size+j][l]:\n",
        "          max2,id2=Y[i*batch_size+j][l],l\n",
        "      if id1 != id2: wrong+=1\n",
        "    print(f\"batch: {i}, accuracy: {(batch_size-wrong)/batch_size*100}%\")\n",
        "mod2 = Model(784, 10)\n",
        "mod2.addLayer(32, mod2.Input_layer, mod2.Output_layer)"
      ],
      "metadata": {
        "id": "lcOYUuYAjXPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e8461d-b0b3-49bc-cd05-53b6477f66b5"
      },
      "id": "lcOYUuYAjXPG",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.Model.Neuron at 0x7dddb4b03b10>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7290>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7350>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7590>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7610>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c76d0>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7750>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c77d0>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7850>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7690>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7910>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7990>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7a10>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7a90>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7b10>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7b90>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7c10>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7c90>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7d10>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7d90>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7e10>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7e90>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7f10>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0c7f90>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0d0050>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0d00d0>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0d0150>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0d01d0>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0d0250>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0d02d0>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0d0350>,\n",
              " <__main__.Model.Neuron at 0x7ddd9b0d03d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(mod2.Input_layer))\n",
        "mod2.train(x_train[0:1024], y_train[0:1024], 32, 10, 0.01)\n",
        "for i in range(1,9):\n",
        "  plt.subplot(330+i)\n",
        "  plt.imshow(x_test[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
        "print(mod2.eval(np.array([x_test[5]])))\n",
        "print(y_test[5])\n",
        "test(mod2, x_test, y_test, 500)"
      ],
      "metadata": {
        "id": "ClmATYl3Qcb7",
        "outputId": "06a01aba-8536-4a05-ef16-7b45c57d611c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "ClmATYl3Qcb7",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n",
            "Epoch 0/10, Loss:0.030306411273519223\n",
            "Epoch 1/10, Loss:0.027844793537530728\n",
            "Epoch 2/10, Loss:0.026543731328158553\n",
            "Epoch 3/10, Loss:0.02529161081348907\n",
            "Epoch 4/10, Loss:0.024208623470901833\n",
            "Epoch 5/10, Loss:0.023377935757644738\n",
            "Epoch 6/10, Loss:0.02251910347233744\n",
            "Epoch 7/10, Loss:0.021511433499755515\n",
            "Epoch 8/10, Loss:0.02078484056665236\n",
            "Epoch 9/10, Loss:0.02036742728885913\n",
            "[[-0.04050386  0.66182411  0.15279259  0.01910416 -0.04179198  0.05049054\n",
            "  -0.07468635  0.13628532  0.06616975  0.01633942]]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "batch: 0, accuracy: 56.8%\n",
            "batch: 1, accuracy: 53.400000000000006%\n",
            "batch: 2, accuracy: 55.00000000000001%\n",
            "batch: 3, accuracy: 54.0%\n",
            "batch: 4, accuracy: 54.2%\n",
            "batch: 5, accuracy: 54.2%\n",
            "batch: 6, accuracy: 59.199999999999996%\n",
            "batch: 7, accuracy: 52.800000000000004%\n",
            "batch: 8, accuracy: 51.0%\n",
            "batch: 9, accuracy: 53.800000000000004%\n",
            "batch: 10, accuracy: 68.2%\n",
            "batch: 11, accuracy: 63.0%\n",
            "batch: 12, accuracy: 70.6%\n",
            "batch: 13, accuracy: 62.6%\n",
            "batch: 14, accuracy: 67.4%\n",
            "batch: 15, accuracy: 67.4%\n",
            "batch: 16, accuracy: 71.0%\n",
            "batch: 17, accuracy: 74.8%\n",
            "batch: 18, accuracy: 66.0%\n",
            "batch: 19, accuracy: 67.0%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(mod2, x_test, y_test, 5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhe7AJ7MHY5E",
        "outputId": "4d9cec1c-ebf0-40e2-e5d1-2f704da2ecf8"
      },
      "id": "Bhe7AJ7MHY5E",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch: 0, accuracy: 54.44%\n",
            "batch: 1, accuracy: 67.80000000000001%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the model here aim to some how echo back propagation"
      ],
      "metadata": {
        "id": "sctEXJvN9A8-"
      },
      "id": "sctEXJvN9A8-"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qI3dFD_19LlF"
      },
      "id": "qI3dFD_19LlF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}