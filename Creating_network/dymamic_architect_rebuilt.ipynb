{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diputs03/AI-Studies/blob/main/Creating_network/dymamic_architect_rebuilt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Aiming a Dynaimic Graph-structured NeuronNetwork\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import deque\n",
        "import multiprocessing as mp"
      ],
      "metadata": {
        "id": "6mXjwpToZTeV"
      },
      "id": "6mXjwpToZTeV",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Activation function in this case in \\tanh, thus\n",
        "\\dfrac{d\\tanh(x)}{dx}=1-\\tanh^2(x)\n",
        "however, for other activation funtions\n",
        "\\dfrac{d\\sigma(x)}{dx}=\\sigma(x)\\cdot\\left\\big(1-\\sigma(x)\\right\\big)\n",
        "\\dfrac{d\\mathop{\\mathrm{ReLu}}(x)}{dx}=\\begin{cases}1&x\\ge0\\\\0&\\text{else}\\end{cases}\n",
        "Loss is the Euclidean loss\n",
        "\\dfrac{d\\L}\n",
        "\"\"\"\n",
        "class Model:\n",
        "  class Neuron:\n",
        "    def __init__(self, name, prev, next):\n",
        "      self.name, self.prev, self.next = name, prev, next\n",
        "      self.bias = np.random.uniform(-0.1, 0.1)\n",
        "\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.Input_layer = [Model.Neuron(f\"input{i}\", [], []) for i in range(input_size)]\n",
        "    self.Output_layer = [Model.Neuron(f\"output{o}\", [], []) for o in range(output_size)]\n",
        "    for i in range(input_size): self.Input_layer[i].next = self.Output_layer\n",
        "    for o in range(output_size): self.Output_layer[o].prev = self.Input_layer\n",
        "    self.weight = {}\n",
        "    self.weight_grad_sum, self.weight_grad_sqr = {}, {}\n",
        "    for u in self.Input_layer:\n",
        "      for v in self.Output_layer:\n",
        "        self.weight[(u,v)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_grad_sum[(u,v)], self.weight_grad_sqr[(u,v)] = 0, 0\n",
        "    self.all_neurons = self.Input_layer + self.Output_layer\n",
        "\n",
        "    self.bias_grad_sum, self.bias_grad_sqr = {}, {}\n",
        "    for p in self.all_neurons:\n",
        "      self.bias_grad_sum[p], self.bias_grad_sqr[p] = 0, 0\n",
        "\n",
        "  def forward(self, X, batch_size):\n",
        "    assert X.shape == (batch_size,len(self.Input_layer))\n",
        "    a = {q: np.zeros(batch_size) for q in self.all_neurons}\n",
        "\n",
        "    for i, n in enumerate(self.Input_layer):\n",
        "      a[n] = X[:, i]\n",
        "\n",
        "    q = deque()\n",
        "    for i in self.Input_layer:\n",
        "      q.append(i)\n",
        "\n",
        "    cnt = {q: 0 for q in self.all_neurons}\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      a[c] = np.tanh(a[c] + c.bias)\n",
        "      for n in c.next:\n",
        "        a[n] = a[n] + a[c] * self.weight[(c,n)]\n",
        "        cnt[n] += 1\n",
        "        if cnt[n] == len(n.prev):\n",
        "          q.append(n)\n",
        "    return a\n",
        "\n",
        "  def evaluate(self, X):\n",
        "    a = self.forward(X, len(X))\n",
        "    return np.array([a[o] for o in self.Output_layer]).T\n",
        "\n",
        "  def backward(self, X, Y, batch_size, learning_rate, decay_sum, decay_sqr):\n",
        "    assert X.shape == (batch_size,len(self.Input_layer))\n",
        "    assert Y.shape == (batch_size,len(self.Output_layer))\n",
        "    a = self.forward(X, batch_size)\n",
        "\n",
        "    delta_b, delta_w = {}, {}\n",
        "\n",
        "    par_a = {q: np.zeros(batch_size) for q in self.all_neurons}\n",
        "    for o, n in enumerate(self.Output_layer):\n",
        "      par_a[n] = 2 * (a[n] - Y[:, o])\n",
        "\n",
        "    q = deque()\n",
        "    for o in self.Output_layer:\n",
        "      q.append(o)\n",
        "\n",
        "    cnt = {q: 0 for q in self.all_neurons}\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      par_b = par_a[c] * (1-a[c]**2)\n",
        "\n",
        "      grad_bias = par_b\n",
        "      self.bias_grad_sum[c] = \\\n",
        "       (1-decay_sum)*np.sum(grad_bias)/batch_size + decay_sum*self.bias_grad_sum[c]\n",
        "      self.bias_grad_sqr[c] = \\\n",
        "       (1-decay_sqr)*np.sum(grad_bias**2)/batch_size + decay_sqr*self.bias_grad_sqr[c]\n",
        "      delta_b[c] = \\\n",
        "       -learning_rate * self.bias_grad_sum[c] / (self.bias_grad_sqr[c]**(1/2)+1)\n",
        "\n",
        "      for p in c.prev:\n",
        "        par_a[p] += par_a[c] * (1-a[c]**2) * self.weight[(p,c)]\n",
        "        grad_weight = par_a[c] * (1-a[c]**2) * a[p]\n",
        "        self.weight_grad_sum[(p,c)] = \\\n",
        "         (1-decay_sum)*np.sum(grad_weight)/batch_size + decay_sum*self.weight_grad_sum[(p,c)]\n",
        "        self.weight_grad_sqr[(p,c)] = \\\n",
        "         (1-decay_sqr)*np.sum(grad_weight**2)/batch_size + decay_sqr*self.weight_grad_sqr[(p,c)]\n",
        "        delta_w[(p,c)] = \\\n",
        "         -learning_rate * self.weight_grad_sum[(p,c)] / (self.weight_grad_sqr[(p,c)]**(1/2)+1)\n",
        "\n",
        "        cnt[p] += 1\n",
        "        if cnt[p] == len(p.next):\n",
        "          q.append(p)\n",
        "\n",
        "    return delta_w, delta_b\n",
        "\n",
        "  def update(self, X, Y, batch_size, learning_rate, decay_sum=0.9, decay_sqr=0.9):\n",
        "    delta_w, delta_b = \\\n",
        "     self.backward(X, Y, batch_size, learning_rate, decay_sum, decay_sqr)\n",
        "    for (u,v), w in self.weight.items():\n",
        "        self.weight[(u,v)] += delta_w[(u,v)]\n",
        "    for p in self.all_neurons:\n",
        "        p.bias += delta_b[p]\n",
        "\n",
        "  def addLayer(self, mid_size, UP, DOWN):\n",
        "    Mid_layer = [Model.Neuron(f\"mid{o}\", [], []) for o in range(mid_size)]\n",
        "    for m, mid in enumerate(Mid_layer):\n",
        "      self.bias_grad_sum[mid], self.bias_grad_sqr[mid] = 0, 0\n",
        "      Mid_layer[m].prev = UP\n",
        "      for u in UP:\n",
        "        self.weight[(u,mid)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_grad_sum[(u,mid)] = 0\n",
        "        self.weight_grad_sqr[(u,mid)] = 0\n",
        "      Mid_layer[m].next = DOWN\n",
        "      for v in DOWN:\n",
        "        self.weight[(mid,v)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_grad_sum[(mid,v)] = 0\n",
        "        self.weight_grad_sqr[(mid,v)] = 0\n",
        "    for u in UP:\n",
        "      u.next = Mid_layer\n",
        "    for v in DOWN:\n",
        "      v.prev = Mid_layer\n",
        "    for u in UP:\n",
        "      for v in DOWN:\n",
        "        self.weight.pop((u, v))\n",
        "        self.weight_grad_sum.pop((u, v))\n",
        "        self.weight_grad_sqr.pop((u, v))\n",
        "    self.all_neurons += Mid_layer\n",
        "    return Mid_layer\n",
        "\n",
        "  def train(self, X, Y, batch_size, epochs, learning_rate):\n",
        "    l = len(X)\n",
        "    for epoch in range(epochs):\n",
        "      data=[(X[_], Y[_]) for _ in range(len(X))]\n",
        "      random.shuffle(data)\n",
        "      for _ in range(len(X)):\n",
        "        X[_],Y[_]=data[_]\n",
        "      loss = 0\n",
        "      for batch in range(int(l / batch_size)):\n",
        "        L, R = batch * batch_size, (batch + 1) * batch_size\n",
        "        x_train, y_train = X[L:R], Y[L:R]\n",
        "        self.update(x_train, y_train, batch_size, learning_rate)\n",
        "        output = self.evaluate(x_train)\n",
        "        loss += np.sum(((y_train - output) ** 2), axis=(0,1))\n",
        "      loss = ((loss) ** 0.5) / (int(l / batch_size) * batch_size)\n",
        "      print(f\"Epoch {epoch}/{epochs}, Loss:{loss}\")"
      ],
      "metadata": {
        "id": "YUliU5Cx-oyU"
      },
      "id": "YUliU5Cx-oyU",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "Y=np.array([[0],[1],[1],[0]])\n",
        "mod=Model(2, 1)\n",
        "mid1=mod.addLayer(4, mod.Input_layer, mod.Output_layer)\n",
        "mod.addLayer(4, mid1, mod.Output_layer)\n",
        "mod.evaluate(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfiQiE5l8XOr",
        "outputId": "fde63832-8796-485b-9299-90440b5d5f8a"
      },
      "id": "KfiQiE5l8XOr",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.08374906],\n",
              "       [0.08421259],\n",
              "       [0.08324875],\n",
              "       [0.08371121]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod.train(X, Y, 4, 500, 0.1)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC2n_sC_8xnP",
        "outputId": "353c8524-ee39-4d06-f13d-1efdb0ac96f9"
      },
      "id": "aC2n_sC_8xnP",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/500, Loss:0.3235096792489062\n",
            "Epoch 1/500, Loss:0.38940287036996685\n",
            "Epoch 2/500, Loss:0.31437984709720385\n",
            "Epoch 3/500, Loss:0.37420955982640985\n",
            "Epoch 4/500, Loss:0.30187930392481743\n",
            "Epoch 5/500, Loss:0.09016971223149087\n",
            "Epoch 6/500, Loss:0.09752100802246183\n",
            "Epoch 7/500, Loss:0.10313059732982595\n",
            "Epoch 8/500, Loss:0.10708290636532275\n",
            "Epoch 9/500, Loss:0.10947287155667995\n",
            "Epoch 10/500, Loss:0.11040248254257586\n",
            "Epoch 11/500, Loss:0.10997822099739735\n",
            "Epoch 12/500, Loss:0.1083093039855433\n",
            "Epoch 13/500, Loss:0.10550658606051896\n",
            "Epoch 14/500, Loss:0.10168195218402701\n",
            "Epoch 15/500, Loss:0.09694803460060183\n",
            "Epoch 16/500, Loss:0.09141810236906037\n",
            "Epoch 17/500, Loss:0.0852059954951964\n",
            "Epoch 18/500, Loss:0.07842600189312043\n",
            "Epoch 19/500, Loss:0.07119260197877514\n",
            "Epoch 20/500, Loss:0.06362003116991631\n",
            "Epoch 21/500, Loss:0.055821634331698566\n",
            "Epoch 22/500, Loss:0.047909008021575925\n",
            "Epoch 23/500, Loss:0.03999094606892238\n",
            "Epoch 24/500, Loss:0.03217222130418856\n",
            "Epoch 25/500, Loss:0.02455225074797687\n",
            "Epoch 26/500, Loss:0.01722370284968932\n",
            "Epoch 27/500, Loss:0.010271113043867233\n",
            "Epoch 28/500, Loss:0.003769577713454424\n",
            "Epoch 29/500, Loss:0.002216403503618563\n",
            "Epoch 30/500, Loss:0.00763387130500809\n",
            "Epoch 31/500, Loss:0.012442078754367267\n",
            "Epoch 32/500, Loss:0.0166128469274834\n",
            "Epoch 33/500, Loss:0.020130576474494077\n",
            "Epoch 34/500, Loss:0.022991908706800283\n",
            "Epoch 35/500, Loss:0.02520505396507652\n",
            "Epoch 36/500, Loss:0.026788829275256915\n",
            "Epoch 37/500, Loss:0.027771467809255674\n",
            "Epoch 38/500, Loss:0.028189275559725807\n",
            "Epoch 39/500, Loss:0.028085213463526108\n",
            "Epoch 40/500, Loss:0.027507475998571496\n",
            "Epoch 41/500, Loss:0.026508122559220807\n",
            "Epoch 42/500, Loss:0.025141799679643223\n",
            "Epoch 43/500, Loss:0.023464574431724297\n",
            "Epoch 44/500, Loss:0.02153288495897659\n",
            "Epoch 45/500, Loss:0.019402604443054645\n",
            "Epoch 46/500, Loss:0.01712820979123163\n",
            "Epoch 47/500, Loss:0.014762045087509537\n",
            "Epoch 48/500, Loss:0.012353671150137264\n",
            "Epoch 49/500, Loss:0.009949295217588876\n",
            "Epoch 50/500, Loss:0.007591277914379973\n",
            "Epoch 51/500, Loss:0.005317717581242529\n",
            "Epoch 52/500, Loss:0.003162114392273041\n",
            "Epoch 53/500, Loss:0.0011531182058534034\n",
            "Epoch 54/500, Loss:0.0006856353006994924\n",
            "Epoch 55/500, Loss:0.002335596027833802\n",
            "Epoch 56/500, Loss:0.003783275542607279\n",
            "Epoch 57/500, Loss:0.0050201273180418456\n",
            "Epoch 58/500, Loss:0.006042333723847121\n",
            "Epoch 59/500, Loss:0.006850506120085477\n",
            "Epoch 60/500, Loss:0.007449308363273989\n",
            "Epoch 61/500, Loss:0.007847017656575733\n",
            "Epoch 62/500, Loss:0.008055039379232916\n",
            "Epoch 63/500, Loss:0.008087393853109654\n",
            "Epoch 64/500, Loss:0.007960192756436744\n",
            "Epoch 65/500, Loss:0.0076911212236134546\n",
            "Epoch 66/500, Loss:0.007298938999868839\n",
            "Epoch 67/500, Loss:0.0068030109022022885\n",
            "Epoch 68/500, Loss:0.006222873784740554\n",
            "Epoch 69/500, Loss:0.005577844574499406\n",
            "Epoch 70/500, Loss:0.004886671899991336\n",
            "Epoch 71/500, Loss:0.00416723238690859\n",
            "Epoch 72/500, Loss:0.0034362717470282386\n",
            "Epoch 73/500, Loss:0.0027091902001183917\n",
            "Epoch 74/500, Loss:0.0019998714050329723\n",
            "Epoch 75/500, Loss:0.0013205538194436355\n",
            "Epoch 76/500, Loss:0.0006817431732448261\n",
            "Epoch 77/500, Loss:9.216447461347616e-05\n",
            "Epoch 78/500, Loss:0.0004412483583300323\n",
            "Epoch 79/500, Loss:0.0009133275415493315\n",
            "Epoch 80/500, Loss:0.0013206139007814115\n",
            "Epoch 81/500, Loss:0.001661270028408015\n",
            "Epoch 82/500, Loss:0.0019349678688325611\n",
            "Epoch 83/500, Loss:0.0021427549354312556\n",
            "Epoch 84/500, Loss:0.0022869037220218337\n",
            "Epoch 85/500, Loss:0.002370749083117158\n",
            "Epoch 86/500, Loss:0.0023985183805049592\n",
            "Epoch 87/500, Loss:0.0023751590140956096\n",
            "Epoch 88/500, Loss:0.0023061675880574747\n",
            "Epoch 89/500, Loss:0.002197424451605184\n",
            "Epoch 90/500, Loss:0.002055036755735208\n",
            "Epoch 91/500, Loss:0.0018851925422704791\n",
            "Epoch 92/500, Loss:0.0016940277785292166\n",
            "Epoch 93/500, Loss:0.0014875077015218785\n",
            "Epoch 94/500, Loss:0.0012713233544144966\n",
            "Epoch 95/500, Loss:0.001050803786397673\n",
            "Epoch 96/500, Loss:0.0008308440383405906\n",
            "Epoch 97/500, Loss:0.0006158487406191738\n",
            "Epoch 98/500, Loss:0.00040969089616462763\n",
            "Epoch 99/500, Loss:0.0002156852027111647\n",
            "Epoch 100/500, Loss:3.657507746750233e-05\n",
            "Epoch 101/500, Loss:0.00012546761859614667\n",
            "Epoch 102/500, Loss:0.00026883130254791465\n",
            "Epoch 103/500, Loss:0.00039244310284090487\n",
            "Epoch 104/500, Loss:0.0004957383650844389\n",
            "Epoch 105/500, Loss:0.0005786226601068629\n",
            "Epoch 106/500, Loss:0.0006414277641222874\n",
            "Epoch 107/500, Loss:0.0006848630890998342\n",
            "Epoch 108/500, Loss:0.0007099640161894531\n",
            "Epoch 109/500, Loss:0.0007180385260154916\n",
            "Epoch 110/500, Loss:0.0007106134288377538\n",
            "Epoch 111/500, Loss:0.0006893813795408728\n",
            "Epoch 112/500, Loss:0.0006561497237580892\n",
            "Epoch 113/500, Loss:0.0006127920698690796\n",
            "Epoch 114/500, Loss:0.0005612033247544473\n",
            "Epoch 115/500, Loss:0.0005032587755993581\n",
            "Epoch 116/500, Loss:0.0004407776506680315\n",
            "Epoch 117/500, Loss:0.0003754914520951964\n",
            "Epoch 118/500, Loss:0.00030901722521907334\n",
            "Epoch 119/500, Loss:0.0002428358126614741\n",
            "Epoch 120/500, Loss:0.0001782750374773251\n",
            "Epoch 121/500, Loss:0.00011649766816942827\n",
            "Epoch 122/500, Loss:5.84939389936009e-05\n",
            "Epoch 123/500, Loss:5.078331534502632e-06\n",
            "Epoch 124/500, Loss:4.310973221557803e-05\n",
            "Epoch 125/500, Loss:8.560167932808888e-05\n",
            "Epoch 126/500, Loss:0.00012209248509233316\n",
            "Epoch 127/500, Loss:0.0001524307311570669\n",
            "Epoch 128/500, Loss:0.00017660648358701385\n",
            "Epoch 129/500, Loss:0.00019473745327492844\n",
            "Epoch 130/500, Loss:0.0002070538931072047\n",
            "Epoch 131/500, Loss:0.00021388266939202134\n",
            "Epoch 132/500, Loss:0.00021563092023065556\n",
            "Epoch 133/500, Loss:0.00021276968186942537\n",
            "Epoch 134/500, Loss:0.00020581782693502784\n",
            "Epoch 135/500, Loss:0.00019532661728611944\n",
            "Epoch 136/500, Loss:0.0001818651304998271\n",
            "Epoch 137/500, Loss:0.00016600677417309364\n",
            "Epoch 138/500, Loss:0.00014831705751896192\n",
            "Epoch 139/500, Loss:0.0001293427462356829\n",
            "Epoch 140/500, Loss:0.00010960248516516022\n",
            "Epoch 141/500, Loss:8.95789344810155e-05\n",
            "Epoch 142/500, Loss:6.971242953135552e-05\n",
            "Epoch 143/500, Loss:5.039614234645863e-05\n",
            "Epoch 144/500, Loss:3.197269443678266e-05\n",
            "Epoch 145/500, Loss:1.4732145991775773e-05\n",
            "Epoch 146/500, Loss:1.0887339917447853e-06\n",
            "Epoch 147/500, Loss:1.5306028776299386e-05\n",
            "Epoch 148/500, Loss:2.778719106124572e-05\n",
            "Epoch 149/500, Loss:3.8448650237803734e-05\n",
            "Epoch 150/500, Loss:4.725264917169116e-05\n",
            "Epoch 151/500, Loss:5.420345209671086e-05\n",
            "Epoch 152/500, Loss:5.93430642429122e-05\n",
            "Epoch 153/500, Loss:6.274660006305732e-05\n",
            "Epoch 154/500, Loss:6.451743068511262e-05\n",
            "Epoch 155/500, Loss:6.478223285802163e-05\n",
            "Epoch 156/500, Loss:6.36860515458664e-05\n",
            "Epoch 157/500, Loss:6.13874768472668e-05\n",
            "Epoch 158/500, Loss:5.8054023459163394e-05\n",
            "Epoch 159/500, Loss:5.3857787842003234e-05\n",
            "Epoch 160/500, Loss:4.897144492797829e-05\n",
            "Epoch 161/500, Loss:4.356463296290279e-05\n",
            "Epoch 162/500, Loss:3.780076216399659e-05\n",
            "Epoch 163/500, Loss:3.183427054656372e-05\n",
            "Epoch 164/500, Loss:2.5808338717684344e-05\n",
            "Epoch 165/500, Loss:1.9853064810316833e-05\n",
            "Epoch 166/500, Loss:1.4084091155909682e-05\n",
            "Epoch 167/500, Loss:8.601665853291928e-06\n",
            "Epoch 168/500, Loss:3.490115140741344e-06\n",
            "Epoch 169/500, Loss:1.1823035573408666e-06\n",
            "Epoch 170/500, Loss:5.363202857551795e-06\n",
            "Epoch 171/500, Loss:9.015559494418947e-06\n",
            "Epoch 172/500, Loss:1.2116942302587688e-05\n",
            "Epoch 173/500, Loss:1.4658516552922848e-05\n",
            "Epoch 174/500, Loss:1.664386747575322e-05\n",
            "Epoch 175/500, Loss:1.808768527002014e-05\n",
            "Epoch 176/500, Loss:1.9014352543665828e-05\n",
            "Epoch 177/500, Loss:1.9456473064996885e-05\n",
            "Epoch 178/500, Loss:1.945337803919605e-05\n",
            "Epoch 179/500, Loss:1.904964297261782e-05\n",
            "Epoch 180/500, Loss:1.8293644664089363e-05\n",
            "Epoch 181/500, Loss:1.7236184078139717e-05\n",
            "Epoch 182/500, Loss:1.5929196915892676e-05\n",
            "Epoch 183/500, Loss:1.4424569703523647e-05\n",
            "Epoch 184/500, Loss:1.277307525545312e-05\n",
            "Epoch 185/500, Loss:1.1023437518940407e-05\n",
            "Epoch 186/500, Loss:9.221532136892119e-06\n",
            "Epoch 187/500, Loss:7.409725634220237e-06\n",
            "Epoch 188/500, Loss:5.626352986197206e-06\n",
            "Epoch 189/500, Loss:3.905330500917607e-06\n",
            "Epoch 190/500, Loss:2.2758984678680747e-06\n",
            "Epoch 191/500, Loss:7.624859065783427e-07\n",
            "Epoch 192/500, Loss:6.153119993087072e-07\n",
            "Epoch 193/500, Loss:1.8426545778299343e-06\n",
            "Epoch 194/500, Loss:2.9092862703961287e-06\n",
            "Epoch 195/500, Loss:3.809289651373241e-06\n",
            "Epoch 196/500, Loss:4.5407737090343345e-06\n",
            "Epoch 197/500, Loss:5.105510283325985e-06\n",
            "Epoch 198/500, Loss:5.508531336977054e-06\n",
            "Epoch 199/500, Loss:5.757699278041257e-06\n",
            "Epoch 200/500, Loss:5.863261887139754e-06\n",
            "Epoch 201/500, Loss:5.837402565709988e-06\n",
            "Epoch 202/500, Loss:5.693795646877779e-06\n",
            "Epoch 203/500, Loss:5.447175432099444e-06\n",
            "Epoch 204/500, Loss:5.112926467097335e-06\n",
            "Epoch 205/500, Loss:4.706701381007435e-06\n",
            "Epoch 206/500, Loss:4.24407141143972e-06\n",
            "Epoch 207/500, Loss:3.740213551684114e-06\n",
            "Epoch 208/500, Loss:3.2096371073519445e-06\n",
            "Epoch 209/500, Loss:2.665951358479084e-06\n",
            "Epoch 210/500, Loss:2.121675006120613e-06\n",
            "Epoch 211/500, Loss:1.5880871532869053e-06\n",
            "Epoch 212/500, Loss:1.0751187390669548e-06\n",
            "Epoch 213/500, Loss:5.912826191329884e-07\n",
            "Epoch 214/500, Loss:1.436398699870447e-07\n",
            "Epoch 215/500, Loss:2.622006100602289e-07\n",
            "Epoch 216/500, Loss:6.220525238790579e-07\n",
            "Epoch 217/500, Loss:9.330962290204664e-07\n",
            "Epoch 218/500, Loss:1.1938002114295983e-06\n",
            "Epoch 219/500, Loss:1.4038238556224733e-06\n",
            "Epoch 220/500, Loss:1.563905385766895e-06\n",
            "Epoch 221/500, Loss:1.6757387705194103e-06\n",
            "Epoch 222/500, Loss:1.7418432328696889e-06\n",
            "Epoch 223/500, Loss:1.7654287946707397e-06\n",
            "Epoch 224/500, Loss:1.7502610244418836e-06\n",
            "Epoch 225/500, Loss:1.7005278565668773e-06\n",
            "Epoch 226/500, Loss:1.6207110205730296e-06\n",
            "Epoch 227/500, Loss:1.5154642701624852e-06\n",
            "Epoch 228/500, Loss:1.3895002425500444e-06\n",
            "Epoch 229/500, Loss:1.2474874175744246e-06\n",
            "Epoch 230/500, Loss:1.093958290915981e-06\n",
            "Epoch 231/500, Loss:9.332295330649181e-07\n",
            "Epoch 232/500, Loss:7.693345814455653e-07\n",
            "Epoch 233/500, Loss:6.059688120136994e-07\n",
            "Epoch 234/500, Loss:4.464471627427795e-07\n",
            "Epoch 235/500, Loss:2.9367383742002437e-07\n",
            "Epoch 236/500, Loss:1.501235063643958e-07\n",
            "Epoch 237/500, Loss:1.7833241974709445e-08\n",
            "Epoch 238/500, Loss:1.015957180544924e-07\n",
            "Epoch 239/500, Loss:2.0698739888572386e-07\n",
            "Epoch 240/500, Loss:2.975728068173654e-07\n",
            "Epoch 241/500, Loss:3.729650855112227e-07\n",
            "Epoch 242/500, Loss:4.3312927847951037e-07\n",
            "Epoch 243/500, Loss:4.783478605691336e-07\n",
            "Epoch 244/500, Loss:5.091831721019758e-07\n",
            "Epoch 245/500, Loss:5.26437839859485e-07\n",
            "Epoch 246/500, Loss:5.311142021768245e-07\n",
            "Epoch 247/500, Loss:5.243736742419749e-07\n",
            "Epoch 248/500, Loss:5.074968972894846e-07\n",
            "Epoch 249/500, Loss:4.818454148903806e-07\n",
            "Epoch 250/500, Loss:4.4882551369775875e-07\n",
            "Epoch 251/500, Loss:4.098547576567494e-07\n",
            "Epoch 252/500, Loss:3.6633163628901937e-07\n",
            "Epoch 253/500, Loss:3.1960864132515677e-07\n",
            "Epoch 254/500, Loss:2.7096898393616285e-07\n",
            "Epoch 255/500, Loss:2.216070684752374e-07\n",
            "Epoch 256/500, Loss:1.726127496280463e-07\n",
            "Epoch 257/500, Loss:1.2495931919287525e-07\n",
            "Epoch 258/500, Loss:7.949509718291963e-08\n",
            "Epoch 259/500, Loss:3.693844018333441e-08\n",
            "Epoch 260/500, Loss:2.1240718546013115e-09\n",
            "Epoch 261/500, Loss:3.723655083191425e-08\n",
            "Epoch 262/500, Loss:6.80701298951497e-08\n",
            "Epoch 263/500, Loss:9.441704125175378e-08\n",
            "Epoch 264/500, Loss:1.1618278964505498e-07\n",
            "Epoch 265/500, Loss:1.333767732877303e-07\n",
            "Epoch 266/500, Loss:1.4610170082692276e-07\n",
            "Epoch 267/500, Loss:1.5454214295773157e-07\n",
            "Epoch 268/500, Loss:1.589525411970579e-07\n",
            "Epoch 269/500, Loss:1.596449753361645e-07\n",
            "Epoch 270/500, Loss:1.5697696588998053e-07\n",
            "Epoch 271/500, Loss:1.5133955947850436e-07\n",
            "Epoch 272/500, Loss:1.4314591451937028e-07\n",
            "Epoch 273/500, Loss:1.3282057247402564e-07\n",
            "Epoch 274/500, Loss:1.2078956731446003e-07\n",
            "Epoch 275/500, Loss:1.0747149325595313e-07\n",
            "Epoch 276/500, Loss:9.326961909576663e-08\n",
            "Epoch 277/500, Loss:7.856510705894705e-08\n",
            "Epoch 278/500, Loss:6.371136550319436e-08\n",
            "Epoch 279/500, Loss:4.902953851812828e-08\n",
            "Epoch 280/500, Loss:3.480511170765582e-08\n",
            "Epoch 281/500, Loss:2.1285592529448464e-08\n",
            "Epoch 282/500, Loss:8.679205539593308e-09\n",
            "Epoch 283/500, Loss:2.845472034211649e-09\n",
            "Epoch 284/500, Loss:1.3159020921960776e-08\n",
            "Epoch 285/500, Loss:2.2169932107143895e-08\n",
            "Epoch 286/500, Loss:2.982270233368996e-08\n",
            "Epoch 287/500, Loss:3.60953777895668e-08\n",
            "Epoch 288/500, Loss:4.0996651763051163e-08\n",
            "Epoch 289/500, Loss:4.456262071459912e-08\n",
            "Epoch 290/500, Loss:4.6853299816729816e-08\n",
            "Epoch 291/500, Loss:4.794899382100702e-08\n",
            "Epoch 292/500, Loss:4.794661256305578e-08\n",
            "Epoch 293/500, Loss:4.695601258167246e-08\n",
            "Epoch 294/500, Loss:4.509643767311779e-08\n",
            "Epoch 295/500, Loss:4.249312186279861e-08\n",
            "Epoch 296/500, Loss:3.9274108583342485e-08\n",
            "Epoch 297/500, Loss:3.556733001956047e-08\n",
            "Epoch 298/500, Loss:3.149798081299861e-08\n",
            "Epoch 299/500, Loss:2.718621082984163e-08\n",
            "Epoch 300/500, Loss:2.2745152648481742e-08\n",
            "Epoch 301/500, Loss:1.8279290984503365e-08\n",
            "Epoch 302/500, Loss:1.3883173423812758e-08\n",
            "Epoch 303/500, Loss:9.640454977713485e-09\n",
            "Epoch 304/500, Loss:5.6232627178819106e-09\n",
            "Epoch 305/500, Loss:1.8918616096451346e-09\n",
            "Epoch 306/500, Loss:1.5054016757123923e-09\n",
            "Epoch 307/500, Loss:4.53190365226272e-09\n",
            "Epoch 308/500, Loss:7.162326712186611e-09\n",
            "Epoch 309/500, Loss:9.38205048936838e-09\n",
            "Epoch 310/500, Loss:1.1186383542128444e-08\n",
            "Epoch 311/500, Loss:1.2579667179896805e-08\n",
            "Epoch 312/500, Loss:1.3574282690508723e-08\n",
            "Epoch 313/500, Loss:1.4189592091595756e-08\n",
            "Epoch 314/500, Loss:1.4450840893701073e-08\n",
            "Epoch 315/500, Loss:1.4388049299294405e-08\n",
            "Epoch 316/500, Loss:1.4034915838886938e-08\n",
            "Epoch 317/500, Loss:1.3427754818207818e-08\n",
            "Epoch 318/500, Loss:1.2604486093747976e-08\n",
            "Epoch 319/500, Loss:1.1603692767498532e-08\n",
            "Epoch 320/500, Loss:1.0463759443982144e-08\n",
            "Epoch 321/500, Loss:9.222100749717297e-09\n",
            "Epoch 322/500, Loss:7.914486995028862e-09\n",
            "Epoch 323/500, Loss:6.57447118100618e-09\n",
            "Epoch 324/500, Loss:5.232919000463382e-09\n",
            "Epoch 325/500, Loss:3.917641251769549e-09\n",
            "Epoch 326/500, Loss:2.653125987135685e-09\n",
            "Epoch 327/500, Loss:1.4603659431906835e-09\n",
            "Epoch 328/500, Loss:3.567752920353995e-10\n",
            "Epoch 329/500, Loss:6.438115062769312e-10\n",
            "Epoch 330/500, Loss:1.5310669446676195e-09\n",
            "Epoch 331/500, Loss:2.2980326825489117e-09\n",
            "Epoch 332/500, Loss:2.9409260820893735e-09\n",
            "Epoch 333/500, Loss:3.4589006173146863e-09\n",
            "Epoch 334/500, Loss:3.853769716972294e-09\n",
            "Epoch 335/500, Loss:4.129703383946859e-09\n",
            "Epoch 336/500, Loss:4.292906567553156e-09\n",
            "Epoch 337/500, Loss:4.3512877537232875e-09\n",
            "Epoch 338/500, Loss:4.3141255667671075e-09\n",
            "Epoch 339/500, Loss:4.191740469051269e-09\n",
            "Epoch 340/500, Loss:3.9951778196475884e-09\n",
            "Epoch 341/500, Loss:3.735907667425098e-09\n",
            "Epoch 342/500, Loss:3.4255458235613045e-09\n",
            "Epoch 343/500, Loss:3.0755998117228123e-09\n",
            "Epoch 344/500, Loss:2.6972424636666215e-09\n",
            "Epoch 345/500, Loss:2.3011150511106915e-09\n",
            "Epoch 346/500, Loss:1.8971610623620683e-09\n",
            "Epoch 347/500, Loss:1.4944909866934641e-09\n",
            "Epoch 348/500, Loss:1.1012777981211919e-09\n",
            "Epoch 349/500, Loss:7.246822100737127e-10\n",
            "Epoch 350/500, Loss:3.708062784775479e-10\n",
            "Epoch 351/500, Loss:4.4673474121348367e-11\n",
            "Epoch 352/500, Loss:2.497670299750343e-10\n",
            "Epoch 353/500, Loss:5.096143381420892e-10\n",
            "Epoch 354/500, Loss:7.32970811215955e-10\n",
            "Epoch 355/500, Loss:9.188808620247402e-10\n",
            "Epoch 356/500, Loss:1.067256446413145e-09\n",
            "Epoch 357/500, Loss:1.178792130313716e-09\n",
            "Epoch 358/500, Loss:1.254872509723337e-09\n",
            "Epoch 359/500, Loss:1.2974746559264727e-09\n",
            "Epoch 360/500, Loss:1.309068113457268e-09\n",
            "Epoch 361/500, Loss:1.2925147458396624e-09\n",
            "Epoch 362/500, Loss:1.2509704938601374e-09\n",
            "Epoch 363/500, Loss:1.1877909155376398e-09\n",
            "Epoch 364/500, Loss:1.1064420399851949e-09\n",
            "Epoch 365/500, Loss:1.010417867864516e-09\n",
            "Epoch 366/500, Loss:9.031655436551878e-10\n",
            "Epoch 367/500, Loss:7.880189751598154e-10\n",
            "Epoch 368/500, Loss:6.681414254326723e-10\n",
            "Epoch 369/500, Loss:5.464773629275388e-10\n",
            "Epoch 370/500, Loss:4.257136418557572e-10\n",
            "Epoch 371/500, Loss:3.0824986660404896e-10\n",
            "Epoch 372/500, Loss:1.9617765224799744e-10\n",
            "Epoch 373/500, Loss:9.126830020911303e-11\n",
            "Epoch 374/500, Loss:5.031684704309702e-12\n",
            "Epoch 375/500, Loss:9.159793973659291e-11\n",
            "Epoch 376/500, Loss:1.6761920323959956e-10\n",
            "Epoch 377/500, Loss:2.3258273865872647e-10\n",
            "Epoch 378/500, Loss:2.862550405047237e-10\n",
            "Epoch 379/500, Loss:3.286587382453354e-10\n",
            "Epoch 380/500, Loss:3.600465038958167e-10\n",
            "Epoch 381/500, Loss:3.808728475836187e-10\n",
            "Epoch 382/500, Loss:3.91764560896124e-10\n",
            "Epoch 383/500, Loss:3.934905647845488e-10\n",
            "Epoch 384/500, Loss:3.8693184463770014e-10\n",
            "Epoch 385/500, Loss:3.730520900871448e-10\n",
            "Epoch 386/500, Loss:3.5286954478200294e-10\n",
            "Epoch 387/500, Loss:3.274305670347699e-10\n",
            "Epoch 388/500, Loss:2.9778524524709526e-10\n",
            "Epoch 389/500, Loss:2.64965376462617e-10\n",
            "Epoch 390/500, Loss:2.2996502271888097e-10\n",
            "Epoch 391/500, Loss:1.9372379091511793e-10\n",
            "Epoch 392/500, Loss:1.5711291209002942e-10\n",
            "Epoch 393/500, Loss:1.2092411447173212e-10\n",
            "Epoch 394/500, Loss:8.586124866649669e-11\n",
            "Epoch 395/500, Loss:5.253457217857527e-11\n",
            "Epoch 396/500, Loss:2.1457520656131823e-11\n",
            "Epoch 397/500, Loss:6.954189828156654e-12\n",
            "Epoch 398/500, Loss:3.238136191477392e-11\n",
            "Epoch 399/500, Loss:5.459825039993982e-11\n",
            "Epoch 400/500, Loss:7.346786286971385e-11\n",
            "Epoch 401/500, Loss:8.893591462236228e-11\n",
            "Epoch 402/500, Loss:1.0102367813819013e-10\n",
            "Epoch 403/500, Loss:1.0981998166750828e-10\n",
            "Epoch 404/500, Loss:1.1547262885464038e-10\n",
            "Epoch 405/500, Loss:1.1817944973319117e-10\n",
            "Epoch 406/500, Loss:1.181792255201819e-10\n",
            "Epoch 407/500, Loss:1.1574266545189005e-10\n",
            "Epoch 408/500, Loss:1.111636329861887e-10\n",
            "Epoch 409/500, Loss:1.0475077749649842e-10\n",
            "Epoch 410/500, Loss:9.681968855118628e-11\n",
            "Epoch 411/500, Loss:8.768569117328928e-11\n",
            "Epoch 412/500, Loss:7.765738442924719e-11\n",
            "Epoch 413/500, Loss:6.7030939392837e-11\n",
            "Epoch 414/500, Loss:5.608524148575933e-11\n",
            "Epoch 415/500, Loss:4.507787242438721e-11\n",
            "Epoch 416/500, Loss:3.42419248337833e-11\n",
            "Epoch 417/500, Loss:2.3783646059538643e-11\n",
            "Epoch 418/500, Loss:1.3880877350325171e-11\n",
            "Epoch 419/500, Loss:4.682219455903747e-12\n",
            "Epoch 420/500, Loss:3.693099211860007e-12\n",
            "Epoch 421/500, Loss:1.1154751167891108e-11\n",
            "Epoch 422/500, Loss:1.7640272489266584e-11\n",
            "Epoch 423/500, Loss:2.3113574422473082e-11\n",
            "Epoch 424/500, Loss:2.7563041259115462e-11\n",
            "Epoch 425/500, Loss:3.0999326803421434e-11\n",
            "Epoch 426/500, Loss:3.3452908845821305e-11\n",
            "Epoch 427/500, Loss:3.49714731999462e-11\n",
            "Epoch 428/500, Loss:3.561721750866553e-11\n",
            "Epoch 429/500, Loss:3.546410951363321e-11\n",
            "Epoch 430/500, Loss:3.459519216236773e-11\n",
            "Epoch 431/500, Loss:3.309995940531385e-11\n",
            "Epoch 432/500, Loss:3.107186730133549e-11\n",
            "Epoch 433/500, Loss:2.8606012957704818e-11\n",
            "Epoch 434/500, Loss:2.579700775912075e-11\n",
            "Epoch 435/500, Loss:2.273707741182185e-11\n",
            "Epoch 436/500, Loss:1.951439791009202e-11\n",
            "Epoch 437/500, Loss:1.62116813029467e-11\n",
            "Epoch 438/500, Loss:1.2905019067255274e-11\n",
            "Epoch 439/500, Loss:9.662975714724853e-12\n",
            "Epoch 440/500, Loss:6.545928295936809e-12\n",
            "Epoch 441/500, Loss:3.6056344542012564e-12\n",
            "Epoch 442/500, Loss:8.850385702086072e-13\n",
            "Epoch 443/500, Loss:1.5817451515243164e-12\n",
            "Epoch 444/500, Loss:3.76924012834845e-12\n",
            "Epoch 445/500, Loss:5.660280404107265e-12\n",
            "Epoch 446/500, Loss:7.245525360244365e-12\n",
            "Epoch 447/500, Loss:8.522872945587734e-12\n",
            "Epoch 448/500, Loss:9.496781399470633e-12\n",
            "Epoch 449/500, Loss:1.0177514213338679e-11\n",
            "Epoch 450/500, Loss:1.0580355602057834e-11\n",
            "Epoch 451/500, Loss:1.0724791280752877e-11\n",
            "Epoch 452/500, Loss:1.0633683603794575e-11\n",
            "Epoch 453/500, Loss:1.0332464484702486e-11\n",
            "Epoch 454/500, Loss:9.84835477058077e-12\n",
            "Epoch 455/500, Loss:9.209625683598288e-12\n",
            "Epoch 456/500, Loss:8.44490449791735e-12\n",
            "Epoch 457/500, Loss:7.582557411817126e-12\n",
            "Epoch 458/500, Loss:6.650124461521356e-12\n",
            "Epoch 459/500, Loss:5.673837701752893e-12\n",
            "Epoch 460/500, Loss:4.678209208908157e-12\n",
            "Epoch 461/500, Loss:3.685698014149752e-12\n",
            "Epoch 462/500, Loss:2.7164546653746235e-12\n",
            "Epoch 463/500, Loss:1.788133009716919e-12\n",
            "Epoch 464/500, Loss:9.157800386072612e-13\n",
            "Epoch 465/500, Loss:1.117821113449935e-13\n",
            "Epoch 466/500, Loss:6.141194323905363e-13\n",
            "Epoch 467/500, Loss:1.2547705929844e-12\n",
            "Epoch 468/500, Loss:1.805488050732329e-12\n",
            "Epoch 469/500, Loss:2.263912148026126e-12\n",
            "Epoch 470/500, Loss:2.629821707622604e-12\n",
            "Epoch 471/500, Loss:2.9049254321455997e-12\n",
            "Epoch 472/500, Loss:3.092629884821063e-12\n",
            "Epoch 473/500, Loss:3.197808337573882e-12\n",
            "Epoch 474/500, Loss:3.2265427309108308e-12\n",
            "Epoch 475/500, Loss:3.1858860168043623e-12\n",
            "Epoch 476/500, Loss:3.0836158279590187e-12\n",
            "Epoch 477/500, Loss:2.9280011575039122e-12\n",
            "Epoch 478/500, Loss:2.7275837838347528e-12\n",
            "Epoch 479/500, Loss:2.49097877741411e-12\n",
            "Epoch 480/500, Loss:2.2266793443803667e-12\n",
            "Epoch 481/500, Loss:1.9429059056053077e-12\n",
            "Epoch 482/500, Loss:1.6474551757517109e-12\n",
            "Epoch 483/500, Loss:1.347585237843063e-12\n",
            "Epoch 484/500, Loss:1.0499218681958578e-12\n",
            "Epoch 485/500, Loss:7.603796065014379e-13\n",
            "Epoch 486/500, Loss:4.841149182921445e-13\n",
            "Epoch 487/500, Loss:2.2549800568483214e-13\n",
            "Epoch 488/500, Loss:1.1908009300842792e-14\n",
            "Epoch 489/500, Loss:2.2532713542244842e-13\n",
            "Epoch 490/500, Loss:4.127592365121835e-13\n",
            "Epoch 491/500, Loss:5.729375307517159e-13\n",
            "Epoch 492/500, Loss:7.052873909896284e-13\n",
            "Epoch 493/500, Loss:8.098630273345453e-13\n",
            "Epoch 494/500, Loss:8.872841697482592e-13\n",
            "Epoch 495/500, Loss:9.386723169579891e-13\n",
            "Epoch 496/500, Loss:9.655683370912715e-13\n",
            "Epoch 497/500, Loss:9.69869150269087e-13\n",
            "Epoch 498/500, Loss:9.537461966024896e-13\n",
            "Epoch 499/500, Loss:9.195725778066155e-13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X, Y)\n",
        "mod.evaluate(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz2Q6eSC82cI",
        "outputId": "fd77e608-01cd-48e8-d2a0-3873c7baf5d8"
      },
      "id": "Iz2Q6eSC82cI",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1]\n",
            " [1 1]\n",
            " [1 1]\n",
            " [1 1]] [[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.83914516e-12],\n",
              "       [-1.83914516e-12],\n",
              "       [-1.83914516e-12],\n",
              "       [-1.83914516e-12]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Flatten images to 1D vector of 784 features (28*28)\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "def test(model, X, Y, batch_size):\n",
        "  k = int(len(X)/batch_size)\n",
        "  for i in range(k):\n",
        "    Y_hat=model.evaluate(X[i*batch_size:(i+1)*batch_size])\n",
        "    wrong=0\n",
        "    for j in range(batch_size):\n",
        "      max1,max2,id1,id2=-999,-999,-1,-1\n",
        "      for l in range(10):\n",
        "        if max1 < Y_hat[j][l]:\n",
        "          max1,id1=Y_hat[j][l],l\n",
        "        if max2 < Y[i*batch_size+j][l]:\n",
        "          max2,id2=Y[i*batch_size+j][l],l\n",
        "      if id1 != id2: wrong+=1\n",
        "    print(f\"batch: {i}, accuracy: {(batch_size-wrong)/batch_size*100}%\")\n",
        "mod2 = Model(784, 10)\n",
        "mod2.addLayer(32, mod2.Input_layer, mod2.Output_layer)"
      ],
      "metadata": {
        "id": "lcOYUuYAjXPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c316cdf-c7db-4b2b-8d0b-53220f5a0cdc"
      },
      "id": "lcOYUuYAjXPG",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.Model.Neuron at 0x7cb15b727250>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aa190>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aa250>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aa490>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aa510>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aa5d0>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aa650>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aa6d0>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aa750>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aa590>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aa810>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aa890>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aa910>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aa990>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aaa10>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aaa90>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aab10>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aab90>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aac10>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aac90>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aad10>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aad90>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aae10>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aae90>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aaf10>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6aaf90>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6ab010>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6ab090>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6ab110>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6ab190>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6ab210>,\n",
              " <__main__.Model.Neuron at 0x7cb15b6ab290>]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(mod2.Input_layer))\n",
        "mod2.train(x_train, y_train, 2048, 5, 0.01)\n",
        "\"\"\"\n",
        "for i in range(1,9):\n",
        "  plt.subplot(330+i)\n",
        "  plt.imshow(x_test[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
        "print(mod2.evaluate(np.array([x_test[5]])))\n",
        "print(y_test[5])\n",
        "\"\"\"\n",
        "test(mod2, x_test, y_test, 500)"
      ],
      "metadata": {
        "id": "ClmATYl3Qcb7",
        "outputId": "dbf86731-b05d-478b-cf7d-92ff0026739e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ClmATYl3Qcb7",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n",
            "Epoch 0/5, Loss:0.0032852750452220065\n",
            "Epoch 1/5, Loss:0.0032457336255522916\n",
            "Epoch 2/5, Loss:0.0032104943934628083\n",
            "Epoch 3/5, Loss:0.003181738407090361\n",
            "Epoch 4/5, Loss:0.003154146300852825\n",
            "batch: 0, accuracy: 71.6%\n",
            "batch: 1, accuracy: 70.6%\n",
            "batch: 2, accuracy: 65.8%\n",
            "batch: 3, accuracy: 69.39999999999999%\n",
            "batch: 4, accuracy: 67.80000000000001%\n",
            "batch: 5, accuracy: 69.8%\n",
            "batch: 6, accuracy: 74.2%\n",
            "batch: 7, accuracy: 67.4%\n",
            "batch: 8, accuracy: 66.4%\n",
            "batch: 9, accuracy: 70.39999999999999%\n",
            "batch: 10, accuracy: 80.0%\n",
            "batch: 11, accuracy: 76.4%\n",
            "batch: 12, accuracy: 84.2%\n",
            "batch: 13, accuracy: 73.4%\n",
            "batch: 14, accuracy: 78.2%\n",
            "batch: 15, accuracy: 78.0%\n",
            "batch: 16, accuracy: 77.60000000000001%\n",
            "batch: 17, accuracy: 88.0%\n",
            "batch: 18, accuracy: 78.0%\n",
            "batch: 19, accuracy: 68.8%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(mod2, x_test, y_test, 5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhe7AJ7MHY5E",
        "outputId": "d105e306-d537-4270-df77-0b5a40849400"
      },
      "id": "Bhe7AJ7MHY5E",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch: 0, accuracy: 69.34%\n",
            "batch: 1, accuracy: 78.25999999999999%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}