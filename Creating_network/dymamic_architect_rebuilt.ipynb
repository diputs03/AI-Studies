{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diputs03/AI-Studies/blob/main/Creating_network/dymamic_architect_rebuilt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Aiming a Dynaimic Graph-structured NeuronNetwork\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "6mXjwpToZTeV"
      },
      "id": "6mXjwpToZTeV",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Activation function in this case in tanh\n",
        "Loss is the Euclidean loss\n",
        "\"\"\"\n",
        "class Model:\n",
        "  class Neuron:\n",
        "    def __init__(self, name, prev, next):\n",
        "      self.name, self.prev, self.next = name, prev, next\n",
        "      self.bias = np.random.uniform(-0.1, 0.1)\n",
        "\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.Input_layer = [Model.Neuron(f\"input{i}\", [], []) for i in range(input_size)]\n",
        "    self.Output_layer = [Model.Neuron(f\"output{o}\", [], []) for o in range(output_size)]\n",
        "    for i in range(input_size): self.Input_layer[i].next = self.Output_layer\n",
        "    for o in range(output_size): self.Output_layer[o].prev = self.Input_layer\n",
        "    self.weight = {}\n",
        "    for u in self.Input_layer:\n",
        "      for v in self.Output_layer:\n",
        "        self.weight[(u,v)] = np.random.uniform(-0.1, 0.1)\n",
        "    self.all_neurons = self.Input_layer + self.Output_layer\n",
        "\n",
        "  def forward(self, X, batch_size):\n",
        "    assert X.shape == (batch_size,self.input_size)\n",
        "    a = {q: [0 for _ in range(batch_size)] for q in self.all_neurons}\n",
        "\n",
        "    for i, n in enumerate(self.Input_layer):\n",
        "      a[n] = X[:, i]\n",
        "\n",
        "    q = deque()\n",
        "    for i in self.Input_layer:\n",
        "      q.append(i)\n",
        "\n",
        "    cnt = {q: 0 for q in self.all_neurons}\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      a[c] = np.tanh(a[c] + c.bias)\n",
        "      for n in c.next:\n",
        "        a[n] = a[n] + a[c]*self.weight[(c,n)]\n",
        "        cnt[n] += 1\n",
        "        if cnt[n] == len(n.prev):\n",
        "          q.append(n)\n",
        "    return a\n",
        "\n",
        "  def eval(self, X):\n",
        "    a = self.forward(X, len(X))\n",
        "    return np.array([a[o] for o in self.Output_layer]).T\n",
        "\n",
        "  def backward(self, X, Y, batch_size, learning_rate):\n",
        "    assert X.shape == (batch_size,self.input_size)\n",
        "    assert Y.shape == (batch_size,self.output_size)\n",
        "    a = self.forward(X, batch_size)\n",
        "    par_a = {q: [0 for _ in range(batch_size)] for q in self.all_neurons}\n",
        "    for o, n in enumerate(self.Output_layer):\n",
        "      par_a[n] = 2 * (a[n] - Y[:, o])\n",
        "\n",
        "    q = deque()\n",
        "    for o in self.Output_layer:\n",
        "      q.append(o)\n",
        "\n",
        "    cnt = {q: 0 for q in self.all_neurons}\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      par_b = par_a[c] * (1-a[c]**2)\n",
        "      c.bias -= np.mean(par_b) * learning_rate\n",
        "      for p in c.prev:\n",
        "        par_a[p] += par_a[c] * (1-a[c]**2) * self.weight[(p,c)]\n",
        "        par_w_pc = par_a[c] * (1-a[c]**2) * a[p]\n",
        "        self.weight[(p,c)] -= np.mean(par_w_pc) * learning_rate\n",
        "        cnt[p] += 1\n",
        "        if cnt[p] == len(p.next):\n",
        "          q.append(p)\n",
        "\n",
        "  def addLayer(self, mid_size, UP, DOWN):\n",
        "    Mid_layer = [Model.Neuron(f\"mid{o}\", [], []) for o in range(mid_size)]\n",
        "    for m, mid in enumerate(Mid_layer):\n",
        "      Mid_layer[m].prev = UP\n",
        "      for u in UP:\n",
        "        self.weight[(u,mid)] = np.random.uniform(-0.1, 0.1)\n",
        "      Mid_layer[m].next = DOWN\n",
        "      for v in DOWN:\n",
        "        self.weight[(mid,v)] = np.random.uniform(-0.1, 0.1)\n",
        "    for u in UP:\n",
        "      u.next = Mid_layer\n",
        "    for v in DOWN:\n",
        "      v.prev = Mid_layer\n",
        "    for u in UP:\n",
        "      for v in DOWN:\n",
        "        self.weight.pop((u, v))\n",
        "    self.all_neurons += Mid_layer\n",
        "    return Mid_layer\n",
        "\n",
        "  def train(self, X, Y, batch_size, epochs, learning_rate):\n",
        "    l = len(X)\n",
        "    for epoch in range(epochs):\n",
        "      data=[(X[_], Y[_]) for _ in range(len(X))]\n",
        "      random.shuffle(data)\n",
        "      for _ in range(len(X)):\n",
        "        X[_],Y[_]=data[_]\n",
        "      loss = 0\n",
        "      for batch in range(int(l / batch_size)):\n",
        "        L, R = batch * batch_size, (batch + 1) * batch_size\n",
        "        output = self.eval(X[L:R])\n",
        "        self.backward(X[L:R], Y[L:R], R-L, learning_rate)\n",
        "        for e in range(batch_size):\n",
        "          for o in range(len(self.Output_layer)):\n",
        "            loss += (Y[L+e][o] - output[e][o]) ** 2\n",
        "      loss = ((loss) ** 0.5) / (int(l / batch_size) * batch_size)\n",
        "      print(f\"Epoch {epoch}, Loss:{loss}\")"
      ],
      "metadata": {
        "id": "YUliU5Cx-oyU"
      },
      "id": "YUliU5Cx-oyU",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "Y=np.array([[0],[1],[1],[0]])\n",
        "mod=Model(2, 1)\n",
        "mid1=mod.addLayer(4, mod.Input_layer, mod.Output_layer)\n",
        "mod.addLayer(4, mid1, mod.Output_layer)\n",
        "mod.eval(X)"
      ],
      "metadata": {
        "id": "KfiQiE5l8XOr",
        "outputId": "aa609c4c-a869-459c-f46f-dcbd452831d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KfiQiE5l8XOr",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.07398855],\n",
              "       [-0.07444478],\n",
              "       [-0.07403891],\n",
              "       [-0.07449677]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod.train(X, Y, 4, 500, 0.001)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aC2n_sC_8xnP",
        "outputId": "354716c0-1e74-4d0e-c3a2-c28e6cf1dbd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "aC2n_sC_8xnP",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss:0.38064244373620926\n",
            "Epoch 1, Loss:0.46492825023706186\n",
            "Epoch 2, Loss:0.4641873226153796\n",
            "Epoch 3, Loss:0.534762046582328\n",
            "Epoch 4, Loss:0.5336727715968619\n",
            "Epoch 5, Loss:0.5325850380774986\n",
            "Epoch 6, Loss:0.5314988664986448\n",
            "Epoch 7, Loss:0.5304142771048055\n",
            "Epoch 8, Loss:0.5293312899095699\n",
            "Epoch 9, Loss:0.5282499246946601\n",
            "Epoch 10, Loss:0.5271702010090441\n",
            "Epoch 11, Loss:0.5260921381681086\n",
            "Epoch 12, Loss:0.5250157552528961\n",
            "Epoch 13, Loss:0.5239410711094014\n",
            "Epoch 14, Loss:0.5228681043479289\n",
            "Epoch 15, Loss:0.5217968733425117\n",
            "Epoch 16, Loss:0.5207273962303882\n",
            "Epoch 17, Loss:0.5196596909115392\n",
            "Epoch 18, Loss:0.5185937750482825\n",
            "Epoch 19, Loss:0.517529666064926\n",
            "Epoch 20, Loss:0.5164673811474777\n",
            "Epoch 21, Loss:0.5154069372434117\n",
            "Epoch 22, Loss:0.5143483510614915\n",
            "Epoch 23, Loss:0.5132916390716461\n",
            "Epoch 24, Loss:0.5122368175049032\n",
            "Epoch 25, Loss:0.5111839023533746\n",
            "Epoch 26, Loss:0.5101329093702941\n",
            "Epoch 27, Loss:0.5090838540701096\n",
            "Epoch 28, Loss:0.5080367517286247\n",
            "Epoch 29, Loss:0.5069916173831929\n",
            "Epoch 30, Loss:0.5059484658329598\n",
            "Epoch 31, Loss:0.5049073116391566\n",
            "Epoch 32, Loss:0.5038681691254405\n",
            "Epoch 33, Loss:0.5028310523782832\n",
            "Epoch 34, Loss:0.501795975247406\n",
            "Epoch 35, Loss:0.5007629513462621\n",
            "Epoch 36, Loss:0.49973199405256175\n",
            "Epoch 37, Loss:0.49870311650884397\n",
            "Epoch 38, Loss:0.49767633162309044\n",
            "Epoch 39, Loss:0.4966516520693828\n",
            "Epoch 40, Loss:0.4956290902886009\n",
            "Epoch 41, Loss:0.49460865848916324\n",
            "Epoch 42, Loss:0.49359036864780625\n",
            "Epoch 43, Loss:0.49257423251040355\n",
            "Epoch 44, Loss:0.4915602615928236\n",
            "Epoch 45, Loss:0.4905484671818248\n",
            "Epoch 46, Loss:0.48953886033598704\n",
            "Epoch 47, Loss:0.4885314518866793\n",
            "Epoch 48, Loss:0.4875262524390623\n",
            "Epoch 49, Loss:0.48652327237312515\n",
            "Epoch 50, Loss:0.4855225218447552\n",
            "Epoch 51, Loss:0.4845240107868402\n",
            "Epoch 52, Loss:0.48352774891040246\n",
            "Epoch 53, Loss:0.4825337457057629\n",
            "Epoch 54, Loss:0.48154201044373596\n",
            "Epoch 55, Loss:0.48055255217685233\n",
            "Epoch 56, Loss:0.4795653797406109\n",
            "Epoch 57, Loss:0.47858050175475736\n",
            "Epoch 58, Loss:0.47759792662458944\n",
            "Epoch 59, Loss:0.4766176625422879\n",
            "Epoch 60, Loss:0.4756397174882721\n",
            "Epoch 61, Loss:0.47466409923257996\n",
            "Epoch 62, Loss:0.4736908153362707\n",
            "Epoch 63, Loss:0.47271987315285013\n",
            "Epoch 64, Loss:0.4717512798297176\n",
            "Epoch 65, Loss:0.47078504230963353\n",
            "Epoch 66, Loss:0.46982116733220713\n",
            "Epoch 67, Loss:0.4688596614354034\n",
            "Epoch 68, Loss:0.46790053095706813\n",
            "Epoch 69, Loss:0.4669437820364711\n",
            "Epoch 70, Loss:0.465989420615866\n",
            "Epoch 71, Loss:0.46503745244206646\n",
            "Epoch 72, Loss:0.4640878830680375\n",
            "Epoch 73, Loss:0.463140717854502\n",
            "Epoch 74, Loss:0.46219596197156065\n",
            "Epoch 75, Loss:0.46125362040032564\n",
            "Epoch 76, Loss:0.4603136979345665\n",
            "Epoch 77, Loss:0.4593761991823682\n",
            "Epoch 78, Loss:0.45844112856780034\n",
            "Epoch 79, Loss:0.4575084903325965\n",
            "Epoch 80, Loss:0.4565782885378438\n",
            "Epoch 81, Loss:0.4556505270656819\n",
            "Epoch 82, Loss:0.45472520962100965\n",
            "Epoch 83, Loss:0.4538023397332006\n",
            "Epoch 84, Loss:0.4528819207578253\n",
            "Epoch 85, Loss:0.45196395587838034\n",
            "Epoch 86, Loss:0.4510484481080235\n",
            "Epoch 87, Loss:0.45013540029131455\n",
            "Epoch 88, Loss:0.4492248151059603\n",
            "Epoch 89, Loss:0.4483166950645648\n",
            "Epoch 90, Loss:0.44741104251638286\n",
            "Epoch 91, Loss:0.4465078596490767\n",
            "Epoch 92, Loss:0.44560714849047545\n",
            "Epoch 93, Loss:0.44470891091033654\n",
            "Epoch 94, Loss:0.4438131486221092\n",
            "Epoch 95, Loss:0.4429198631846984\n",
            "Epoch 96, Loss:0.44202905600422976\n",
            "Epoch 97, Loss:0.4411407283358147\n",
            "Epoch 98, Loss:0.44025488128531487\n",
            "Epoch 99, Loss:0.4393715158111059\n",
            "Epoch 100, Loss:0.43849063272584005\n",
            "Epoch 101, Loss:0.4376122326982068\n",
            "Epoch 102, Loss:0.4367363162546914\n",
            "Epoch 103, Loss:0.43586288378133103\n",
            "Epoch 104, Loss:0.43499193552546755\n",
            "Epoch 105, Loss:0.43412347159749726\n",
            "Epoch 106, Loss:0.4332574919726162\n",
            "Epoch 107, Loss:0.4323939964925624\n",
            "Epoch 108, Loss:0.43153298486735214\n",
            "Epoch 109, Loss:0.4306744566770123\n",
            "Epoch 110, Loss:0.42981841137330734\n",
            "Epoch 111, Loss:0.4289648482814599\n",
            "Epoch 112, Loss:0.42811376660186634\n",
            "Epoch 113, Loss:0.42726516541180537\n",
            "Epoch 114, Loss:0.4264190436671406\n",
            "Epoch 115, Loss:0.4255754002040155\n",
            "Epoch 116, Loss:0.42473423374054176\n",
            "Epoch 117, Loss:0.42389554287847986\n",
            "Epoch 118, Loss:0.4230593261049119\n",
            "Epoch 119, Loss:0.42222558179390673\n",
            "Epoch 120, Loss:0.4213943082081758\n",
            "Epoch 121, Loss:0.42056550350072164\n",
            "Epoch 122, Loss:0.419739165716477\n",
            "Epoch 123, Loss:0.41891529279393475\n",
            "Epoch 124, Loss:0.4180938825667688\n",
            "Epoch 125, Loss:0.41727493276544586\n",
            "Epoch 126, Loss:0.41645844101882645\n",
            "Epoch 127, Loss:0.41564440485575743\n",
            "Epoch 128, Loss:0.41483282170665337\n",
            "Epoch 129, Loss:0.4140236889050682\n",
            "Epoch 130, Loss:0.41321700368925607\n",
            "Epoch 131, Loss:0.4124127632037222\n",
            "Epoch 132, Loss:0.4116109645007621\n",
            "Epoch 133, Loss:0.4108116045419907\n",
            "Epoch 134, Loss:0.4100146801998599\n",
            "Epoch 135, Loss:0.4092201882591649\n",
            "Epoch 136, Loss:0.4084281254185396\n",
            "Epoch 137, Loss:0.40763848829194016\n",
            "Epoch 138, Loss:0.40685127341011706\n",
            "Epoch 139, Loss:0.40606647722207545\n",
            "Epoch 140, Loss:0.4052840960965238\n",
            "Epoch 141, Loss:0.40450412632331056\n",
            "Epoch 142, Loss:0.40372656411484864\n",
            "Epoch 143, Loss:0.40295140560752823\n",
            "Epoch 144, Loss:0.40217864686311694\n",
            "Epoch 145, Loss:0.4014082838701482\n",
            "Epoch 146, Loss:0.40064031254529686\n",
            "Epoch 147, Loss:0.39987472873474283\n",
            "Epoch 148, Loss:0.39911152821552176\n",
            "Epoch 149, Loss:0.39835070669686407\n",
            "Epoch 150, Loss:0.39759225982152024\n",
            "Epoch 151, Loss:0.39683618316707475\n",
            "Epoch 152, Loss:0.3960824722472465\n",
            "Epoch 153, Loss:0.3953311225131771\n",
            "Epoch 154, Loss:0.39458212935470593\n",
            "Epoch 155, Loss:0.39383548810163327\n",
            "Epoch 156, Loss:0.39309119402496984\n",
            "Epoch 157, Loss:0.3923492423381742\n",
            "Epoch 158, Loss:0.3916096281983771\n",
            "Epoch 159, Loss:0.3908723467075932\n",
            "Epoch 160, Loss:0.39013739291391997\n",
            "Epoch 161, Loss:0.38940476181272377\n",
            "Epoch 162, Loss:0.3886744483478133\n",
            "Epoch 163, Loss:0.3879464474126\n",
            "Epoch 164, Loss:0.38722075385124616\n",
            "Epoch 165, Loss:0.3864973624598\n",
            "Epoch 166, Loss:0.3857762679873179\n",
            "Epoch 167, Loss:0.3850574651369745\n",
            "Epoch 168, Loss:0.3843409485671595\n",
            "Epoch 169, Loss:0.38362671289256217\n",
            "Epoch 170, Loss:0.38291475268524333\n",
            "Epoch 171, Loss:0.38220506247569463\n",
            "Epoch 172, Loss:0.38149763675388515\n",
            "Epoch 173, Loss:0.3807924699702958\n",
            "Epoch 174, Loss:0.38008955653694126\n",
            "Epoch 175, Loss:0.37938889082837934\n",
            "Epoch 176, Loss:0.3786904671827081\n",
            "Epoch 177, Loss:0.3779942799025505\n",
            "Epoch 178, Loss:0.37730032325602736\n",
            "Epoch 179, Loss:0.3766085914777173\n",
            "Epoch 180, Loss:0.37591907876960523\n",
            "Epoch 181, Loss:0.37523177930201856\n",
            "Epoch 182, Loss:0.3745466872145512\n",
            "Epoch 183, Loss:0.3738637966169758\n",
            "Epoch 184, Loss:0.37318310159014423\n",
            "Epoch 185, Loss:0.37250459618687604\n",
            "Epoch 186, Loss:0.37182827443283506\n",
            "Epoch 187, Loss:0.37115413032739475\n",
            "Epoch 188, Loss:0.37048215784449157\n",
            "Epoch 189, Loss:0.36981235093346687\n",
            "Epoch 190, Loss:0.36914470351989753\n",
            "Epoch 191, Loss:0.3684792095064151\n",
            "Epoch 192, Loss:0.36781586277351336\n",
            "Epoch 193, Loss:0.36715465718034523\n",
            "Epoch 194, Loss:0.36649558656550796\n",
            "Epoch 195, Loss:0.36583864474781785\n",
            "Epoch 196, Loss:0.3651838255270731\n",
            "Epoch 197, Loss:0.3645311226848069\n",
            "Epoch 198, Loss:0.3638805299850286\n",
            "Epoch 199, Loss:0.363232041174955\n",
            "Epoch 200, Loss:0.3625856499857306\n",
            "Epoch 201, Loss:0.361941350133137\n",
            "Epoch 202, Loss:0.3612991353182926\n",
            "Epoch 203, Loss:0.36065899922834127\n",
            "Epoch 204, Loss:0.36002093553713077\n",
            "Epoch 205, Loss:0.3593849379058813\n",
            "Epoch 206, Loss:0.3587509999838435\n",
            "Epoch 207, Loss:0.3581191154089473\n",
            "Epoch 208, Loss:0.3574892778084391\n",
            "Epoch 209, Loss:0.35686148079951086\n",
            "Epoch 210, Loss:0.3562357179899188\n",
            "Epoch 211, Loss:0.355611982978592\n",
            "Epoch 212, Loss:0.35499026935623196\n",
            "Epoch 213, Loss:0.35437057070590233\n",
            "Epoch 214, Loss:0.35375288060360965\n",
            "Epoch 215, Loss:0.35313719261887433\n",
            "Epoch 216, Loss:0.35252350031529245\n",
            "Epoch 217, Loss:0.35191179725108906\n",
            "Epoch 218, Loss:0.35130207697966165\n",
            "Epoch 219, Loss:0.3506943330501149\n",
            "Epoch 220, Loss:0.350088559007787\n",
            "Epoch 221, Loss:0.34948474839476673\n",
            "Epoch 222, Loss:0.348882894750402\n",
            "Epoch 223, Loss:0.3482829916118002\n",
            "Epoch 224, Loss:0.34768503251431904\n",
            "Epoch 225, Loss:0.34708901099205036\n",
            "Epoch 226, Loss:0.3464949205782948\n",
            "Epoch 227, Loss:0.3459027548060282\n",
            "Epoch 228, Loss:0.3453125072083604\n",
            "Epoch 229, Loss:0.34472417131898536\n",
            "Epoch 230, Loss:0.34413774067262426\n",
            "Epoch 231, Loss:0.3435532088054597\n",
            "Epoch 232, Loss:0.34297056925556274\n",
            "Epoch 233, Loss:0.3423898155633124\n",
            "Epoch 234, Loss:0.34181094127180717\n",
            "Epoch 235, Loss:0.34123393992726925\n",
            "Epoch 236, Loss:0.34065880507944135\n",
            "Epoch 237, Loss:0.3400855302819761\n",
            "Epoch 238, Loss:0.33951410909281854\n",
            "Epoch 239, Loss:0.3389445350745808\n",
            "Epoch 240, Loss:0.3383768017949107\n",
            "Epoch 241, Loss:0.33781090282685244\n",
            "Epoch 242, Loss:0.3372468317492009\n",
            "Epoch 243, Loss:0.33668458214684926\n",
            "Epoch 244, Loss:0.3361241476111292\n",
            "Epoch 245, Loss:0.33556552174014553\n",
            "Epoch 246, Loss:0.3350086981391034\n",
            "Epoch 247, Loss:0.33445367042062946\n",
            "Epoch 248, Loss:0.33390043220508625\n",
            "Epoch 249, Loss:0.33334897712088113\n",
            "Epoch 250, Loss:0.3327992988047679\n",
            "Epoch 251, Loss:0.332251390902143\n",
            "Epoch 252, Loss:0.33170524706733573\n",
            "Epoch 253, Loss:0.33116086096389175\n",
            "Epoch 254, Loss:0.33061822626485154\n",
            "Epoch 255, Loss:0.3300773366530225\n",
            "Epoch 256, Loss:0.3295381858212456\n",
            "Epoch 257, Loss:0.32900076747265594\n",
            "Epoch 258, Loss:0.3284650753209384\n",
            "Epoch 259, Loss:0.32793110309057716\n",
            "Epoch 260, Loss:0.32739884451710005\n",
            "Epoch 261, Loss:0.32686829334731743\n",
            "Epoch 262, Loss:0.326339443339556\n",
            "Epoch 263, Loss:0.3258122882638872\n",
            "Epoch 264, Loss:0.3252868219023505\n",
            "Epoch 265, Loss:0.3247630380491712\n",
            "Epoch 266, Loss:0.3242409305109748\n",
            "Epoch 267, Loss:0.32372049310699386\n",
            "Epoch 268, Loss:0.32320171966927247\n",
            "Epoch 269, Loss:0.3226846040428645\n",
            "Epoch 270, Loss:0.3221691400860275\n",
            "Epoch 271, Loss:0.3216553216704121\n",
            "Epoch 272, Loss:0.32114314268124666\n",
            "Epoch 273, Loss:0.32063259701751723\n",
            "Epoch 274, Loss:0.32012367859214375\n",
            "Epoch 275, Loss:0.31961638133215065\n",
            "Epoch 276, Loss:0.31911069917883456\n",
            "Epoch 277, Loss:0.31860662608792667\n",
            "Epoch 278, Loss:0.3181041560297513\n",
            "Epoch 279, Loss:0.3176032829893803\n",
            "Epoch 280, Loss:0.3171040009667835\n",
            "Epoch 281, Loss:0.3166063039769751\n",
            "Epoch 282, Loss:0.3161101860501556\n",
            "Epoch 283, Loss:0.315615641231851\n",
            "Epoch 284, Loss:0.31512266358304675\n",
            "Epoch 285, Loss:0.31463124718031915\n",
            "Epoch 286, Loss:0.3141413861159623\n",
            "Epoch 287, Loss:0.31365307449811164\n",
            "Epoch 288, Loss:0.31316630645086396\n",
            "Epoch 289, Loss:0.31268107611439383\n",
            "Epoch 290, Loss:0.31219737764506617\n",
            "Epoch 291, Loss:0.311715205215546\n",
            "Epoch 292, Loss:0.31123455301490477\n",
            "Epoch 293, Loss:0.31075541524872263\n",
            "Epoch 294, Loss:0.310277786139188\n",
            "Epoch 295, Loss:0.3098016599251943\n",
            "Epoch 296, Loss:0.30932703086243263\n",
            "Epoch 297, Loss:0.308853893223482\n",
            "Epoch 298, Loss:0.30838224129789615\n",
            "Epoch 299, Loss:0.3079120693922874\n",
            "Epoch 300, Loss:0.3074433718304079\n",
            "Epoch 301, Loss:0.30697614295322745\n",
            "Epoch 302, Loss:0.30651037711900886\n",
            "Epoch 303, Loss:0.30604606870338\n",
            "Epoch 304, Loss:0.30558321209940387\n",
            "Epoch 305, Loss:0.30512180171764497\n",
            "Epoch 306, Loss:0.3046618319862338\n",
            "Epoch 307, Loss:0.3042032973509283\n",
            "Epoch 308, Loss:0.30374619227517297\n",
            "Epoch 309, Loss:0.3032905112401549\n",
            "Epoch 310, Loss:0.30283624874485815\n",
            "Epoch 311, Loss:0.3023833993061151\n",
            "Epoch 312, Loss:0.30193195745865503\n",
            "Epoch 313, Loss:0.30148191775515154\n",
            "Epoch 314, Loss:0.3010332747662662\n",
            "Epoch 315, Loss:0.3005860230806911\n",
            "Epoch 316, Loss:0.3001401573051884\n",
            "Epoch 317, Loss:0.29969567206462766\n",
            "Epoch 318, Loss:0.299252562002022\n",
            "Epoch 319, Loss:0.29881082177856033\n",
            "Epoch 320, Loss:0.2983704460736396\n",
            "Epoch 321, Loss:0.2979314295848931\n",
            "Epoch 322, Loss:0.29749376702821784\n",
            "Epoch 323, Loss:0.29705745313779985\n",
            "Epoch 324, Loss:0.296622482666137\n",
            "Epoch 325, Loss:0.2961888503840605\n",
            "Epoch 326, Loss:0.29575655108075416\n",
            "Epoch 327, Loss:0.2953255795637719\n",
            "Epoch 328, Loss:0.2948959306590536\n",
            "Epoch 329, Loss:0.294467599210939\n",
            "Epoch 330, Loss:0.2940405800821798\n",
            "Epoch 331, Loss:0.2936148681539501\n",
            "Epoch 332, Loss:0.29319045832585555\n",
            "Epoch 333, Loss:0.2927673455159402\n",
            "Epoch 334, Loss:0.29234552466069225\n",
            "Epoch 335, Loss:0.29192499071504796\n",
            "Epoch 336, Loss:0.29150573865239404\n",
            "Epoch 337, Loss:0.29108776346456866\n",
            "Epoch 338, Loss:0.29067106016186084\n",
            "Epoch 339, Loss:0.2902556237730082\n",
            "Epoch 340, Loss:0.28984144934519374\n",
            "Epoch 341, Loss:0.2894285319440411\n",
            "Epoch 342, Loss:0.2890168666536078\n",
            "Epoch 343, Loss:0.28860644857637796\n",
            "Epoch 344, Loss:0.28819727283325336\n",
            "Epoch 345, Loss:0.2877893345635432\n",
            "Epoch 346, Loss:0.2873826289249525\n",
            "Epoch 347, Loss:0.28697715109356947\n",
            "Epoch 348, Loss:0.28657289626385135\n",
            "Epoch 349, Loss:0.2861698596486097\n",
            "Epoch 350, Loss:0.2857680364789936\n",
            "Epoch 351, Loss:0.2853674220044726\n",
            "Epoch 352, Loss:0.2849680114928177\n",
            "Epoch 353, Loss:0.28456980023008205\n",
            "Epoch 354, Loss:0.28417278352057995\n",
            "Epoch 355, Loss:0.2837769566868654\n",
            "Epoch 356, Loss:0.2833823150697089\n",
            "Epoch 357, Loss:0.2829888540280738\n",
            "Epoch 358, Loss:0.2825965689390914\n",
            "Epoch 359, Loss:0.28220545519803547\n",
            "Epoch 360, Loss:0.28181550821829515\n",
            "Epoch 361, Loss:0.28142672343134767\n",
            "Epoch 362, Loss:0.28103909628672963\n",
            "Epoch 363, Loss:0.28065262225200754\n",
            "Epoch 364, Loss:0.28026729681274787\n",
            "Epoch 365, Loss:0.2798831154724857\n",
            "Epoch 366, Loss:0.2795000737526928\n",
            "Epoch 367, Loss:0.2791181671927453\n",
            "Epoch 368, Loss:0.2787373913498896\n",
            "Epoch 369, Loss:0.27835774179920864\n",
            "Epoch 370, Loss:0.2779792141335866\n",
            "Epoch 371, Loss:0.2776018039636735\n",
            "Epoch 372, Loss:0.2772255069178484\n",
            "Epoch 373, Loss:0.2768503186421825\n",
            "Epoch 374, Loss:0.27647623480040145\n",
            "Epoch 375, Loss:0.2761032510738467\n",
            "Epoch 376, Loss:0.2757313631614364\n",
            "Epoch 377, Loss:0.27536056677962617\n",
            "Epoch 378, Loss:0.27499085766236797\n",
            "Epoch 379, Loss:0.27462223156106996\n",
            "Epoch 380, Loss:0.27425468424455457\n",
            "Epoch 381, Loss:0.2738882114990166\n",
            "Epoch 382, Loss:0.27352280912798077\n",
            "Epoch 383, Loss:0.27315847295225815\n",
            "Epoch 384, Loss:0.2727951988099032\n",
            "Epoch 385, Loss:0.272432982556169\n",
            "Epoch 386, Loss:0.272071820063463\n",
            "Epoch 387, Loss:0.27171170722130167\n",
            "Epoch 388, Loss:0.2713526399362651\n",
            "Epoch 389, Loss:0.2709946141319511\n",
            "Epoch 390, Loss:0.27063762574892825\n",
            "Epoch 391, Loss:0.27028167074468973\n",
            "Epoch 392, Loss:0.26992674509360537\n",
            "Epoch 393, Loss:0.2695728447868744\n",
            "Epoch 394, Loss:0.2692199658324772\n",
            "Epoch 395, Loss:0.2688681042551269\n",
            "Epoch 396, Loss:0.2685172560962207\n",
            "Epoch 397, Loss:0.26816741741379035\n",
            "Epoch 398, Loss:0.26781858428245303\n",
            "Epoch 399, Loss:0.26747075279336147\n",
            "Epoch 400, Loss:0.26712391905415356\n",
            "Epoch 401, Loss:0.2667780791889023\n",
            "Epoch 402, Loss:0.2664332293380646\n",
            "Epoch 403, Loss:0.26608936565843067\n",
            "Epoch 404, Loss:0.2657464843230726\n",
            "Epoch 405, Loss:0.2654045815212923\n",
            "Epoch 406, Loss:0.26506365345857075\n",
            "Epoch 407, Loss:0.26472369635651494\n",
            "Epoch 408, Loss:0.26438470645280593\n",
            "Epoch 409, Loss:0.2640466800011464\n",
            "Epoch 410, Loss:0.2637096132712078\n",
            "Epoch 411, Loss:0.2633735025485775\n",
            "Epoch 412, Loss:0.26303834413470545\n",
            "Epoch 413, Loss:0.26270413434685097\n",
            "Epoch 414, Loss:0.26237086951802924\n",
            "Epoch 415, Loss:0.2620385459969575\n",
            "Epoch 416, Loss:0.2617071601480012\n",
            "Epoch 417, Loss:0.26137670835112026\n",
            "Epoch 418, Loss:0.2610471870018143\n",
            "Epoch 419, Loss:0.2607185925110688\n",
            "Epoch 420, Loss:0.26039092130530067\n",
            "Epoch 421, Loss:0.2600641698263033\n",
            "Epoch 422, Loss:0.25973833453119216\n",
            "Epoch 423, Loss:0.2594134118923499\n",
            "Epoch 424, Loss:0.25908939839737155\n",
            "Epoch 425, Loss:0.2587662905490091\n",
            "Epoch 426, Loss:0.25844408486511705\n",
            "Epoch 427, Loss:0.2581227778785965\n",
            "Epoch 428, Loss:0.25780236613734037\n",
            "Epoch 429, Loss:0.25748284620417794\n",
            "Epoch 430, Loss:0.25716421465681955\n",
            "Epoch 431, Loss:0.2568464680878006\n",
            "Epoch 432, Loss:0.25652960310442674\n",
            "Epoch 433, Loss:0.25621361632871786\n",
            "Epoch 434, Loss:0.25589850439735257\n",
            "Epoch 435, Loss:0.2555842639616125\n",
            "Epoch 436, Loss:0.25527089168732675\n",
            "Epoch 437, Loss:0.25495838425481626\n",
            "Epoch 438, Loss:0.25464673835883755\n",
            "Epoch 439, Loss:0.25433595070852766\n",
            "Epoch 440, Loss:0.25402601802734803\n",
            "Epoch 441, Loss:0.2537169370530287\n",
            "Epoch 442, Loss:0.25340870453751274\n",
            "Epoch 443, Loss:0.2531013172469004\n",
            "Epoch 444, Loss:0.2527947719613932\n",
            "Epoch 445, Loss:0.25248906547523875\n",
            "Epoch 446, Loss:0.2521841945966742\n",
            "Epoch 447, Loss:0.2518801561478713\n",
            "Epoch 448, Loss:0.2515769469648804\n",
            "Epoch 449, Loss:0.2512745638975748\n",
            "Epoch 450, Loss:0.2509730038095952\n",
            "Epoch 451, Loss:0.2506722635782943\n",
            "Epoch 452, Loss:0.250372340094681\n",
            "Epoch 453, Loss:0.25007323026336514\n",
            "Epoch 454, Loss:0.24977493100250214\n",
            "Epoch 455, Loss:0.2494774392437374\n",
            "Epoch 456, Loss:0.24918075193215117\n",
            "Epoch 457, Loss:0.2488848660262033\n",
            "Epoch 458, Loss:0.24858977849767822\n",
            "Epoch 459, Loss:0.2482954863316294\n",
            "Epoch 460, Loss:0.2480019865263247\n",
            "Epoch 461, Loss:0.24770927609319154\n",
            "Epoch 462, Loss:0.24741735205676146\n",
            "Epoch 463, Loss:0.24712621145461594\n",
            "Epoch 464, Loss:0.2468358513373312\n",
            "Epoch 465, Loss:0.24654626876842411\n",
            "Epoch 466, Loss:0.24625746082429711\n",
            "Epoch 467, Loss:0.24596942459418408\n",
            "Epoch 468, Loss:0.24568215718009606\n",
            "Epoch 469, Loss:0.24539565569676686\n",
            "Epoch 470, Loss:0.24510991727159886\n",
            "Epoch 471, Loss:0.24482493904460922\n",
            "Epoch 472, Loss:0.24454071816837597\n",
            "Epoch 473, Loss:0.2442572518079837\n",
            "Epoch 474, Loss:0.2439745371409704\n",
            "Epoch 475, Loss:0.2436925713572738\n",
            "Epoch 476, Loss:0.24341135165917754\n",
            "Epoch 477, Loss:0.24313087526125826\n",
            "Epoch 478, Loss:0.24285113939033204\n",
            "Epoch 479, Loss:0.2425721412854014\n",
            "Epoch 480, Loss:0.2422938781976024\n",
            "Epoch 481, Loss:0.24201634739015165\n",
            "Epoch 482, Loss:0.24173954613829368\n",
            "Epoch 483, Loss:0.2414634717292482\n",
            "Epoch 484, Loss:0.2411881214621578\n",
            "Epoch 485, Loss:0.24091349264803547\n",
            "Epoch 486, Loss:0.24063958260971252\n",
            "Epoch 487, Loss:0.24036638868178645\n",
            "Epoch 488, Loss:0.24009390821056908\n",
            "Epoch 489, Loss:0.23982213855403461\n",
            "Epoch 490, Loss:0.2395510770817682\n",
            "Epoch 491, Loss:0.23928072117491433\n",
            "Epoch 492, Loss:0.23901106822612556\n",
            "Epoch 493, Loss:0.23874211563951114\n",
            "Epoch 494, Loss:0.23847386083058608\n",
            "Epoch 495, Loss:0.23820630122622038\n",
            "Epoch 496, Loss:0.23793943426458797\n",
            "Epoch 497, Loss:0.23767325739511636\n",
            "Epoch 498, Loss:0.237407768078436\n",
            "Epoch 499, Loss:0.23714296378632999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod.eval(X)"
      ],
      "metadata": {
        "id": "Iz2Q6eSC82cI",
        "outputId": "4eec09dd-5fbe-4cdf-f0df-05d3d895a39f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Iz2Q6eSC82cI",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.52624232],\n",
              "       [0.52624232],\n",
              "       [0.52624232],\n",
              "       [0.52624232]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Flatten images to 1D vector of 784 features (28*28)\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "def test(model, X, Y, batch_size):\n",
        "  k = int(len(X)/batch_size)\n",
        "  for i in range(k):\n",
        "    Y_hat=model.eval(X[i*batch_size:(i+1)*batch_size])\n",
        "    wrong=0\n",
        "    for j in range(batch_size):\n",
        "      max1,max2,id1,id2=-999,-999,-1,-1\n",
        "      for l in range(10):\n",
        "        if max1 < Y_hat[j][l]:\n",
        "          max1,id1=Y_hat[j][l],l\n",
        "        if max2 < Y[i*batch_size+j][l]:\n",
        "          max2,id2=Y[i*batch_size+j][l],l\n",
        "      if id1 != id2: wrong+=1\n",
        "    print(f\"batch: {i}, accuracy: {(batch_size-wrong)/batch_size*100}%\")\n",
        "mod2 = Model(784, 10)"
      ],
      "metadata": {
        "id": "lcOYUuYAjXPG",
        "outputId": "ed652894-7c7e-4e91-c32e-2edbc6161d74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lcOYUuYAjXPG",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(mod2.Input_layer))\n",
        "mod2.train(x_train, y_train, 2048, 10, 0.01)\n",
        "for i in range(1,9):\n",
        "  plt.subplot(330+i)\n",
        "  plt.imshow(x_test[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
        "print(mod2.eval(np.array([x_test[5]])))\n",
        "print(y_test[5])\n",
        "test(mod2, x_test, y_test, 500)"
      ],
      "metadata": {
        "id": "ClmATYl3Qcb7",
        "outputId": "f19d7628-1541-4fd3-abad-e236b42aaa00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "ClmATYl3Qcb7",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n",
            "Epoch 0, Loss:0.004808464905557355\n",
            "Epoch 1, Loss:0.003966246838916834\n",
            "Epoch 2, Loss:0.003691162586209097\n",
            "Epoch 3, Loss:0.0035291976209166622\n",
            "Epoch 4, Loss:0.0034253296704297494\n",
            "Epoch 5, Loss:0.0033483774712800988\n",
            "Epoch 6, Loss:0.0032875045401488884\n",
            "Epoch 7, Loss:0.0032372898503835626\n",
            "Epoch 8, Loss:0.003192969133862954\n",
            "Epoch 9, Loss:0.0031583327451480676\n",
            "[[ 0.02918905  0.56828004  0.02383418  0.05002     0.21184505 -0.10922733\n",
            "   0.0874452   0.00263841 -0.01686543 -0.00535473]]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "batch: 0, accuracy: 78.0%\n",
            "batch: 1, accuracy: 74.2%\n",
            "batch: 2, accuracy: 69.0%\n",
            "batch: 3, accuracy: 73.0%\n",
            "batch: 4, accuracy: 76.0%\n",
            "batch: 5, accuracy: 72.39999999999999%\n",
            "batch: 6, accuracy: 76.8%\n",
            "batch: 7, accuracy: 73.8%\n",
            "batch: 8, accuracy: 71.2%\n",
            "batch: 9, accuracy: 72.8%\n",
            "batch: 10, accuracy: 87.2%\n",
            "batch: 11, accuracy: 81.0%\n",
            "batch: 12, accuracy: 87.0%\n",
            "batch: 13, accuracy: 79.60000000000001%\n",
            "batch: 14, accuracy: 80.80000000000001%\n",
            "batch: 15, accuracy: 83.8%\n",
            "batch: 16, accuracy: 82.19999999999999%\n",
            "batch: 17, accuracy: 91.60000000000001%\n",
            "batch: 18, accuracy: 82.0%\n",
            "batch: 19, accuracy: 75.4%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 8 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAGgCAYAAABCAKXYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANtlJREFUeJzt3Xt0VPW5//EnwWSCkEwIkIQcGIi3oqKoIQkBRbRZRlQURItHTsXLkYITKuKtEQXLsWYtPFWOGOS0R4mXcim2oELLUQOEYgM2qegKwXgpSjwwA6iZCQGSmOzfH13Oj++GTDL3vfe8X2vNWvOZ2TPzZPLAkz179t4JmqZpAgAALCkx1gUAAIDIYdADAGBhDHoAACyMQQ8AgIUx6AEAsDAGPQAAFsagBwDAwhj0AABYGIMeAAALY9ADAGBhERv0FRUVMmLECElJSZHCwkL54IMPIvVSQFjRuzArehenkxCJY92vXbtW7rjjDlmxYoUUFhbK0qVLZd26ddLY2CiZmZl+H9vV1SUHDhyQ1NRUSUhICHdpiABN06SlpUVycnIkMdHcHxLRu/GF3v0netd8AupdLQIKCgo0p9Ppy52dnVpOTo5WXl7e42Obmpo0EeFiwktTU1Mk2imq6N34vNC79K5ZL73p3bD/Cdve3i51dXVSXFzsuy0xMVGKi4ulpqbmlOXb2trE6/X6Lhon0zOt1NTUWJcQEno3ftG79K5Z9aZ3wz7ojxw5Ip2dnZKVlaXcnpWVJS6X65Tly8vLxW63+y4OhyPcJSFKzP6RH70bv+hdetesetO7Md8oVVZWJh6Px3dpamqKdUlAr9C7MCt6N76cEe4nHDRokPTp00fcbrdyu9vtluzs7FOWt9lsYrPZwl0GEDB6F2ZF78KfsK/RJycnS15enlRVVflu6+rqkqqqKikqKgr3ywFhQ+/CrOhd+BX8dzy7t2bNGs1ms2mVlZVaQ0ODNmvWLC09PV1zuVw9Ptbj8cT8W4xcgrt4PJ5ItFNU0bvxeaF36V2zXnrTuxEZ9JqmacuWLdMcDoeWnJysFRQUaDt37uzV42g4816s8J+lptG78Xihd+lds15607sROWBOKLxer9jt9liXgSB4PB5JS0uLdRkxQ++aF71L75pVb3o35t+6BwAAkcOgBwDAwsK+e53VPfTQQ0ru27evki+++GIl33LLLd0+14svvqhk/RGsXnvttWBKBADAhzV6AAAsjEEPAICFMegBALAwttH3YO3atUr2t839dLq6urq972c/+5mSTz7zlIhIdXW1kvfv3x/QawPRct555yn5k08+UfL999+v5GXLlkW8JsSPfv36KfmZZ57xXdf/P1tXV6fkW2+9VclfffVVmKuLPdboAQCwMAY9AAAWxkf3OqF+VK//yPJ///d/fdfPOuss5b7Jkycr+eyzz1byjBkzlFxeXh5QLUC0XHrppUrWb7L6+uuvo1kO4syQIUOUfO+99/qu63sxLy9PyTfccIOSKyoqwlxd7LFGDwCAhTHoAQCwMAY9AAAWFvfb6MeMGaPkqVOn+l1+z549Sr7xxhuVfOTIESUfPXrUdz05OVm5b+fOnUoePXq0kgcOHOi3FsAoLrnkEiW3trYqef369VGsBlY3ePBgJb/yyisxqsQcWKMHAMDCGPQAAFgYgx4AAAuL+230+v0vExISlKzfJl9SUqLkgwcP9vq1HnzwQSVfcMEFfpfftGlTr58biKZRo0YpubS0VMmcYhnh9POf/1zJU6ZMUXJBQUHQzz1hwgQlJyaq678fffSRkrdv3x70a8UKa/QAAFgYgx4AAAtj0AMAYGFxv43+7bffVvI555yj5JaWFiV/++23Qb/WbbfdpuSkpKSgnwuIpZEjRypZf5pQ/TkjgFA899xzSvZ3+u9A3XzzzX6z/rS106dPV7L+tLdGxBo9AAAWxqAHAMDCGPQAAFhY3G+j19NvjwnVww8/7Lt+3nnn+V12165dfjNgFI888oiS9f9uamtro1kOLOZPf/qTkvX7tofim2++UfLJ5yMRERk+fLiSc3NzlfzBBx8ouU+fPmGrLVJYowcAwMIY9AAAWFjAg3779u0yefJkycnJkYSEBNmwYYNyv6ZpsnDhQhkyZIj07dtXiouL5bPPPgtXvUDQ6F2YFb2LUAS8jb61tVVGjx4td9999yn7G4qILFmyRJ5//nl55ZVXJDc3V5544gkpKSmRhoYGSUlJCUvRRnbDDTcoefHixb7r+vPRHzp0SMllZWVKPnbsWJiri2/0bvBGjBih5DFjxij5008/VbL+fPQIjdV798orr1Tyj370IyXr95sPZD/6FStWKPmdd95RssfjUfLVV1+t5AULFvh9/jlz5ij5xRdf7HVt0RLwoJ80aZJMmjTptPdpmiZLly6Vxx9/XG666SYREXn11VclKytLNmzYcMoBY0RE2trapK2tzZe9Xm+gJQG9Qu/CrOhdhCKs2+j37dsnLpdLiouLfbfZ7XYpLCyUmpqa0z6mvLxc7Ha77zJs2LBwlgT0Cr0Ls6J30ZOwDnqXyyUiIllZWcrtWVlZvvv0ysrKxOPx+C5NTU3hLAnoFXoXZkXvoicx34/eZrOJzWaLdRlho992qd8ufzL98cCrq6sjUhMiw2q9649+G6re4cOHo1QJwiHWvav/zseaNWuUPGjQoICeT38chz/84Q++67/85S+V+3r67pP+uWbNmqXkwYMHK3nJkiVK1n8n4oUXXlByR0eH39ePhLCu0WdnZ4uIiNvtVm53u92++wAjondhVvQuehLWQZ+bmyvZ2dlSVVXlu83r9cquXbukqKgonC8FhBW9C7Oid9GTgD+6P3r0qHz++ee+vG/fPtm9e7dkZGSIw+GQefPmyVNPPSXnnnuubzePnJwcmTJlSjjrBgJG78Ks6F2EIuBBX1tbK1dddZUvz58/X0REZs6cKZWVlfLII49Ia2urzJo1S5qbm+Xyyy+XzZs3m2JfzmDoD1xxzTXXdLvsq6++quTHH388EiWhG/Ru8C666CK/9+u3UyK8rNa7Z5yhjp5At8nrv8+k34XwyJEjwRUmp26jLy8vV/Kzzz6r5DPPPFPJ+n8Lb731lpK/+OKLoGsLVsCDfuLEiaJpWrf3JyQkyOLFi5UDxQBGQO/CrOhdhIJj3QMAYGEMegAALCzm+9GbzZAhQ5Q8btw4Jev3TT15W9FTTz2l3Kc/DzJgJGPHjvVdv+uuu5T7PvzwQyW/++67UakJ8am2tlbJd999t5JD2SbfE/029hkzZig5Pz8/Yq8dLqzRAwBgYQx6AAAsjI/uA3TyoRVFRAYOHOh3+ddff913PRa7VQDBOvkkKRkZGcp9mzdvVvKJEyeiUhOsKTHR/zpnYWFhlCo5VUJCgpL1tfZU+5NPPqnkn/70p2GpKxCs0QMAYGEMegAALIxBDwCAhbGNvgc33nijki+77DK/y2/btk3JixYtCndJQFSMHj3ad11/VLY33ngj2uXAQmbPnq3krq6uGFXSs8mTJyv50ksvVbK+dn3Wb6OPBdboAQCwMAY9AAAWxqAHAMDC2Eavo98v/rHHHlNyUlKS38fv3r1byRzmFmaRnZ2t5CuuuMJ3vbGxUblv/fr1UakJ1qTf7h1LgwcPVvIFF1ygZP0M6Mnhw4eV3NHREVxhYcQaPQAAFsagBwDAwhj0AABYGNvodR588EEl93QKwg0bNiiZ/eZhVnfeeaeSMzMzfdf//Oc/R7kaIDoWLFigZKfTGdDjv/zySyXPnDlTyfv37w+qrnBijR4AAAtj0AMAYGEMegAALIxt9Drz588PaPnS0lIls988zGr48OHd3vfdd99FsRIgcv70pz8p+Uc/+lFIz9fQ0KDkHTt2hPR8kcAaPQAAFsagBwDAwhj0AABYGNvoQ5SRkaHkUI5r7PF4/D6X/jj7drvd7/Olp6crOZDvH3R2dir50UcfVfKxY8d6/VwwhxtuuKHb+95+++0oVgKrS0hIUHJiov91zkmTJvm9/ze/+Y2Sc3Jyul1W/1r688cHykjH7e8Oa/QAAFgYgx4AAAsLaNCXl5dLfn6+pKamSmZmpkyZMuWU01eeOHFCnE6nDBw4UPr37y/Tpk0Tt9sd1qKBQNG7MCt6F6EKaBt9dXW1OJ1Oyc/Pl++//14ee+wxueaaa6ShoUH69esnIiIPPPCAbNq0SdatWyd2u11KS0vl5ptvlvfffz8iP0Csffzxx2F7rnXr1in54MGDSs7KylLy9OnTw/baPXG5XEr+1a9+FbXXDgd691SXX365kvXno4cxWLF3X3zxRSUvWbLE7/IbN25Uck/b1QPZ7h7oNvoVK1YEtLwRBDToN2/erOTKykrJzMyUuro6mTBhgng8HnnppZdk1apVcvXVV4uIyMqVK+X888+XnTt3ytixY095zra2Nmlra/Nlr9cbzM8B+EXvwqzoXYQqpG30P3xL/IdvntfV1UlHR4cUFxf7lhk5cqQ4HA6pqak57XOUl5eL3W73XYYNGxZKSUCv0LswK3oXgQp60Hd1dcm8efNk/PjxMmrUKBH558e7ycnJp+zWlZWVdcpHvz8oKysTj8fjuzQ1NQVbEtAr9C7Mit5FMILej97pdEp9fX3Ix/W12Wxis9lCeo5w0h8H+aabboraa996660hPf77779Xck/bnt566y3f9draWr/L/uUvfwm+MIOxau8GaurUqUru06ePkj/88EPf9e3bt0elJvhnld794x//qOSHH35YyYMHD45aLYcPH1by3r17lTxr1iwl6787ZQZBrdGXlpbKxo0bZevWrTJ06FDf7dnZ2dLe3i7Nzc3K8m63my/6wBDoXZgVvYtgBTToNU2T0tJSWb9+vWzZskVyc3OV+/Py8iQpKUmqqqp8tzU2Nsr+/fulqKgoPBUDQaB3YVb0LkIV0Ef3TqdTVq1aJW+++aakpqb6tv/Y7Xbp27ev2O12ueeee2T+/PmSkZEhaWlpMnfuXCkqKjrtNz+BaKF3YVb0LkKVoGma1uuFdccn/sHKlSvlzjvvFJF/HrjhwQcflNWrV0tbW5uUlJTI8uXLe/0Rktfr7fEY7tH0yCOPKFl/vPmeXHjhhb7rge73/vLLLyv5yy+/9Lv8H/7wByV/8sknAb1eqDwej6SlpUX1NXsrHntX78wzz1RyXV2dkvXn5V6wYIHvenl5eeQKMwB6N7a9O2HCBCVPmTJFyffff7+SQzk+vf5Y9z//+c+VXFFREfRzx0JvejegNfre/E2QkpIiFRUVpnuzYG30LsyK3kWoONY9AAAWxqAHAMDCAtpGHw2x3laE4Bl5O2c0GL139d8vqa6uVvKhQ4eUfPvtt/uuHzt2LHKFGQC9a+zevfbaa5Ws37ddf074k48Roj9Xvf47Dw0NDUrev39/0HXGQm96lzV6AAAsjEEPAICF8dE9woaPP+lds6J36V2z4qN7AADiHIMeAAALY9ADAGBhDHoAACyMQQ8AgIUx6AEAsDAGPQAAFsagBwDAwhj0AABYGIMeAAALY9ADAGBhDHoAACyMQQ8AgIUx6AEAsDDDDXqDnTUXAYj33128//xmFu+/u3j/+c2sN787ww36lpaWWJeAIMX77y7ef34zi/ffXbz//GbWm99dgmawP+W6urrkwIEDommaOBwOaWpqkrS0tFiXZRper1eGDRsW1fdN0zRpaWmRnJwcSUw03N+OUUPvhobejR16NzRG790zolJRABITE2Xo0KHi9XpFRCQtLY2GC0K03ze73R611zIqejc86N3oo3fDw6i9G79/wgIAEAcY9AAAWJhhB73NZpNFixaJzWaLdSmmwvsWe/wOgsP7Fnv8DoJj9PfNcF/GAwAA4WPYNXoAABA6Bj0AABbGoAcAwMIY9AAAWBiDHgAACzPsoK+oqJARI0ZISkqKFBYWygcffBDrkgyjvLxc8vPzJTU1VTIzM2XKlCnS2NioLHPixAlxOp0ycOBA6d+/v0ybNk3cbneMKo4v9G736F1jo3e7Z+re1QxozZo1WnJysvbyyy9re/bs0e69914tPT1dc7vdsS7NEEpKSrSVK1dq9fX12u7du7XrrrtOczgc2tGjR33LzJ49Wxs2bJhWVVWl1dbWamPHjtXGjRsXw6rjA73rH71rXPSuf2buXUMO+oKCAs3pdPpyZ2enlpOTo5WXl8ewKuM6dOiQJiJadXW1pmma1tzcrCUlJWnr1q3zLbN3715NRLSamppYlRkX6N3A0LvGQe8Gxky9a7iP7tvb26Wurk6Ki4t9tyUmJkpxcbHU1NTEsDLj8ng8IiKSkZEhIiJ1dXXS0dGhvIcjR44Uh8PBexhB9G7g6F1joHcDZ6beNdygP3LkiHR2dkpWVpZye1ZWlrhcrhhVZVxdXV0yb948GT9+vIwaNUpERFwulyQnJ0t6erqyLO9hZNG7gaF3jYPeDYzZetdwp6lFYJxOp9TX18uOHTtiXQoQEHoXZmW23jXcGv2gQYOkT58+p3xT0e12S3Z2doyqMqbS0lLZuHGjbN26VYYOHeq7PTs7W9rb26W5uVlZnvcwsujd3qN3jYXe7T0z9q7hBn1ycrLk5eVJVVWV77auri6pqqqSoqKiGFZmHJqmSWlpqaxfv162bNkiubm5yv15eXmSlJSkvIeNjY2yf/9+3sMIond7Ru8aE73bM1P3bqS+5ffCCy9ow4cP12w2m1ZQUKDt2rWr149ds2aNZrPZtMrKSq2hoUGbNWuWlp6errlcrkiVaypz5szR7Ha7tm3bNu3gwYO+y7Fjx3zLzJ49W3M4HNqWLVu02tparaioSCsqKoph1eZB70YOvRtZ9G7kmLl3I3Ka2rVr18odd9whK1askMLCQlm6dKmsW7dOGhsbJTMz0+9ju7q65MCBA7Jq1SpZtmyZuN1uufjii2XJkiUyZsyYcJdqSna7/bS3L1++XGbMmCEi/zxww4IFC+SNN96QtrY2+fGPfyzPPvvsKV+2CQdN06SlpUVycnIkMdFwHxIFhN6NLHo3cujdyDJ170bir4dQ9sdsamrSRISLCS9NTU2RaKeoonfj80Lv0rtmvfSmd8P+J2yg+2O2tbWJ1+v1XbTwf8CAKElNTY11CSGhd+MXvUvvmlVvejfsgz7Q/THLy8vFbrf7Lg6HI9wlIUoSEhJiXUJI6N34Re/Su2bVm96N+UapsrIy8Xg8vktTU1OsSwJ6hd6FWdG78SXsB8wJdH9Mm80mNpst3GUAAaN3YVb0LvwJ+xo9+2PCrOhdmBW9C7+C/45n90LZH9Pj8cT8W4xcgrt4PJ5ItFNU0bvxeaF36V2zXnrTuxE7YM6yZcs0h8OhJScnawUFBdrOnTt79TgazrwXK/xnqWn0bjxe6F1616yX3vRuRA6YEwqv19vtgQlgbB6PR9LS0mJdRszQu+ZF79K7ZtWb3o35t+4BAEDkMOgBALAwBj0AABbGoAcAwMIY9AAAWBiDHgAAC2PQAwBgYQx6AAAsjEEPAICFMegBALCwsJ+m1uouu+wyJf/xj39U8ogRI6JWyzXXXKPkvXv3KplzTCNaJk+erOS33npLyaWlpUpesWKFkjs7OyNTGEwpMzNTyb///e+V/Ne//lXJv/nNb5T85ZdfRqSu3tAfSnjChAlK3rx5s+96R0dHVGpijR4AAAtj0AMAYGEMegAALIxt9AEqKSlRss1mi1Elp24Xvfvuu5V82223RbMcxJGBAwcqefny5X6Xf+GFF5T88ssvK/n48ePhKQymNGDAACXv2bNHyfrt3m63W8lG2iZfV1en5MGDBys5Ly/Pd/3zzz+PXGEnYY0eAAALY9ADAGBhDHoAACyMbfQ9OOMM9S267rrrYlTJqfTbgubPn6/kfv36Kbm1tTXiNSE+6PcNHjp0qN/lV69ereQTJ06EvSaYx6BBg5S8du1aJWdkZChZ/x2QuXPnRqawIDz++ONKzs3NVfLPfvYzJUdru/zJWKMHAMDCGPQAAFgYgx4AAAtjG30PrrrqKiUXFRUpecmSJdEsR6Hf9/SCCy5Q8plnnqlkttEjWPrjRSxYsCCgx7/22mtK1jQt5JpgXvpzhkycONHv8osXL45gNYG58MILlfzggw8qef369UrWf/8gFlijBwDAwhj0AABYGIMeAAALYxu9zqhRo5Ss3//3iy++UPLTTz8d8Zq6c9NNN8XstRFfLrroIiWffLzu0/n++++V/Oc//znsNcE89OeXnzZtmt/l77nnHiUfPnw47DX1ln6b/Hvvved3ef02+paWlrDXFCjW6AEAsLCAB/327dtl8uTJkpOTIwkJCbJhwwblfk3TZOHChTJkyBDp27evFBcXy2effRaueoGg0bswK3oXoQh40Le2tsro0aOloqLitPcvWbJEnn/+eVmxYoXs2rVL+vXrJyUlJRzyEjFH78Ks6F2EIuBt9JMmTZJJkyad9j5N02Tp0qXy+OOP+7Yfv/rqq5KVlSUbNmwwxfnR9cct1h8v/tprr1Xy0aNHI17TD/THf77yyiuV3NXVFbVazMjqvRtJPW1T1XvnnXciVEl8Mnvv/vrXv1byv/3bvylZf96OdevWRbym3rriiiuUnJWVpeTKykolv/7665EuKWBh3Ua/b98+cblcUlxc7LvNbrdLYWGh1NTUnPYxbW1t4vV6lQsQbfQuzIreRU/COuhdLpeInPoXT1ZWlu8+vfLycrHb7b7LsGHDwlkS0Cv0LsyK3kVPYv6t+7KyMvF4PL5LU1NTrEsCeoXehVnRu/ElrPvRZ2dni4iI2+2WIUOG+G53u91yySWXnPYxNpvtlONoR9Mtt9yiZP355vXnDq6trY14Td3RH19cv01+27ZtSm5ubo5wRdZhxt6NJv355/Xa29uVHOix8BE8M/Su/twG+v+7Dhw4oGR9P0VS3759lfzYY48p+b777lOy/me5++67I1NYGIV1jT43N1eys7OlqqrKd5vX65Vdu3adcjIYwEjoXZgVvYueBLxGf/ToUWUtd9++fbJ7927JyMgQh8Mh8+bNk6eeekrOPfdcyc3NlSeeeEJycnJkypQp4awbCBi9C7OidxGKgAd9bW2tcurW+fPni4jIzJkzpbKyUh555BFpbW2VWbNmSXNzs1x++eWyefNmSUlJCV/VQBDoXZgVvYtQJGgGOzG01+sVu90etdfTnytYv7/w3Llzlfziiy9GvKYfjBgxQsk7d+5Usn6/+pKSEiVv3bo1InV1x+PxSFpaWlRf00ii3buRNm7cON/1999/3++y3333nZL1vWl09G5ke/fVV19V8owZM/wuv337diXrv28Uyv/D+uOPTJw4Ucljx471+/g33nhDydOnTw+6lnDoTe/G/Fv3AAAgchj0AABYGIMeAAALi7vz0eu3Q/W0PSaa2+T1Zs2apeRBgwYpee/evUqO9jZ5WFt+fn6vl43lvxMY33/9138p+eQvFoqI5OTkKFl/3IaEhAQl33jjjUHXon+unr6m9o9//EPJ+v3szYA1egAALIxBDwCAhcXdR/f6wz7+y7/8i5JXr14dzXL8Ovvss/3eX19fH6VKEI/GjBnT7X3h3N0J1qc/De3FF1+sZP2hevWnA3/44YeVfPjwYSW/8sorva7ltddeU/JHH33kd/m//vWvSv7iiy96/VpGwRo9AAAWxqAHAMDCGPQAAFhY3G2jb2lpUfLu3buVrN92pD+U57fffhuRukREMjMzlaw/ha7ejh07IlYL4s/ll1+u5Ntvv73bZT0ej5K//vrriNQEa9IfMlm/a7A+P/roo2F77bPOOkvJ+t3t9DPhoYceCttrxwpr9AAAWBiDHgAAC2PQAwBgYXG3jf748eNK1u8TqT9N7aZNm5T87LPPBv3ao0aNUrJ+W5H+tLQ9HZqxq6sr6FoAvYEDByo5MbH79YB333030uUAEbFw4UIl6/+f1X8fQL/PvhmxRg8AgIUx6AEAsDAGPQAAFhZ32+j1Fi1apGT9PpXXX3+9kkM5Fv6RI0eUrN82pD8NbU8qKyuDrgXQ83fcBv2x7f/7v/87wtUA4XHrrbcq+Y477lCy/tgq33zzTcRrijbW6AEAsDAGPQAAFsagBwDAwuJ+G/0nn3yi5J/85CdK1p8n+Zxzzgn6td544w2/9+vPqTxjxgy/y+uPCQAEYujQoUr2d2x7/bHsa2trI1ITEG6TJk3ye//GjRuV/Pe//z2S5cQEa/QAAFgYgx4AAAtj0AMAYGFxv42+J/pzE+tzOP3jH/8IaHn9sfPr6+vDWQ4sbty4cUr2d2z7DRs2RLgaIDL02+hbW1uV/Otf/zqa5cQEa/QAAFhYQIO+vLxc8vPzJTU1VTIzM2XKlCnS2NioLHPixAlxOp0ycOBA6d+/v0ybNk3cbndYiwYCRe/CrOhdhCqgQV9dXS1Op1N27twp7777rnR0dMg111yjfBTywAMPyNtvvy3r1q2T6upqOXDggNx8881hLxwIBL0Ls6J3EaoEraeTnvtx+PBhyczMlOrqapkwYYJ4PB4ZPHiwrFq1ynfc7E8++UTOP/98qampkbFjx/b4nF6vV+x2e7AlmdqTTz6p5CeeeMLv8n369IlgNYHzeDySlpYW6zJ6hd4VmTNnjpKXL1+u5JPPzXD++ed3e58V0Lvm6t2ezJ4923dd39eHDh1ScnZ2dlRqipTe9G5I2+g9Ho+IiGRkZIiISF1dnXR0dEhxcbFvmZEjR4rD4ZCamprTPkdbW5t4vV7lAkQavQuzoncRqKAHfVdXl8ybN0/Gjx/v+/a3y+WS5ORkSU9PV5bNysoSl8t12ucpLy8Xu93uuwwbNizYkoBeoXdhVvQughH0oHc6nVJfXy9r1qwJqYCysjLxeDy+S1NTU0jPB/SE3oVZ0bsIRlD70ZeWlsrGjRtl+/btyvGys7Ozpb29XZqbm5W/Lt1ud7fbQWw2m9hstmDKsBz91yVC+PoEukHv/n8lJSV+79+/f7/v+g8fFyN26N3eO3kbvf7/0U2bNvl9bGpqqpIHDBig5JP/XZhFQGv0mqZJaWmprF+/XrZs2SK5ubnK/Xl5eZKUlCRVVVW+2xobG2X//v1SVFQUnoqBINC7MCt6F6EKaI3e6XTKqlWr5M0335TU1FTf9h+73S59+/YVu90u99xzj8yfP18yMjIkLS1N5s6dK0VFRb365icQKfQuzIreRagCGvQvvviiiIhMnDhRuX3lypVy5513iojIc889J4mJiTJt2jRpa2uTkpKSU3ZvAKKN3oVZ0bsIVUCDvjfbjFNSUqSiokIqKiqCLipepaSk+L2f888Hj94VSUpKUvLZZ5/td/kTJ074rnd0dESkJvSM3g2vzs5OJc+YMUPJDzzwgJL37Nmj5JkzZ0amsAjiWPcAAFgYgx4AAAtj0AMAYGGcj95A7rrrLiU3Nzcr+T/+4z+iWA2spqurS8m1tbVK/uFIaz/4/PPPI14TEG3//u//ruR77rlHyS+99JKSrfD/Lmv0AABYGIMeAAAL46N7A/nb3/6m5GeffVbJW7dujWY5sBj9bkULFixQsn43rrq6uojXBERCaWmp7/rixYuV+7Zv367kH45T8IPvvvtOye3t7WGuLvpYowcAwMIY9AAAWBiDHgAAC0vQDHYuVK/XK3a7PdZlIAgej0fS0tJiXUbM0LvmRe/Su2bVm95ljR4AAAtj0AMAYGEMegAALIxBDwCAhTHoAQCwMAY9AAAWxqAHAMDCGPQAAFgYgx4AAAtj0AMAYGGGG/QGOyIvAhDvv7t4//nNLN5/d/H+85tZb353hhv0LS0tsS4BQYr33128//xmFu+/u3j/+c2sN787w53UpqurSw4cOCCaponD4ZCmpqa4PtlEoLxerwwbNiyq75umadLS0iI5OTmSmGi4vx2jht4NDb0bO/RuaIzeu2dEpaIAJCYmytChQ8Xr9YqISFpaGg0XhGi/b5z5it4NF3o3+ujd8DBq78bvn7AAAMQBBj0AABZm2EFvs9lk0aJFYrPZYl2KqfC+xR6/g+DwvsUev4PgGP19M9yX8QAAQPgYdo0eAACEjkEPAICFMegBALAwBj0AABZm2EFfUVEhI0aMkJSUFCksLJQPPvgg1iUZRnl5ueTn50tqaqpkZmbKlClTpLGxUVnmxIkT4nQ6ZeDAgdK/f3+ZNm2auN3uGFUcX+jd7tG7xkbvds/UvasZ0Jo1a7Tk5GTt5Zdf1vbs2aPde++9Wnp6uuZ2u2NdmiGUlJRoK1eu1Orr67Xdu3dr1113neZwOLSjR4/6lpk9e7Y2bNgwraqqSqutrdXGjh2rjRs3LoZVxwd61z9617joXf/M3LuGHPQFBQWa0+n05c7OTi0nJ0crLy+PYVXGdejQIU1EtOrqak3TNK25uVlLSkrS1q1b51tm7969mohoNTU1sSozLtC7gaF3jYPeDYyZetdwH923t7dLXV2dFBcX+25LTEyU4uJiqampiWFlxuXxeEREJCMjQ0RE6urqpKOjQ3kPR44cKQ6Hg/cwgujdwNG7xkDvBs5MvWu4QX/kyBHp7OyUrKws5fasrCxxuVwxqsq4urq6ZN68eTJ+/HgZNWqUiIi4XC5JTk6W9PR0ZVnew8iidwND7xoHvRsYs/Wu4c5eh8A4nU6pr6+XHTt2xLoUICD0LszKbL1ruDX6QYMGSZ8+fU75pqLb7Zbs7OwYVWVMpaWlsnHjRtm6dasMHTrUd3t2dra0t7dLc3OzsjzvYWTRu71H7xoLvdt7Zuxdww365ORkycvLk6qqKt9tXV1dUlVVJUVFRTGszDg0TZPS0lJZv369bNmyRXJzc5X78/LyJCkpSXkPGxsbZf/+/byHEUTv9ozeNSZ6t2em7t2YfhWwG2vWrNFsNptWWVmpNTQ0aLNmzdLS09M1l8sV69IMYc6cOZrdbte2bdumHTx40Hc5duyYb5nZs2drDodD27Jli1ZbW6sVFRVpRUVFMaw6PtC7/tG7xkXv+mfm3o3YoH/hhRe04cOHazabTSsoKNB27doV0OOXLVumORwOLTk5WSsoKNB27twZoUrNR0ROe1m5cqVvmePHj2v33XefNmDAAO3MM8/Upk6dqh08eDB2RZsIvRs59K6x0bvdM3PvRuQ0tWvXrpU77rhDVqxYIYWFhbJ06VJZt26dNDY2SmZmpt/HdnV1yYEDByQ1NVUSEhLCXRoiQNM0aWlpkZycHElMNNzWoIDQu/HFSr0LdCsSfz2EcuCFpqambv9y4mLsS1NTUyTaKaro3fi8WKF3ge6E/U/YQA+80NbWJl6v13fRwv8BA6IkNTU11iWEhN6NX2bvXcCfsA/6QA+8UF5eLna73XdxOBzhLglRYvaPq+nd+GX23gX8iflGqbKyMvF4PL5LU1NTrEsCeoXeBWAGYT8yXqAHXrDZbGKz2cJdBhAweheAFYV9jZ4DL8Cs6F0AVhSRY93Pnz9fZs6cKWPGjJGCggJZunSptLa2yl133RWJlwPCht4FYDURGfTTp0+Xw4cPy8KFC8Xlcskll1wimzdvPuVLToDR0LsArCYiB8wJhdfrFbvdHusyEASPxyNpaWmxLiNm6F3zivfehbXF/Fv3AAAgchj0AABYGIMeAAALY9ADAGBhDHoAACyMQQ8AgIVFZD96APFtwIABSg70hD9fffWVkh944AEl19fX+65/+umnyn0fffRRQK8FWB1r9AAAWBiDHgAAC+OjewABu/7665V84403KnnixIlKPueccwJ6fv3H8cOHD1eyv7MG9unTJ6DXAqyONXoAACyMQQ8AgIUx6AEAsDC20YdIf8ar8vJyJY8aNcp3vbi4WLmvo6MjcoUBATr77LOV7HQ6fdfvvfde5b6+ffsqOSEhIay1nHfeeWF9PiCesUYPAICFMegBALAwBj0AABbGNvoAzZgxQ8m/+tWvlDxs2LBuH6vfnv/NN9+ErzAgREOHDlXy/fffH7XX/uSTT5S8Z8+eqL02YHWs0QMAYGEMegAALIxBDwCAhbGNvgf67ZZLly5V8sCBA5WsaVq3z7Vs2TIll5aWKvnbb78NokLgnwYNGqRk/Tb2999/X8mbN29Wcltbm5I9Ho/vemtrq3Jfv379lPzOO+8o+eTTyIqI7Nq1S8kffvihko8fP65k/esBCB5r9AAAWBiDHgAAC2PQAwBgYWyj78FDDz2k5IyMjKCfa/r06Uq+9tprlazfJ1+/Tb+9vT3o14b19LSdfPTo0UqeOnWq3+fbuXOnki+77DLf9S+//FK5z+FwKPnrr79WcldXl9/XAhA9rNEDAGBhDHoAACws4EG/fft2mTx5suTk5EhCQoJs2LBBuV/TNFm4cKEMGTJE+vbtK8XFxfLZZ5+Fq14gaPQugHgU8Db61tZWGT16tNx9991y8803n3L/kiVL5Pnnn5dXXnlFcnNz5YknnpCSkhJpaGiQlJSUsBQdScOHD1fyXXfd5Xf5jz/+WMlut1vJ+nPQn8xutytZ/32A3/3ud0p2uVx+a4F/Zu/d5ORkJa9atUrJ+m3yTz/9tJLfe++9gF5Pv13+ZPv37w/ouQDETsCDftKkSTJp0qTT3qdpmixdulQef/xxuemmm0RE5NVXX5WsrCzZsGGD3Hbbbac8pq2tTTlQh9frDbQkoFfoXQDxKKzb6Pft2ycul0tZi7Xb7VJYWCg1NTWnfUx5ebnY7Xbfxd/Z34BIoXcBWFVYB/0PHy1nZWUpt2dlZXX7sXNZWZl4PB7fpampKZwlAb1C7wKwqpjvR2+z2cRms8W6DJ9LLrlEyampqUr+y1/+ouQrr7xSyfptuf/6r//qu/7YY48p95199tlKzs7OVvKbb76pZP3HzhwbP7Yi3bv9+/dXcllZmZJvuOEGJR85ckTJ//mf/6nkY8eOhbE6AGYR1jX6HwaV/gtpbrf7lCEGGAm9C8Cqwjroc3NzJTs7W6qqqny3eb1e2bVrlxQVFYXzpYCwoncBWFXAH90fPXpUPv/8c1/et2+f7N69WzIyMsThcMi8efPkqaeeknPPPde3i1JOTo5MmTIlnHUDAaN3AcSjgAd9bW2tXHXVVb48f/58ERGZOXOmVFZWyiOPPCKtra0ya9YsaW5ulssvv1w2b95siP2Qe0O/zVV/fvnnnnvO7+NPnDih5JUrV/qu33rrrcp9Z511lt/n0m9T5Vj3oTFb7+r/wPjFL36hZP2+7FdccYWSTz6fPID4FfCgnzhx4inD72QJCQmyePFiWbx4cUiFAeFG7wKIRxzrHgAAC2PQAwBgYTHfj95oTt7v/XSuv/56JetPjOLPmDFjAqpFf37wo0ePBvR4mNu4ceP83v/hhx8qWX9OeAAQYY0eAABLY9ADAGBhfHSvs3r1aiXfeOONSs7Pz1fyyJEjlXzRRRcpeerUqb7rAwYMUO5rbm5Wsv7+e++9V8mvvfaakhsaGgTWdcstt/i9/9prr1XyokWLlKw/hPLu3bvDUhcAc2GNHgAAC2PQAwBgYQx6AAAsLEHzd6iwGPB6vWK322P2+hkZGUo++djoInJKbQkJCUr293a+9957SnY6nUreuHGjks8991wl//a3v1Xy7Nmzu32tWPB4PJKWlhbrMmIm3L2r76Wurq6AHq9ffsWKFUrW777pcDiUfHLv79mzx+9rXXjhhUquqalRstF3/Yv33oW1sUYPAICFMegBALAwBj0AABbGNvoeFBcXK/mNN95Qsr5W/du5bNky3/VHH31UuU9/Stunn35ayfrTkn711Vd+a/viiy8kluJ9O2e4e/eZZ55R8g+n1TWDw4cPK3nbtm1Kvu2226JYTc/ivXdhbazRAwBgYQx6AAAsjEEPAICFsY0+QPrt4rfffruS9cevX7hwoe96T6eZ7du3r5JXrVqlZP1x919//XUlz5w50+/zR1q8b+cMd+/26dNHyZdeeqmS9f1xxhnqqSuGDRum5MTE2P1dr/9v5sknn1TyU089FcVqThXvvQtrY40eAAALY9ADAGBhDHoAACyMbfQGpt/X+He/+52S/+///k/Jl1xyiZK//fbbiNTVnXjfzmm03v3xj3+s5KSkJCXrt5Pn5+dHuiSft956S8lTp06N2mufTrz3LqyNNXoAACyMQQ8AgIUx6AEAsLAzel4EsfL73/9eyfr96KdPn67k0tJSJS9evDgyhcEUqqqq/N6v/06Hfhv9999/77u+cuVK5b7f/va3Sp43b56S9ceXABA7rNEDAGBhDHoAACwsoEFfXl4u+fn5kpqaKpmZmTJlyhRpbGxUljlx4oQ4nU4ZOHCg9O/fX6ZNmyZutzusRQOBoncBxKuA9qO/9tpr5bbbbpP8/Hz5/vvv5bHHHpP6+nppaGiQfv36iYjInDlzZNOmTVJZWSl2u11KS0slMTFR3n///V69htH2RTYS/TZV/XuakpKi5PPPP1/Jn376aUTq+oGR90Wmd0912WWXKflvf/tbrx+7detWJU+cOFHJCQkJfh+/fPlyJc+dO7fXrx0JRu5dIFQBfRlv8+bNSq6srJTMzEypq6uTCRMmiMfjkZdeeklWrVolV199tYj880s8559/vuzcuVPGjh17ynO2tbVJW1ubL3u93mB+DsAvehdAvAppG73H4xERkYyMDBERqaurk46ODuUMbyNHjhSHwyE1NTWnfY7y8nKx2+2+i/6MW0Ak0LsA4kXQg76rq0vmzZsn48ePl1GjRomIiMvlkuTkZElPT1eWzcrKEpfLddrnKSsrE4/H47s0NTUFWxLQK/QugHgS9H70TqdT6uvrZceOHSEVYLPZxGazhfQc8WL37t1KPvlc9yIizzzzjJKffvppJf/0pz9V8vHjx8NXnInQu/+0d+9eJeuP2/CTn/yk28deddVVfp+7s7NTyZs2bVLyL37xi96UCCAMglqjLy0tlY0bN8rWrVtl6NChvtuzs7Olvb1dmpubleXdbrdkZ2eHVCgQDvQugHgT0KDXNE1KS0tl/fr1smXLFsnNzVXuz8vLk6SkJOWIXI2NjbJ//34pKioKT8VAEOhdAPEqoI/unU6nrFq1St58801JTU31bbu02+3St29fsdvtcs8998j8+fMlIyND0tLSZO7cuVJUVHTaby0D0ULvAohXAe1H392+sStXrpQ777xTRP550JEHH3xQVq9eLW1tbVJSUiLLly/v9cefZtsXOZYGDx6sZP3+3uecc46S9fvhf/zxx2Gtx8j7ItO7PcvKylLy//zP//iujxkzRrkvMzNTyV9++aWSX3vtNSU/+eSToRcYQUbuXSBUAa3R9+ZvgpSUFKmoqJCKioqgiwLCjd4FEK841j0AABbGoAcAwMIC2kYfDWbfzhlLDodDyfrtpqtXr1byjBkzwvr68b6d08q9qz8Gg/4Lir/85S+VfOjQoYjXFE7x3ruwNtboAQCwMAY9AAAWxkf3FvbOO+8oWX/gl8LCQt/1hoaGkF8v3j/+pHfNK957F9bGGj0AABbGoAcAwMIY9AAAWFjQp6mF8d1yyy1K/uijj5R88iFyw7GNHgBgPKzRAwBgYQx6AAAsjEEPAICFsY3ewrxer5Jzc3NjVAkAIFZYowcAwMIY9AAAWBiDHgAAC2PQAwBgYQx6AAAsjEEPAICFGW7QG+ysuQhAvP/u4v3nNzN+d7Ayww36lpaWWJeAIMX77y7ef34z43cHK0vQDPanbFdXlxw4cEA0TROHwyFNTU2SlpYW67JMw+v1yrBhw6L6vmmaJi0tLZKTkyOJiYb72zFq6N3Q0LtAZBjuyHiJiYkydOhQ31Hd0tLS+M8yCNF+3+x2e9Rey6jo3fCgd4Hw4k9YAAAsjEEPAICFGXbQ22w2WbRokdhstliXYiq8b7HH7yA4vG9AZBjuy3gAACB8DLtGDwAAQsegBwDAwhj0AABYGIMeAAALY9ADAGBhhh30FRUVMmLECElJSZHCwkL54IMPYl2SYZSXl0t+fr6kpqZKZmamTJkyRRobG5VlTpw4IU6nUwYOHCj9+/eXadOmidvtjlHF8YXe7R69C0SfIQf92rVrZf78+bJo0SL5+9//LqNHj5aSkhI5dOhQrEszhOrqanE6nbJz50559913paOjQ6655hppbW31LfPAAw/I22+/LevWrZPq6mo5cOCA3HzzzTGsOj7Qu/7Ru0AMaAZUUFCgOZ1OX+7s7NRycnK08vLyGFZlXIcOHdJERKuurtY0TdOam5u1pKQkbd26db5l9u7dq4mIVlNTE6sy4wK9Gxh6F4g8w63Rt7e3S11dnRQXF/tuS0xMlOLiYqmpqYlhZcbl8XhERCQjI0NEROrq6qSjo0N5D0eOHCkOh4P3MILo3cDRu0DkGW7QHzlyRDo7OyUrK0u5PSsrS1wuV4yqMq6uri6ZN2+ejB8/XkaNGiUiIi6XS5KTkyU9PV1ZlvcwsujdwNC7QHQY7jS1CIzT6ZT6+nrZsWNHrEsBAkLvAtFhuDX6QYMGSZ8+fU75lq3b7Zbs7OwYVWVMpaWlsnHjRtm6dasMHTrUd3t2dra0t7dLc3OzsjzvYWTRu71H7wLRY7hBn5ycLHl5eVJVVeW7raurS6qqqqSoqCiGlRmHpmlSWloq69evly1btkhubq5yf15eniQlJSnvYWNjo+zfv5/3MILo3Z7Ru0AMxPrbgKezZs0azWazaZWVlVpDQ4M2a9YsLT09XXO5XLEuzRDmzJmj2e12bdu2bdrBgwd9l2PHjvmWmT17tuZwOLQtW7ZotbW1WlFRkVZUVBTDquMDvesfvQtEnyEHvaZp2rJlyzSHw6ElJydrBQUF2s6dO2NdkmGIyGkvK1eu9C1z/Phx7b777tMGDBignXnmmdrUqVO1gwcPxq7oOELvdo/eBaKP89EDAGBhhttGDwAAwodBDwCAhTHoAQCwMAY9AAAWxqAHAMDCGPQAAFgYgx4AAAtj0AMAYGEMegAALIxBDwCAhTHoAQCwsP8HfXHsbL5Dh1oAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(mod2, x_test, y_test, 5000)"
      ],
      "metadata": {
        "id": "Bhe7AJ7MHY5E",
        "outputId": "5a18c2ba-6f9e-446b-8710-ffb4ef03dc43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Bhe7AJ7MHY5E",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch: 0, accuracy: 73.72%\n",
            "batch: 1, accuracy: 83.06%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}