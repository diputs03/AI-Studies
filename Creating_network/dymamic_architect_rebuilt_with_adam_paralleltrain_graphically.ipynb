{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diputs03/AI-Studies/blob/main/Creating_network/dymamic_architect_rebuilt_with_adam_paralleltrain_graphically.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Aiming a Dynaimic Graph-structured NeuronNetwork\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import deque\n",
        "import multiprocessing as mp"
      ],
      "metadata": {
        "id": "6mXjwpToZTeV"
      },
      "id": "6mXjwpToZTeV",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Activation function in this case in \\tanh, thus\n",
        "\\dfrac{d\\tanh(x)}{dx}=1-\\tanh^2(x)\n",
        "however, for other activation funtions\n",
        "\\dfrac{d\\sigma(x)}{dx}=\\sigma(x)\\cdot\\left\\big(1-\\sigma(x)\\right\\big)\n",
        "\\dfrac{d\\mathop{\\mathrm{ReLu}}(x)}{dx}=\\begin{cases}1&x\\ge0\\\\0&\\text{else}\\end{cases}\n",
        "Loss is the Euclidean loss\n",
        "\\dfrac{d\\L}\n",
        "\"\"\"\n",
        "class Model:\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.idcnt = 0\n",
        "    self.prev, self.next = {}, {}\n",
        "    self.neurons = set()\n",
        "\n",
        "    self.Input_layer = [self.idcnt+i for i in range(input_size)]\n",
        "    self.neurons.add(self.idcnt+i for i in range(input_size))\n",
        "    self.idcnt+=input_size\n",
        "\n",
        "    self.Output_layer = [self.idcnt+o for o in range(output_size)]\n",
        "    self.neurons.add(self.idcnt+o for o in range(output_size))\n",
        "    self.idcnt+=output_size\n",
        "\n",
        "    for i in self.Input_layer: self.next[i], self.prev[i] = self.Output_layer, []\n",
        "    for o in self.Output_layer: self.prev[o], self.next[o] = self.Input_layer, []\n",
        "\n",
        "    self.weight = {}\n",
        "    self.weight_gsum, self.weight_gsqr = {}, {}\n",
        "    for u in self.Input_layer:\n",
        "      for v in self.Output_layer:\n",
        "        self.weight[(u,v)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_gsum[(u,v)], self.weight_gsqr[(u,v)] = 0, 0\n",
        "\n",
        "    self.bias = {}\n",
        "    self.bias_gsum, self.bias_gsqr = {}, {}\n",
        "    for i in self.neurons:\n",
        "      self.bias[i] = np.random.uniform(-0.1, 0.1)\n",
        "      self.bias_gsum[i], self.bias_gsqr[i] = 0, 0\n",
        "\n",
        "  def forward(self, X, batch_size):\n",
        "    assert X.shape == (batch_size,len(self.Input_layer))\n",
        "    a = {q: np.zeros(batch_size) for q in self.neurons}\n",
        "\n",
        "    for i, n in enumerate(self.Input_layer):\n",
        "      a[n] = X[:, i]\n",
        "\n",
        "    q = deque()\n",
        "    for i in self.Input_layer:\n",
        "      q.append(i)\n",
        "\n",
        "    cnt = {q: 0 for q in self.neurons}\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      a[c] = np.tanh(a[c] + self.bias[c])\n",
        "      for n in next[c]:\n",
        "        a[n] = a[n] + a[c] * self.weight[(c,n)]\n",
        "        cnt[n] += 1\n",
        "        if cnt[n] == len(self.prev[n]):\n",
        "          q.append(n)\n",
        "    return a\n",
        "\n",
        "  def evaluate(self, X):\n",
        "    a = self.forward(X, len(X))\n",
        "    return np.array([a[o] for o in self.Output_layer]).T\n",
        "\n",
        "  def backward(self, X, Y, batch_size, learning_rate, dsum, dsqr):\n",
        "    assert X.shape == (batch_size,len(self.Input_layer))\n",
        "    assert Y.shape == (batch_size,len(self.Output_layer))\n",
        "    a = self.forward(X, batch_size)\n",
        "\n",
        "    delta_b, delta_w = {}, {}\n",
        "\n",
        "    par_a = {q.i: np.zeros(batch_size) for q in self.neurons}\n",
        "    for o, n in enumerate(self.Output_layer):\n",
        "      par_a[n] = 2 * (a[n] - Y[:, o])\n",
        "\n",
        "    q = deque()\n",
        "    for o in self.Output_layer:\n",
        "      q.append(o)\n",
        "\n",
        "    cnt = {q: 0 for q in self.neurons}\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      par_b = par_a[c] * (1-a[c]**2)\n",
        "\n",
        "      gbias = par_b\n",
        "      self.bias_gsum[c] = (1-dsum)*np.sum(gbias)/batch_size + dsum*self.bias_gsum[c]\n",
        "      self.bias_gsqr[c] = (1-dsqr)*np.sum(gbias**2)/batch_size + dsqr*self.bias_gsqr[c]\n",
        "      delta_b[c] = -learning_rate * self.bias_gsum[c] / (self.bias_gsqr[c]**(1/2)+1)\n",
        "\n",
        "      for p in self.prev[c]:\n",
        "        par_a[p] += par_a[c] * (1-a[c]**2) * self.weight[(p,c)]\n",
        "        gweight = par_a[c] * (1-a[c]**2) * a[p]\n",
        "        self.weight_gsum[(p,c)] = \\\n",
        "         (1-dsum)*np.sum(gweight)/batch_size + dsum*self.weight_gsum[(p,c)]\n",
        "        self.weight_gsqr[(p,c)] = \\\n",
        "         (1-dsqr)*np.sum(gweight**2)/batch_size + dsqr*self.weight_gsqr[(p,c)]\n",
        "        delta_w[(p,c)] = \\\n",
        "         -learning_rate * self.weight_gsum[(p,c)] / (self.weight_gsqr[(p,c)]**(1/2)+1)\n",
        "\n",
        "        cnt[p] += 1\n",
        "        if cnt[p] == len(p.next):\n",
        "          q.append(p)\n",
        "\n",
        "    return delta_w, delta_b\n",
        "\n",
        "  def update(self, X, Y, batch_size, learning_rate, dsum=0.9, dsqr=0.9):\n",
        "    delta_w, delta_b = \\\n",
        "     self.backward(X, Y, batch_size, learning_rate, dsum, dsqr)\n",
        "    for key in self.weight:\n",
        "        self.weight[key] += delta_w[key]\n",
        "    for p in self.neurons:\n",
        "        p.bias += delta_b[p]\n",
        "\n",
        "  def addLayer(self, mid_size, UP, DOWN):\n",
        "    Mid_layer = [self.idcnt+m for m in range(mid_size)]\n",
        "    self.neurons.add(self.idcnt+m for m in range(mid_size))\n",
        "    self.idcnt+=mid_size\n",
        "\n",
        "    for m in Mid_layer:\n",
        "      self.bias[m] = np.random.uniform(-0.1, 0.1)\n",
        "      self.bias_gsum[m], self.bias_gsqr[m] = 0, 0\n",
        "\n",
        "      self.prev[m] = UP\n",
        "      for u in UP:\n",
        "        self.weight[(u,m)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_gsum[(u,m)], self.weight_gsqr[(u,m)] = 0, 0\n",
        "\n",
        "      self.next[m] = DOWN\n",
        "      for v in DOWN:\n",
        "        self.weight[(m,v)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_gsum[(m,v)], self.weight_gsqr[(m,v)] = 0, 0\n",
        "\n",
        "    for u in UP:\n",
        "      self.next[u] = Mid_layer\n",
        "    for v in DOWN:\n",
        "      self.prev[v] = Mid_layer\n",
        "\n",
        "    for u in UP:\n",
        "      for v in DOWN:\n",
        "        self.weight.pop((u,v))\n",
        "        self.weight_gsum.pop((u,v))\n",
        "        self.weight_gsqr.pop((u,v))\n",
        "    return Mid_layer\n",
        "\n",
        "  def train(self, X, Y, batch_size, epochs, learning_rate):\n",
        "    assert len(X) == len(Y)\n",
        "    l = len(X)\n",
        "    for epoch in range(epochs):\n",
        "      data=[(X[_], Y[_]) for _ in range(len(X))]\n",
        "      random.shuffle(data)\n",
        "      for _ in range(len(X)):\n",
        "        X[_],Y[_]=data[_]\n",
        "      loss = 0\n",
        "      for batch in range(int(l / batch_size)):\n",
        "        L, R = batch * batch_size, (batch + 1) * batch_size\n",
        "        x_train, y_train = X[L:R], Y[L:R]\n",
        "        self.update(x_train, y_train, batch_size, learning_rate)\n",
        "        output = self.evaluate(x_train)\n",
        "        loss += np.sum(((y_train-output) ** 2), axis=(0,1))\n",
        "      loss = ((loss) ** 0.5) / (int(l / batch_size) * batch_size)\n",
        "      print(f\"Epoch {epoch}/{epochs}, Loss:{loss}\")\n",
        "\n",
        "  def parallel_train(self, X, Y, batch_size, epochs=10, learning_rate=0.1):\n",
        "    assert len(X) == len(Y)\n",
        "    l = len(X)\n",
        "    for epoch in range(epochs):\n",
        "      data=[(X[_], Y[_]) for _ in range(len(X))]\n",
        "      random.shuffle(data)\n",
        "      for _ in range(len(X)):\n",
        "        X[_],Y[_]=data[_]\n",
        "\n",
        "      k = int(l / batch_size)\n",
        "      with mp.Pool(processes=k) as pool: results = pool.starmap(Model.backward, \\\n",
        "       [(self, X[b*batch_size:(b+1)*batch_size], Y[b*batch_size:(b+1)*batch_size],\\\n",
        "         batch_size, learning_rate, 0.9, 0.9) for b in range(k)])\n",
        "      delta_w = {key: np.sum([res[0][key] for res in results], axis=0) / k for key in self.weight}\n",
        "      delta_b = {n: np.sum([res[1][n] for res in results], axis=0) / k for n in self.neurons}\n",
        "      for key in self.weight:\n",
        "        self.weight[key] += delta_w[key]\n",
        "      for p in self.neurons:\n",
        "        self.bias[p] += delta_b[p]\n",
        "      output = self.evaluate(X)\n",
        "      loss = np.sum(((Y-output) ** 2), axis=(0,1))\n",
        "      print(f\"Epoch {epoch}/{epochs}, Loss:{loss}\")"
      ],
      "metadata": {
        "id": "YUliU5Cx-oyU"
      },
      "id": "YUliU5Cx-oyU",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "Y=np.array([[0],[1],[1],[0]])\n",
        "mod=Model(2, 1)\n",
        "mid1=mod.addLayer(4, mod.Input_layer, mod.Output_layer)\n",
        "mod.addLayer(4, mid1, mod.Output_layer)\n",
        "mod.evaluate(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "KfiQiE5l8XOr",
        "outputId": "ae3d1022-a74d-418e-f750-ade9df9a11ec"
      },
      "id": "KfiQiE5l8XOr",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4bfdc47e117c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmid1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmid1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-10dd204ff2fc>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutput_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-10dd204ff2fc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, batch_size)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m       \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod.parallel_train(X, Y, 4, 500, 0.1)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC2n_sC_8xnP",
        "outputId": "6a6f1a78-20cf-42bb-ec29-964bdb6194ee"
      },
      "id": "aC2n_sC_8xnP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/500, Loss:1.214644913626065\n",
            "Epoch 1/500, Loss:2.358760289369366\n",
            "Epoch 2/500, Loss:3.453697005546797\n",
            "Epoch 3/500, Loss:3.384178873381895\n",
            "Epoch 4/500, Loss:2.226237847718614\n",
            "Epoch 5/500, Loss:3.266775820395009\n",
            "Epoch 6/500, Loss:2.1474517377584372\n",
            "Epoch 7/500, Loss:0.004669510085433876\n",
            "Epoch 8/500, Loss:0.004485331557508367\n",
            "Epoch 9/500, Loss:0.004308333954674422\n",
            "Epoch 10/500, Loss:0.004138242415824734\n",
            "Epoch 11/500, Loss:0.003974792285257439\n",
            "Epoch 12/500, Loss:0.00381772875340271\n",
            "Epoch 13/500, Loss:0.0036668065089516563\n",
            "Epoch 14/500, Loss:0.003521789402107219\n",
            "Epoch 15/500, Loss:0.0033824501186780157\n",
            "Epoch 16/500, Loss:0.003248569864737628\n",
            "Epoch 17/500, Loss:0.0031199380615739387\n",
            "Epoch 18/500, Loss:0.002996352050655576\n",
            "Epoch 19/500, Loss:0.002877616808345455\n",
            "Epoch 20/500, Loss:0.002763544670094539\n",
            "Epoch 21/500, Loss:0.0026539550638524784\n",
            "Epoch 22/500, Loss:0.002548674252435519\n",
            "Epoch 23/500, Loss:0.002447535084596108\n",
            "Epoch 24/500, Loss:0.0023503767545427334\n",
            "Epoch 25/500, Loss:0.0022570445696629966\n",
            "Epoch 26/500, Loss:0.002167389726207353\n",
            "Epoch 27/500, Loss:0.002081269092695655\n",
            "Epoch 28/500, Loss:0.001998545000813334\n",
            "Epoch 29/500, Loss:0.0019190850435689152\n",
            "Epoch 30/500, Loss:0.0018427618804894507\n",
            "Epoch 31/500, Loss:0.0017694530496353844\n",
            "Epoch 32/500, Loss:0.0016990407862213699\n",
            "Epoch 33/500, Loss:0.0016314118476345246\n",
            "Epoch 34/500, Loss:0.0015664573446466485\n",
            "Epoch 35/500, Loss:0.001504072578621894\n",
            "Epoch 36/500, Loss:0.0014441568845263856\n",
            "Epoch 37/500, Loss:0.0013866134795512321\n",
            "Epoch 38/500, Loss:0.0013313493171652987\n",
            "Epoch 39/500, Loss:0.0012782749464189823\n",
            "Epoch 40/500, Loss:0.0012273043763250692\n",
            "Epoch 41/500, Loss:0.0011783549451475314\n",
            "Epoch 42/500, Loss:0.0011313471944338074\n",
            "Epoch 43/500, Loss:0.0010862047476307953\n",
            "Epoch 44/500, Loss:0.0010428541931293282\n",
            "Epoch 45/500, Loss:0.0010012249715864144\n",
            "Epoch 46/500, Loss:0.0009612492673789563\n",
            "Epoch 47/500, Loss:0.0009228619040469837\n",
            "Epoch 48/500, Loss:0.0008860002435887045\n",
            "Epoch 49/500, Loss:0.0008506040894738549\n",
            "Epoch 50/500, Loss:0.0008166155932459013\n",
            "Epoch 51/500, Loss:0.0007839791645876586\n",
            "Epoch 52/500, Loss:0.0007526413847287863\n",
            "Epoch 53/500, Loss:0.0007225509230774527\n",
            "Epoch 54/500, Loss:0.000693658456962181\n",
            "Epoch 55/500, Loss:0.0006659165943735292\n",
            "Epoch 56/500, Loss:0.000639279799598811\n",
            "Epoch 57/500, Loss:0.0006137043216465174\n",
            "Epoch 58/500, Loss:0.0005891481253604776\n",
            "Epoch 59/500, Loss:0.0005655708251270669\n",
            "Epoch 60/500, Loss:0.0005429336210819794\n",
            "Epoch 61/500, Loss:0.000521199237726175\n",
            "Epoch 62/500, Loss:0.0005003318648636468\n",
            "Epoch 63/500, Loss:0.000480297100776577\n",
            "Epoch 64/500, Loss:0.0004610618975563129\n",
            "Epoch 65/500, Loss:0.00044259450851136283\n",
            "Epoch 66/500, Loss:0.00042486443757630316\n",
            "Epoch 67/500, Loss:0.00040784239064809287\n",
            "Epoch 68/500, Loss:0.0003915002287788366\n",
            "Epoch 69/500, Loss:0.00037581092315647885\n",
            "Epoch 70/500, Loss:0.00036074851180730957\n",
            "Epoch 71/500, Loss:0.00034628805795645603\n",
            "Epoch 72/500, Loss:0.0003324056099847857\n",
            "Epoch 73/500, Loss:0.0003190781629227956\n",
            "Epoch 74/500, Loss:0.00030628362142418215\n",
            "Epoch 75/500, Loss:0.00029400076416379717\n",
            "Epoch 76/500, Loss:0.0002822092096066785\n",
            "Epoch 77/500, Loss:0.0002708893830967327\n",
            "Epoch 78/500, Loss:0.00026002248521550357\n",
            "Epoch 79/500, Loss:0.0002495904613632218\n",
            "Epoch 80/500, Loss:0.0002395759725160741\n",
            "Epoch 81/500, Loss:0.00022996236711527747\n",
            "Epoch 82/500, Loss:0.00022073365404516287\n",
            "Epoch 83/500, Loss:0.00021187447665902667\n",
            "Epoch 84/500, Loss:0.00020337008781300795\n",
            "Epoch 85/500, Loss:0.00019520632586970523\n",
            "Epoch 86/500, Loss:0.00018736959163464503\n",
            "Epoch 87/500, Loss:0.00017984682619007347\n",
            "Epoch 88/500, Loss:0.0001726254895918435\n",
            "Epoch 89/500, Loss:0.0001656935403964395\n",
            "Epoch 90/500, Loss:0.0001590394159863943\n",
            "Epoch 91/500, Loss:0.0001526520136635329\n",
            "Epoch 92/500, Loss:0.00014652067248061083\n",
            "Epoch 93/500, Loss:0.000140635155783009\n",
            "Epoch 94/500, Loss:0.0001349856344332075\n",
            "Epoch 95/500, Loss:0.00012956267069177307\n",
            "Epoch 96/500, Loss:0.000124357202729584\n",
            "Epoch 97/500, Loss:0.0001193605297469607\n",
            "Epoch 98/500, Loss:0.00011456429767628335\n",
            "Epoch 99/500, Loss:0.00010996048544556242\n",
            "Epoch 100/500, Loss:0.00010554139178127261\n",
            "Epoch 101/500, Loss:0.00010129962252958269\n",
            "Epoch 102/500, Loss:9.722807847590176e-05\n",
            "Epoch 103/500, Loss:9.331994364342224e-05\n",
            "Epoch 104/500, Loss:8.956867405207177e-05\n",
            "Epoch 105/500, Loss:8.596798691999513e-05\n",
            "Epoch 106/500, Loss:8.251185029036305e-05\n",
            "Epoch 107/500, Loss:7.919447306696303e-05\n",
            "Epoch 108/500, Loss:7.601029544265703e-05\n",
            "Epoch 109/500, Loss:7.295397970539871e-05\n",
            "Epoch 110/500, Loss:7.002040140708693e-05\n",
            "Epoch 111/500, Loss:6.720464088109868e-05\n",
            "Epoch 112/500, Loss:6.450197509488464e-05\n",
            "Epoch 113/500, Loss:6.19078698245331e-05\n",
            "Epoch 114/500, Loss:5.941797213871223e-05\n",
            "Epoch 115/500, Loss:5.7028103179885524e-05\n",
            "Epoch 116/500, Loss:5.4734251231157305e-05\n",
            "Epoch 117/500, Loss:5.253256505755951e-05\n",
            "Epoch 118/500, Loss:5.04193475110179e-05\n",
            "Epoch 119/500, Loss:4.8391049388653884e-05\n",
            "Epoch 120/500, Loss:4.6444263534477475e-05\n",
            "Epoch 121/500, Loss:4.457571917491218e-05\n",
            "Epoch 122/500, Loss:4.278227647896022e-05\n",
            "Epoch 123/500, Loss:4.1060921334177926e-05\n",
            "Epoch 124/500, Loss:3.9408760329967816e-05\n",
            "Epoch 125/500, Loss:3.782301594002859e-05\n",
            "Epoch 126/500, Loss:3.63010218961177e-05\n",
            "Epoch 127/500, Loss:3.484021874558935e-05\n",
            "Epoch 128/500, Loss:3.343814958546202e-05\n",
            "Epoch 129/500, Loss:3.209245596605233e-05\n",
            "Epoch 130/500, Loss:3.080087395748459e-05\n",
            "Epoch 131/500, Loss:2.9561230372645626e-05\n",
            "Epoch 132/500, Loss:2.8371439140404948e-05\n",
            "Epoch 133/500, Loss:2.7229497823163287e-05\n",
            "Epoch 134/500, Loss:2.6133484273023397e-05\n",
            "Epoch 135/500, Loss:2.508155342110099e-05\n",
            "Epoch 136/500, Loss:2.4071934194707893e-05\n",
            "Epoch 137/500, Loss:2.3102926557346148e-05\n",
            "Epoch 138/500, Loss:2.2172898666650553e-05\n",
            "Epoch 139/500, Loss:2.128028414560651e-05\n",
            "Epoch 140/500, Loss:2.042357946255561e-05\n",
            "Epoch 141/500, Loss:1.960134141567537e-05\n",
            "Epoch 142/500, Loss:1.8812184717791205e-05\n",
            "Epoch 143/500, Loss:1.8054779677539172e-05\n",
            "Epoch 144/500, Loss:1.7327849973057475e-05\n",
            "Epoch 145/500, Loss:1.6630170514532353e-05\n",
            "Epoch 146/500, Loss:1.5960565392070882e-05\n",
            "Epoch 147/500, Loss:1.531790590551062e-05\n",
            "Epoch 148/500, Loss:1.4701108672911057e-05\n",
            "Epoch 149/500, Loss:1.4109133814598706e-05\n",
            "Epoch 150/500, Loss:1.3540983209762715e-05\n",
            "Epoch 151/500, Loss:1.2995698822715095e-05\n",
            "Epoch 152/500, Loss:1.2472361096044025e-05\n",
            "Epoch 153/500, Loss:1.1970087407999035e-05\n",
            "Epoch 154/500, Loss:1.1488030591550749e-05\n",
            "Epoch 155/500, Loss:1.1025377512669763e-05\n",
            "Epoch 156/500, Loss:1.0581347705466413e-05\n",
            "Epoch 157/500, Loss:1.0155192061926295e-05\n",
            "Epoch 158/500, Loss:9.746191574065916e-06\n",
            "Epoch 159/500, Loss:9.353656126419631e-06\n",
            "Epoch 160/500, Loss:8.976923336850939e-06\n",
            "Epoch 161/500, Loss:8.615357443761875e-06\n",
            "Epoch 162/500, Loss:8.268348237849342e-06\n",
            "Epoch 163/500, Loss:7.935310036631662e-06\n",
            "Epoch 164/500, Loss:7.615680700038638e-06\n",
            "Epoch 165/500, Loss:7.3089206854264514e-06\n",
            "Epoch 166/500, Loss:7.014512140443299e-06\n",
            "Epoch 167/500, Loss:6.731958032234928e-06\n",
            "Epoch 168/500, Loss:6.460781311538497e-06\n",
            "Epoch 169/500, Loss:6.200524110271466e-06\n",
            "Epoch 170/500, Loss:5.950746971277324e-06\n",
            "Epoch 171/500, Loss:5.7110281089431195e-06\n",
            "Epoch 172/500, Loss:5.480962699455295e-06\n",
            "Epoch 173/500, Loss:5.260162199508672e-06\n",
            "Epoch 174/500, Loss:5.048253692331623e-06\n",
            "Epoch 175/500, Loss:4.844879259934517e-06\n",
            "Epoch 176/500, Loss:4.649695380533213e-06\n",
            "Epoch 177/500, Loss:4.462372350140359e-06\n",
            "Epoch 178/500, Loss:4.282593727357573e-06\n",
            "Epoch 179/500, Loss:4.110055800440496e-06\n",
            "Epoch 180/500, Loss:3.944467075744914e-06\n",
            "Epoch 181/500, Loss:3.7855477866986175e-06\n",
            "Epoch 182/500, Loss:3.633029422477017e-06\n",
            "Epoch 183/500, Loss:3.486654275593951e-06\n",
            "Epoch 184/500, Loss:3.3461750076498004e-06\n",
            "Epoch 185/500, Loss:3.2113542325103342e-06\n",
            "Epoch 186/500, Loss:3.081964116217631e-06\n",
            "Epoch 187/500, Loss:2.9577859929631644e-06\n",
            "Epoch 188/500, Loss:2.8386099964791463e-06\n",
            "Epoch 189/500, Loss:2.7242347062308233e-06\n",
            "Epoch 190/500, Loss:2.6144668078161667e-06\n",
            "Epoch 191/500, Loss:2.5091207670036913e-06\n",
            "Epoch 192/500, Loss:2.408018516861783e-06\n",
            "Epoch 193/500, Loss:2.310989157454713e-06\n",
            "Epoch 194/500, Loss:2.2178686676012773e-06\n",
            "Epoch 195/500, Loss:2.1284996282127035e-06\n",
            "Epoch 196/500, Loss:2.0427309567452725e-06\n",
            "Epoch 197/500, Loss:1.960417652321888e-06\n",
            "Epoch 198/500, Loss:1.8814205510946987e-06\n",
            "Epoch 199/500, Loss:1.8056060914378727e-06\n",
            "Epoch 200/500, Loss:1.7328460885763933e-06\n",
            "Epoch 201/500, Loss:1.6630175182719141e-06\n",
            "Epoch 202/500, Loss:1.596002309202405e-06\n",
            "Epoch 203/500, Loss:1.5316871436868956e-06\n",
            "Epoch 204/500, Loss:1.4699632664200176e-06\n",
            "Epoch 205/500, Loss:1.4107263008950974e-06\n",
            "Epoch 206/500, Loss:1.353876073207178e-06\n",
            "Epoch 207/500, Loss:1.2993164429394573e-06\n",
            "Epoch 208/500, Loss:1.246955140849068e-06\n",
            "Epoch 209/500, Loss:1.1967036130790072e-06\n",
            "Epoch 210/500, Loss:1.148476871634122e-06\n",
            "Epoch 211/500, Loss:1.102193350869734e-06\n",
            "Epoch 212/500, Loss:1.057774769751296e-06\n",
            "Epoch 213/500, Loss:1.015145999653442e-06\n",
            "Epoch 214/500, Loss:9.74234937475673e-07\n",
            "Epoch 215/500, Loss:9.349723838613875e-07\n",
            "Epoch 216/500, Loss:8.972919263150212e-07\n",
            "Epoch 217/500, Loss:8.611298270206745e-07\n",
            "Epoch 218/500, Loss:8.264249151730923e-07\n",
            "Epoch 219/500, Loss:7.931184836400201e-07\n",
            "Epoch 220/500, Loss:7.611541897815006e-07\n",
            "Epoch 221/500, Loss:7.304779602593705e-07\n",
            "Epoch 222/500, Loss:7.010378996763598e-07\n",
            "Epoch 223/500, Loss:6.727842028910694e-07\n",
            "Epoch 224/500, Loss:6.456690708608933e-07\n",
            "Epoch 225/500, Loss:6.196466298711761e-07\n",
            "Epoch 226/500, Loss:5.946728540145018e-07\n",
            "Epoch 227/500, Loss:5.707054907893372e-07\n",
            "Epoch 228/500, Loss:5.477039896927252e-07\n",
            "Epoch 229/500, Loss:5.256294336866263e-07\n",
            "Epoch 230/500, Loss:5.044444734222959e-07\n",
            "Epoch 231/500, Loss:4.841132641118748e-07\n",
            "Epoch 232/500, Loss:4.6460140494073165e-07\n",
            "Epoch 233/500, Loss:4.4587588091830416e-07\n",
            "Epoch 234/500, Loss:4.279050070694662e-07\n",
            "Epoch 235/500, Loss:4.106583748722398e-07\n",
            "Epoch 236/500, Loss:3.9410680085145654e-07\n",
            "Epoch 237/500, Loss:3.7822227724180203e-07\n",
            "Epoch 238/500, Loss:3.62977924636823e-07\n",
            "Epoch 239/500, Loss:3.4834794654406925e-07\n",
            "Epoch 240/500, Loss:3.3430758576980224e-07\n",
            "Epoch 241/500, Loss:3.208330825594841e-07\n",
            "Epoch 242/500, Loss:3.0790163442354416e-07\n",
            "Epoch 243/500, Loss:2.95491357580561e-07\n",
            "Epoch 244/500, Loss:2.835812499527812e-07\n",
            "Epoch 245/500, Loss:2.72151155651565e-07\n",
            "Epoch 246/500, Loss:2.611817308927376e-07\n",
            "Epoch 247/500, Loss:2.506544112843379e-07\n",
            "Epoch 248/500, Loss:2.405513804315741e-07\n",
            "Epoch 249/500, Loss:2.3085553980590167e-07\n",
            "Epoch 250/500, Loss:2.2155047982743698e-07\n",
            "Epoch 251/500, Loss:2.1262045211178617e-07\n",
            "Epoch 252/500, Loss:2.0405034283451253e-07\n",
            "Epoch 253/500, Loss:1.9582564716819778e-07\n",
            "Epoch 254/500, Loss:1.8793244474893553e-07\n",
            "Epoch 255/500, Loss:1.80357376130846e-07\n",
            "Epoch 256/500, Loss:1.7308762018885697e-07\n",
            "Epoch 257/500, Loss:1.6611087243147478e-07\n",
            "Epoch 258/500, Loss:1.5941532418707693e-07\n",
            "Epoch 259/500, Loss:1.5298964262842245e-07\n",
            "Epoch 260/500, Loss:1.4682295160170494e-07\n",
            "Epoch 261/500, Loss:1.4090481322775285e-07\n",
            "Epoch 262/500, Loss:1.3522521024426298e-07\n",
            "Epoch 263/500, Loss:1.2977452905927623e-07\n",
            "Epoch 264/500, Loss:1.2454354348722652e-07\n",
            "Epoch 265/500, Loss:1.1952339914007391e-07\n",
            "Epoch 266/500, Loss:1.1470559844719808e-07\n",
            "Epoch 267/500, Loss:1.1008198627866633e-07\n",
            "Epoch 268/500, Loss:1.0564473614764279e-07\n",
            "Epoch 269/500, Loss:1.0138633696857616e-07\n",
            "Epoch 270/500, Loss:9.729958034879905e-08\n",
            "Epoch 271/500, Loss:9.3377548392082e-08\n",
            "Epoch 272/500, Loss:8.961360199348754e-08\n",
            "Epoch 273/500, Loss:8.600136960576753e-08\n",
            "Epoch 274/500, Loss:8.253473645829434e-08\n",
            "Epoch 275/500, Loss:7.920783421033703e-08\n",
            "Epoch 276/500, Loss:7.601503102114705e-08\n",
            "Epoch 277/500, Loss:7.295092202008615e-08\n",
            "Epoch 278/500, Loss:7.001032016069777e-08\n",
            "Epoch 279/500, Loss:6.718824744323127e-08\n",
            "Epoch 280/500, Loss:6.447992649081078e-08\n",
            "Epoch 281/500, Loss:6.188077246497862e-08\n",
            "Epoch 282/500, Loss:5.938638530695703e-08\n",
            "Epoch 283/500, Loss:5.69925422914957e-08\n",
            "Epoch 284/500, Loss:5.469519088075773e-08\n",
            "Epoch 285/500, Loss:5.249044186609162e-08\n",
            "Epoch 286/500, Loss:5.0374562786133276e-08\n",
            "Epoch 287/500, Loss:4.83439716101315e-08\n",
            "Epoch 288/500, Loss:4.639523067573635e-08\n",
            "Epoch 289/500, Loss:4.4525040871101464e-08\n",
            "Epoch 290/500, Loss:4.273023605135759e-08\n",
            "Epoch 291/500, Loss:4.1007777680088296e-08\n",
            "Epoch 292/500, Loss:3.935474968672239e-08\n",
            "Epoch 293/500, Loss:3.7768353531126154e-08\n",
            "Epoch 294/500, Loss:3.624590346707799e-08\n",
            "Epoch 295/500, Loss:3.478482199658557e-08\n",
            "Epoch 296/500, Loss:3.338263550739098e-08\n",
            "Epoch 297/500, Loss:3.20369700862459e-08\n",
            "Epoch 298/500, Loss:3.074554750090062e-08\n",
            "Epoch 299/500, Loss:2.9506181344008298e-08\n",
            "Epoch 300/500, Loss:2.8316773332402104e-08\n",
            "Epoch 301/500, Loss:2.7175309755516394e-08\n",
            "Epoch 302/500, Loss:2.6079858066914197e-08\n",
            "Epoch 303/500, Loss:2.5028563613167357e-08\n",
            "Epoch 304/500, Loss:2.4019646494562235e-08\n",
            "Epoch 305/500, Loss:2.3051398552308456e-08\n",
            "Epoch 306/500, Loss:2.2122180477153514e-08\n",
            "Epoch 307/500, Loss:2.1230419034524302e-08\n",
            "Epoch 308/500, Loss:2.037460440148627e-08\n",
            "Epoch 309/500, Loss:1.9553287611025456e-08\n",
            "Epoch 310/500, Loss:1.87650780993159e-08\n",
            "Epoch 311/500, Loss:1.800864135184122e-08\n",
            "Epoch 312/500, Loss:1.728269664437506e-08\n",
            "Epoch 313/500, Loss:1.6586014874989734e-08\n",
            "Epoch 314/500, Loss:1.591741648345461e-08\n",
            "Epoch 315/500, Loss:1.527576945446456e-08\n",
            "Epoch 316/500, Loss:1.465998740135728e-08\n",
            "Epoch 317/500, Loss:1.4069027727045472e-08\n",
            "Epoch 318/500, Loss:1.3501889859075647e-08\n",
            "Epoch 319/500, Loss:1.295761355580719e-08\n",
            "Epoch 320/500, Loss:1.2435277280865816e-08\n",
            "Epoch 321/500, Loss:1.1933996643108261e-08\n",
            "Epoch 322/500, Loss:1.14529228994447e-08\n",
            "Epoch 323/500, Loss:1.0991241518026294e-08\n",
            "Epoch 324/500, Loss:1.0548170799318212e-08\n",
            "Epoch 325/500, Loss:1.0122960552762865e-08\n",
            "Epoch 326/500, Loss:9.714890826763164e-09\n",
            "Epoch 327/500, Loss:9.32327068986301e-09\n",
            "Epoch 328/500, Loss:8.947437061051206e-09\n",
            "Epoch 329/500, Loss:8.586753587195314e-09\n",
            "Epoch 330/500, Loss:8.240609565732462e-09\n",
            "Epoch 331/500, Loss:7.908418910775209e-09\n",
            "Epoch 332/500, Loss:7.589619160887517e-09\n",
            "Epoch 333/500, Loss:7.283670526850379e-09\n",
            "Epoch 334/500, Loss:6.990054977807578e-09\n",
            "Epoch 335/500, Loss:6.708275364246687e-09\n",
            "Epoch 336/500, Loss:6.437854576325925e-09\n",
            "Epoch 337/500, Loss:6.178334736122521e-09\n",
            "Epoch 338/500, Loss:5.929276422440601e-09\n",
            "Epoch 339/500, Loss:5.690257926865221e-09\n",
            "Epoch 340/500, Loss:5.460874539795707e-09\n",
            "Epoch 341/500, Loss:5.240737865259152e-09\n",
            "Epoch 342/500, Loss:5.029475163342425e-09\n",
            "Epoch 343/500, Loss:4.826728719123421e-09\n",
            "Epoch 344/500, Loss:4.632155237044777e-09\n",
            "Epoch 345/500, Loss:4.445425259692049e-09\n",
            "Epoch 346/500, Loss:4.266222609998105e-09\n",
            "Epoch 347/500, Loss:4.094243855937564e-09\n",
            "Epoch 348/500, Loss:3.929197796790771e-09\n",
            "Epoch 349/500, Loss:3.77080497011826e-09\n",
            "Epoch 350/500, Loss:3.6187971786026953e-09\n",
            "Epoch 351/500, Loss:3.4729170359713286e-09\n",
            "Epoch 352/500, Loss:3.3329175312089237e-09\n",
            "Epoch 353/500, Loss:3.1985616103472563e-09\n",
            "Epoch 354/500, Loss:3.0696217751053958e-09\n",
            "Epoch 355/500, Loss:2.945879697706802e-09\n",
            "Epoch 356/500, Loss:2.827125851227459e-09\n",
            "Epoch 357/500, Loss:2.713159154840278e-09\n",
            "Epoch 358/500, Loss:2.6037866333552177e-09\n",
            "Epoch 359/500, Loss:2.4988230904921768e-09\n",
            "Epoch 360/500, Loss:2.398090795313029e-09\n",
            "Epoch 361/500, Loss:2.3014191813004964e-09\n",
            "Epoch 362/500, Loss:2.2086445575634655e-09\n",
            "Epoch 363/500, Loss:2.119609831682916e-09\n",
            "Epoch 364/500, Loss:2.034164243730159e-09\n",
            "Epoch 365/500, Loss:1.9521631110058554e-09\n",
            "Epoch 366/500, Loss:1.8734675830680304e-09\n",
            "Epoch 367/500, Loss:1.797944406636579e-09\n",
            "Epoch 368/500, Loss:1.7254656999704962e-09\n",
            "Epoch 369/500, Loss:1.6559087363422186e-09\n",
            "Epoch 370/500, Loss:1.5891557362411051e-09\n",
            "Epoch 371/500, Loss:1.5250936679467146e-09\n",
            "Epoch 372/500, Loss:1.4636140561498793e-09\n",
            "Epoch 373/500, Loss:1.4046127982799104e-09\n",
            "Epoch 374/500, Loss:1.3479899882402917e-09\n",
            "Epoch 375/500, Loss:1.2936497472476176e-09\n",
            "Epoch 376/500, Loss:1.2415000614893535e-09\n",
            "Epoch 377/500, Loss:1.1914526263257197e-09\n",
            "Epoch 378/500, Loss:1.1434226967719688e-09\n",
            "Epoch 379/500, Loss:1.0973289440071927e-09\n",
            "Epoch 380/500, Loss:1.053093317666932e-09\n",
            "Epoch 381/500, Loss:1.0106409136874362e-09\n",
            "Epoch 382/500, Loss:9.698998474764429e-10\n",
            "Epoch 383/500, Loss:9.308011321964038e-10\n",
            "Epoch 384/500, Loss:8.932785619552138e-10\n",
            "Epoch 385/500, Loss:8.57268599702888e-10\n",
            "Epoch 386/500, Loss:8.227102696504917e-10\n",
            "Epoch 387/500, Loss:7.895450540231631e-10\n",
            "Epoch 388/500, Loss:7.577167939747351e-10\n",
            "Epoch 389/500, Loss:7.271715944994241e-10\n",
            "Epoch 390/500, Loss:6.978577331730773e-10\n",
            "Epoch 391/500, Loss:6.697255725752594e-10\n",
            "Epoch 392/500, Loss:6.427274762408109e-10\n",
            "Epoch 393/500, Loss:6.168177279988642e-10\n",
            "Epoch 394/500, Loss:5.919524545649788e-10\n",
            "Epoch 395/500, Loss:5.680895512520399e-10\n",
            "Epoch 396/500, Loss:5.451886106768373e-10\n",
            "Epoch 397/500, Loss:5.232108543387724e-10\n",
            "Epoch 398/500, Loss:5.021190669592646e-10\n",
            "Epoch 399/500, Loss:4.818775334655336e-10\n",
            "Epoch 400/500, Loss:4.6245197851618995e-10\n",
            "Epoch 401/500, Loss:4.4380950846414437e-10\n",
            "Epoch 402/500, Loss:4.259185556577663e-10\n",
            "Epoch 403/500, Loss:4.087488249900595e-10\n",
            "Epoch 404/500, Loss:3.922712425995409e-10\n",
            "Epoch 405/500, Loss:3.7645790664073915e-10\n",
            "Epoch 406/500, Loss:3.6128204003889435e-10\n",
            "Epoch 407/500, Loss:3.4671794514890023e-10\n",
            "Epoch 408/500, Loss:3.327409602413517e-10\n",
            "Epoch 409/500, Loss:3.1932741774466736e-10\n",
            "Epoch 410/500, Loss:3.0645460416822797e-10\n",
            "Epoch 411/500, Loss:2.9410072164247707e-10\n",
            "Epoch 412/500, Loss:2.8224485100892935e-10\n",
            "Epoch 413/500, Loss:2.708669163976469e-10\n",
            "Epoch 414/500, Loss:2.5994765123358354e-10\n",
            "Epoch 415/500, Loss:2.4946856561209063e-10\n",
            "Epoch 416/500, Loss:2.3941191499124357e-10\n",
            "Epoch 417/500, Loss:2.2976067014366998e-10\n",
            "Epoch 418/500, Loss:2.2049848832253108e-10\n",
            "Epoch 419/500, Loss:2.1160968558708203e-10\n",
            "Epoch 420/500, Loss:2.0307921024656913e-10\n",
            "Epoch 421/500, Loss:1.9489261737175703e-10\n",
            "Epoch 422/500, Loss:1.8703604433661427e-10\n",
            "Epoch 423/500, Loss:1.7949618734431525e-10\n",
            "Epoch 424/500, Loss:1.7226027889934724e-10\n",
            "Epoch 425/500, Loss:1.6531606618905e-10\n",
            "Epoch 426/500, Loss:1.5865179033579286e-10\n",
            "Epoch 427/500, Loss:1.522561664851156e-10\n",
            "Epoch 428/500, Loss:1.4611836469750405e-10\n",
            "Epoch 429/500, Loss:1.402279916098087e-10\n",
            "Epoch 430/500, Loss:1.3457507283611617e-10\n",
            "Epoch 431/500, Loss:1.291500360781275e-10\n",
            "Epoch 432/500, Loss:1.239436949160793e-10\n",
            "Epoch 433/500, Loss:1.1894723325335393e-10\n",
            "Epoch 434/500, Loss:1.1415219038841412e-10\n",
            "Epoch 435/500, Loss:1.0955044668765967e-10\n",
            "Epoch 436/500, Loss:1.0513420983656006e-10\n",
            "Epoch 437/500, Loss:1.0089600164511163e-10\n",
            "Epoch 438/500, Loss:9.682864538442797e-11\n",
            "Epoch 439/500, Loss:9.292525363457828e-11\n",
            "Epoch 440/500, Loss:8.917921662177994e-11\n",
            "Epoch 441/500, Loss:8.558419102639218e-11\n",
            "Epoch 442/500, Loss:8.213408924099229e-11\n",
            "Epoch 443/500, Loss:7.882306906255076e-11\n",
            "Epoch 444/500, Loss:7.564552379984419e-11\n",
            "Epoch 445/500, Loss:7.25960727789286e-11\n",
            "Epoch 446/500, Loss:6.966955223267186e-11\n",
            "Epoch 447/500, Loss:6.686100655617469e-11\n",
            "Epoch 448/500, Loss:6.416567991573966e-11\n",
            "Epoch 449/500, Loss:6.157900819555097e-11\n",
            "Epoch 450/500, Loss:5.909661126913063e-11\n",
            "Epoch 451/500, Loss:5.6714285582394775e-11\n",
            "Epoch 452/500, Loss:5.442799703577266e-11\n",
            "Epoch 453/500, Loss:5.223387415294925e-11\n",
            "Epoch 454/500, Loss:5.0128201525269015e-11\n",
            "Epoch 455/500, Loss:4.810741352029303e-11\n",
            "Epoch 456/500, Loss:4.616808824421565e-11\n",
            "Epoch 457/500, Loss:4.430694174720019e-11\n",
            "Epoch 458/500, Loss:4.2520822462754234e-11\n",
            "Epoch 459/500, Loss:4.0806705870960785e-11\n",
            "Epoch 460/500, Loss:3.916168937696297e-11\n",
            "Epoch 461/500, Loss:3.758298739609701e-11\n",
            "Epoch 462/500, Loss:3.6067926636736464e-11\n",
            "Epoch 463/500, Loss:3.461394157360668e-11\n",
            "Epoch 464/500, Loss:3.321857010369207e-11\n",
            "Epoch 465/500, Loss:3.1879449376530065e-11\n",
            "Epoch 466/500, Loss:3.0594311793842405e-11\n",
            "Epoch 467/500, Loss:2.936098116905644e-11\n",
            "Epoch 468/500, Loss:2.817736904274938e-11\n",
            "Epoch 469/500, Loss:2.704147114608692e-11\n",
            "Epoch 470/500, Loss:2.595136400674833e-11\n",
            "Epoch 471/500, Loss:2.4905201691928575e-11\n",
            "Epoch 472/500, Loss:2.390121268267519e-11\n",
            "Epoch 473/500, Loss:2.293769687393921e-11\n",
            "Epoch 474/500, Loss:2.2013022695798482e-11\n",
            "Epoch 475/500, Loss:2.112562435079249e-11\n",
            "Epoch 476/500, Loss:2.027399916203083e-11\n",
            "Epoch 477/500, Loss:1.9456705029357403e-11\n",
            "Epoch 478/500, Loss:1.8672357986672692e-11\n",
            "Epoch 479/500, Loss:1.7919629858973158e-11\n",
            "Epoch 480/500, Loss:1.7197246012863078e-11\n",
            "Epoch 481/500, Loss:1.6503983198502154e-11\n",
            "Epoch 482/500, Loss:1.5838667478065807e-11\n",
            "Epoch 483/500, Loss:1.5200172237816477e-11\n",
            "Epoch 484/500, Loss:1.4587416280507404e-11\n",
            "Epoch 485/500, Loss:1.3999361994414712e-11\n",
            "Epoch 486/500, Loss:1.3435013596413031e-11\n",
            "Epoch 487/500, Loss:1.2893415445606638e-11\n",
            "Epoch 488/500, Loss:1.2373650425350728e-11\n",
            "Epoch 489/500, Loss:1.1874838390019819e-11\n",
            "Epoch 490/500, Loss:1.1396134674795584e-11\n",
            "Epoch 491/500, Loss:1.0936728665172672e-11\n",
            "Epoch 492/500, Loss:1.049584242456529e-11\n",
            "Epoch 493/500, Loss:1.007272937669363e-11\n",
            "Epoch 494/500, Loss:9.666673041599557e-12\n",
            "Epoch 495/500, Loss:9.276985822327659e-12\n",
            "Epoch 496/500, Loss:8.903007840515333e-12\n",
            "Epoch 497/500, Loss:8.544105819091458e-12\n",
            "Epoch 498/500, Loss:8.199672009897164e-12\n",
            "Epoch 499/500, Loss:7.869123164494795e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X, Y)\n",
        "mod.evaluate(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz2Q6eSC82cI",
        "outputId": "0aecaaa5-1f84-436e-8b7a-14d4ccc11374"
      },
      "id": "Iz2Q6eSC82cI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1]\n",
            " [1 1]\n",
            " [1 1]\n",
            " [1 1]] [[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.40259787e-06],\n",
              "       [-1.40259787e-06],\n",
              "       [-1.40259787e-06],\n",
              "       [-1.40259787e-06]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Flatten images to 1D vector of 784 features (28*28)\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "def test(model, X, Y, batch_size):\n",
        "  k = int(len(X)/batch_size)\n",
        "  for i in range(k):\n",
        "    Y_hat=model.evaluate(X[i*batch_size:(i+1)*batch_size])\n",
        "    wrong=0\n",
        "    for j in range(batch_size):\n",
        "      max1,max2,id1,id2=-999,-999,-1,-1\n",
        "      for l in range(10):\n",
        "        if max1 < Y_hat[j][l]:\n",
        "          max1,id1=Y_hat[j][l],l\n",
        "        if max2 < Y[i*batch_size+j][l]:\n",
        "          max2,id2=Y[i*batch_size+j][l],l\n",
        "      if id1 != id2: wrong+=1\n",
        "    print(f\"batch: {i}, accuracy: {(batch_size-wrong)/batch_size*100}%\")\n",
        "mod2 = Model(784, 10)\n",
        "mod2.addLayer(32, mod2.Input_layer, mod2.Output_layer)"
      ],
      "metadata": {
        "id": "lcOYUuYAjXPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb62d420-4797-4824-bfcf-002fc6add02a"
      },
      "id": "lcOYUuYAjXPG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.Model.Neuron at 0x7dd201290890>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd893390>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd8935d0>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd893690>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd893750>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd893850>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd893910>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd8939d0>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd893a90>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd893810>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd893bd0>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd893c90>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd893d50>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd893e10>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd893ed0>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd893f90>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd898090>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd898150>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd898210>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd8982d0>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd898390>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd898450>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd898510>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd8985d0>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd898690>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd898750>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd898810>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd8988d0>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd898990>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd898a50>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd898b10>,\n",
              " <__main__.Model.Neuron at 0x7dd1fd898bd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(mod2.Input_layer))\n",
        "mod2.parallel_train(x_train, y_train, 2048, 5, 0.01)\n",
        "\"\"\"\n",
        "for i in range(1,9):\n",
        "  plt.subplot(330+i)\n",
        "  plt.imshow(x_test[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
        "print(mod2.evaluate(np.array([x_test[5]])))\n",
        "print(y_test[5])\n",
        "\"\"\"\n",
        "test(mod2, x_test, y_test, 500)"
      ],
      "metadata": {
        "id": "ClmATYl3Qcb7",
        "outputId": "3b224b3f-86c0-40f8-c5e0-78389b2faaf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        }
      },
      "id": "ClmATYl3Qcb7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b26c3796ca9e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmod2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m330\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-258d2c9cbf69>\u001b[0m in \u001b[0;36mparallel_train\u001b[0;34m(self, X, Y, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m       with mp.Pool(processes=k) as pool: results = pool.starmap(Model.backward, \\\n\u001b[0m\u001b[1;32m    175\u001b[0m        [(self, X[b*batch_size:(b+1)*batch_size], Y[b*batch_size:(b+1)*batch_size],\\\n\u001b[1;32m    176\u001b[0m          batch_size, learning_rate, 0.9, 0.9) for b in range(k)])\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         '''\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(mod2, x_test, y_test, 5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhe7AJ7MHY5E",
        "outputId": "d105e306-d537-4270-df77-0b5a40849400"
      },
      "id": "Bhe7AJ7MHY5E",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch: 0, accuracy: 69.34%\n",
            "batch: 1, accuracy: 78.25999999999999%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}