{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diputs03/AI-Studies/blob/main/Creating_network/dymamic_architect_rebuilt_with_adam_paralleltrain_graphically.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Aiming a Dynaimic Graph-structured NeuronNetwork\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import deque\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "metadata": {
        "id": "6mXjwpToZTeV"
      },
      "id": "6mXjwpToZTeV",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Activation function in this case in \\tanh, thus\n",
        "\\dfrac{d\\tanh(x)}{dx}=1-\\tanh^2(x)\n",
        "however, for other activation funtions\n",
        "\\dfrac{d\\sigma(x)}{dx}=\\sigma(x)\\cdot\\left\\big(1-\\sigma(x)\\right\\big)\n",
        "\\dfrac{d\\mathop{\\mathrm{ReLu}}(x)}{dx}=\\begin{cases}1&x\\ge0\\\\0&\\text{else}\\end{cases}\n",
        "Loss is the Euclidean loss\n",
        "\\dfrac{d\\L}\n",
        "\"\"\"\n",
        "class Model:\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.idcnt = 0\n",
        "    self.prev, self.next = {}, {}\n",
        "    self.neurons = set()\n",
        "\n",
        "    self.Input_layer = [self.idcnt+i for i in range(input_size)]\n",
        "    self.neurons.update([self.idcnt+i for i in range(input_size)])\n",
        "    self.idcnt+=input_size\n",
        "\n",
        "    self.Output_layer = [self.idcnt+o for o in range(output_size)]\n",
        "    self.neurons.update([self.idcnt+o for o in range(output_size)])\n",
        "    self.idcnt+=output_size\n",
        "\n",
        "    for i in self.Input_layer: self.next[i], self.prev[i] = self.Output_layer, []\n",
        "    for o in self.Output_layer: self.prev[o], self.next[o] = self.Input_layer, []\n",
        "\n",
        "    self.weight = {}\n",
        "    self.weight_gsum, self.weight_gsqr = {}, {}\n",
        "    for u in self.Input_layer:\n",
        "      for v in self.Output_layer:\n",
        "        self.weight[(u,v)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_gsum[(u,v)], self.weight_gsqr[(u,v)] = 0, 0\n",
        "\n",
        "    self.bias = {}\n",
        "    self.bias_gsum, self.bias_gsqr = {}, {}\n",
        "    for i in self.neurons:\n",
        "      self.bias[i] = np.random.uniform(-0.1, 0.1)\n",
        "      self.bias_gsum[i], self.bias_gsqr[i] = 0, 0\n",
        "\n",
        "  def forward(self, X, batch_size):\n",
        "    assert X.shape == (batch_size,len(self.Input_layer))\n",
        "    a = {q: np.zeros(batch_size) for q in self.neurons}\n",
        "\n",
        "    for i, n in enumerate(self.Input_layer):\n",
        "      a[n] = X[:, i]\n",
        "\n",
        "    q = deque()\n",
        "    for i in self.Input_layer:\n",
        "      q.append(i)\n",
        "\n",
        "    cnt = {q: 0 for q in self.neurons}\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      a[c] = np.tanh(a[c] + self.bias[c])\n",
        "      for n in self.next[c]:\n",
        "        a[n] = a[n] + a[c] * self.weight[(c,n)]\n",
        "        cnt[n] += 1\n",
        "        if cnt[n] == len(self.prev[n]):\n",
        "          q.append(n)\n",
        "    return a\n",
        "\n",
        "  def evaluate(self, X):\n",
        "    a = self.forward(X, len(X))\n",
        "    return np.array([a[o] for o in self.Output_layer]).T\n",
        "\n",
        "  def backward(self, X, Y, batch_size, learning_rate, dsum, dsqr):\n",
        "    assert X.shape == (batch_size,len(self.Input_layer))\n",
        "    assert Y.shape == (batch_size,len(self.Output_layer))\n",
        "    a = self.forward(X, batch_size)\n",
        "\n",
        "    delta_b, delta_w = {}, {}\n",
        "\n",
        "    par_a = {q: np.zeros(batch_size) for q in self.neurons}\n",
        "    for o, n in enumerate(self.Output_layer):\n",
        "      par_a[n] = 2 * (a[n] - Y[:, o])\n",
        "\n",
        "    q = deque()\n",
        "    for o in self.Output_layer:\n",
        "      q.append(o)\n",
        "\n",
        "    cnt = {q: 0 for q in self.neurons}\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      par_b = par_a[c] * (1-a[c]**2)\n",
        "\n",
        "      gbias = par_b\n",
        "      self.bias_gsum[c] = (1-dsum)*np.sum(gbias)/batch_size + dsum*self.bias_gsum[c]\n",
        "      self.bias_gsqr[c] = (1-dsqr)*np.sum(gbias**2)/batch_size + dsqr*self.bias_gsqr[c]\n",
        "      delta_b[c] = -learning_rate * self.bias_gsum[c] / (self.bias_gsqr[c]**(1/2)+1)\n",
        "\n",
        "      for p in self.prev[c]:\n",
        "        par_a[p] += par_a[c] * (1-a[c]**2) * self.weight[(p,c)]\n",
        "        gweight = par_a[c] * (1-a[c]**2) * a[p]\n",
        "        self.weight_gsum[(p,c)] = \\\n",
        "         (1-dsum)*np.sum(gweight)/batch_size + dsum*self.weight_gsum[(p,c)]\n",
        "        self.weight_gsqr[(p,c)] = \\\n",
        "         (1-dsqr)*np.sum(gweight**2)/batch_size + dsqr*self.weight_gsqr[(p,c)]\n",
        "        delta_w[(p,c)] = \\\n",
        "         -learning_rate * self.weight_gsum[(p,c)] / (self.weight_gsqr[(p,c)]**(1/2)+1)\n",
        "\n",
        "        cnt[p] += 1\n",
        "        if cnt[p] == len(self.next[p]):\n",
        "          q.append(p)\n",
        "\n",
        "    return delta_w, delta_b\n",
        "\n",
        "  def update(self, X, Y, batch_size, learning_rate, dsum=0.9, dsqr=0.9):\n",
        "    delta_w, delta_b = \\\n",
        "     self.backward(X, Y, batch_size, learning_rate, dsum, dsqr)\n",
        "    for w in self.weight:\n",
        "        self.weight[w] += delta_w[w]\n",
        "    for p in self.neurons:\n",
        "        self.bias[p] += delta_b[p]\n",
        "\n",
        "  def addLayer(self, mid_size, UP, DOWN):\n",
        "    Mid_layer = [self.idcnt+m for m in range(mid_size)]\n",
        "    self.neurons.update([self.idcnt+m for m in range(mid_size)])\n",
        "    self.idcnt+=mid_size\n",
        "\n",
        "    for m in Mid_layer:\n",
        "      self.bias[m] = np.random.uniform(-0.1, 0.1)\n",
        "      self.bias_gsum[m], self.bias_gsqr[m] = 0, 0\n",
        "\n",
        "      self.prev[m] = UP\n",
        "      for u in UP:\n",
        "        self.weight[(u,m)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_gsum[(u,m)], self.weight_gsqr[(u,m)] = 0, 0\n",
        "\n",
        "      self.next[m] = DOWN\n",
        "      for v in DOWN:\n",
        "        self.weight[(m,v)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_gsum[(m,v)], self.weight_gsqr[(m,v)] = 0, 0\n",
        "\n",
        "    for u in UP:\n",
        "      self.next[u] = Mid_layer\n",
        "    for v in DOWN:\n",
        "      self.prev[v] = Mid_layer\n",
        "\n",
        "    for u in UP:\n",
        "      for v in DOWN:\n",
        "        self.weight.pop((u,v))\n",
        "        self.weight_gsum.pop((u,v))\n",
        "        self.weight_gsqr.pop((u,v))\n",
        "    return Mid_layer\n",
        "\n",
        "  def train(self, X, Y, batch_size, epochs, learning_rate):\n",
        "    assert len(X) == len(Y)\n",
        "    l = len(X)\n",
        "    for epoch in range(epochs):\n",
        "      data=[(X[_], Y[_]) for _ in range(len(X))]\n",
        "      random.shuffle(data)\n",
        "      for _ in range(len(X)):\n",
        "        X[_],Y[_]=data[_]\n",
        "      loss = 0\n",
        "      for batch in range(int(l / batch_size)):\n",
        "        L, R = batch * batch_size, (batch + 1) * batch_size\n",
        "        x_train, y_train = X[L:R], Y[L:R]\n",
        "        self.update(x_train, y_train, batch_size, learning_rate)\n",
        "        output = self.evaluate(x_train)\n",
        "        loss += np.sum(((y_train-output) ** 2), axis=(0,1))\n",
        "      loss = ((loss) ** 0.5) / (int(l / batch_size) * batch_size)\n",
        "      print(f\"Epoch {epoch}/{epochs}, Loss:{loss}\")\n",
        "\n",
        "  def parallel_train(self, X, Y, batch_size, epochs=10, learning_rate=0.1):\n",
        "    assert len(X) == len(Y)\n",
        "    l = len(X)\n",
        "    for epoch in range(epochs):\n",
        "      data=[(X[_], Y[_]) for _ in range(len(X))]\n",
        "      random.shuffle(data)\n",
        "      for _ in range(len(X)):\n",
        "        X[_],Y[_]=data[_]\n",
        "\n",
        "      k = int(l / batch_size)\n",
        "      with ThreadPoolExecutor(max_workers=k) as executor:\n",
        "        futures = executor.submit(Model.backward, \\\n",
        "         [(self, X[b*batch_size:(b+1)*batch_size], Y[b*batch_size:(b+1)*batch_size],\\\n",
        "          batch_size, learning_rate, 0.9, 0.9) for b in range(k)])\n",
        "      print(futures)\n",
        "      results = [f.result() for f in futures]\n",
        "\n",
        "      delta_w = {w: np.sum([res[0][w] for res in results], axis=0) / k for w in self.weight}\n",
        "      delta_b = {n: np.sum([res[1][n] for res in results], axis=0) / k for n in self.neurons}\n",
        "      for w in self.weight:\n",
        "        self.weight[w] += delta_w[w]\n",
        "      for p in self.neurons:\n",
        "        self.bias[p] += delta_b[p]\n",
        "      output = self.evaluate(X)\n",
        "      loss = (np.sum(((Y-output) ** 2), axis=(0,1)) ** 0.5)\\\n",
        "       / (int(l / batch_size) * batch_size)\n",
        "      print(f\"Epoch {epoch}/{epochs}, Loss:{loss}\")"
      ],
      "metadata": {
        "id": "YUliU5Cx-oyU"
      },
      "id": "YUliU5Cx-oyU",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "Y=np.array([[0],[1],[1],[0]])\n",
        "mod=Model(2, 1)\n",
        "mid1=mod.addLayer(4, mod.Input_layer, mod.Output_layer)\n",
        "mod.addLayer(4, mid1, mod.Output_layer)\n",
        "mod.evaluate(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfiQiE5l8XOr",
        "outputId": "ce764519-f3b3-4a3c-bc19-8c3a04462664"
      },
      "id": "KfiQiE5l8XOr",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02379764],\n",
              "       [0.02426833],\n",
              "       [0.02363847],\n",
              "       [0.02410763]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod.train(X, Y, 4, 500, 0.1)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC2n_sC_8xnP",
        "outputId": "9d54e304-98d9-488a-a402-75e6afa9e9ca"
      },
      "id": "aC2n_sC_8xnP",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/500, Loss:0.34289946410931593\n",
            "Epoch 1/500, Loss:0.24071016542345774\n",
            "Epoch 2/500, Loss:0.023898640166187417\n",
            "Epoch 3/500, Loss:0.02706523810784121\n",
            "Epoch 4/500, Loss:0.029562562379608737\n",
            "Epoch 5/500, Loss:0.0314054171997871\n",
            "Epoch 6/500, Loss:0.03261720835359596\n",
            "Epoch 7/500, Loss:0.03322904657112586\n",
            "Epoch 8/500, Loss:0.0332787645412657\n",
            "Epoch 9/500, Loss:0.03280988430878808\n",
            "Epoch 10/500, Loss:0.03187056694902237\n",
            "Epoch 11/500, Loss:0.03051257024460476\n",
            "Epoch 12/500, Loss:0.02879023352714993\n",
            "Epoch 13/500, Loss:0.026759502751925932\n",
            "Epoch 14/500, Loss:0.024477003895736402\n",
            "Epoch 15/500, Loss:0.02199916926809729\n",
            "Epoch 16/500, Loss:0.019381419368334294\n",
            "Epoch 17/500, Loss:0.0166774023287554\n",
            "Epoch 18/500, Loss:0.013938293422037736\n",
            "Epoch 19/500, Loss:0.011212158177258714\n",
            "Epoch 20/500, Loss:0.008543383946337357\n",
            "Epoch 21/500, Loss:0.0059721859454452105\n",
            "Epoch 22/500, Loss:0.0035341945918551856\n",
            "Epoch 23/500, Loss:0.0012601311694473334\n",
            "Epoch 24/500, Loss:0.000824421641399737\n",
            "Epoch 25/500, Loss:0.0026991491069149314\n",
            "Epoch 26/500, Loss:0.004349030446842303\n",
            "Epoch 27/500, Loss:0.0057642610651283226\n",
            "Epoch 28/500, Loss:0.006940073900264696\n",
            "Epoch 29/500, Loss:0.007876466447961418\n",
            "Epoch 30/500, Loss:0.008577843648655211\n",
            "Epoch 31/500, Loss:0.009052590059651972\n",
            "Epoch 32/500, Loss:0.009312587159432995\n",
            "Epoch 33/500, Loss:0.00937269296542576\n",
            "Epoch 34/500, Loss:0.009250201258413448\n",
            "Epoch 35/500, Loss:0.008964296668944887\n",
            "Epoch 36/500, Loss:0.008535519943885028\n",
            "Epoch 37/500, Loss:0.007985255232994717\n",
            "Epoch 38/500, Loss:0.007335248591995066\n",
            "Epoch 39/500, Loss:0.006607164403902188\n",
            "Epoch 40/500, Loss:0.00582218428012672\n",
            "Epoch 41/500, Loss:0.005000651305478464\n",
            "Epoch 42/500, Loss:0.004161761228187191\n",
            "Epoch 43/500, Loss:0.003323301295195853\n",
            "Epoch 44/500, Loss:0.002501436792800353\n",
            "Epoch 45/500, Loss:0.0017105448691797811\n",
            "Epoch 46/500, Loss:0.0009630947989247848\n",
            "Epoch 47/500, Loss:0.00026957343347061574\n",
            "Epoch 48/500, Loss:0.0003615458766128864\n",
            "Epoch 49/500, Loss:0.0009237931194731049\n",
            "Epoch 50/500, Loss:0.001412653026796474\n",
            "Epoch 51/500, Loss:0.0018254814622932296\n",
            "Epoch 52/500, Loss:0.0021613908178103342\n",
            "Epoch 53/500, Loss:0.0024211089963715143\n",
            "Epoch 54/500, Loss:0.0026068170099704303\n",
            "Epoch 55/500, Loss:0.0027219705099353476\n",
            "Epoch 56/500, Loss:0.002771110660604359\n",
            "Epoch 57/500, Loss:0.0027596696464300343\n",
            "Epoch 58/500, Loss:0.0026937757786597138\n",
            "Epoch 59/500, Loss:0.0025800626755027554\n",
            "Epoch 60/500, Loss:0.0024254863813691243\n",
            "Epoch 61/500, Loss:0.002237153624353145\n",
            "Epoch 62/500, Loss:0.0020221637395549507\n",
            "Epoch 63/500, Loss:0.0017874661493306066\n",
            "Epoch 64/500, Loss:0.0015397347147961273\n",
            "Epoch 65/500, Loss:0.0012852597660916688\n",
            "Epoch 66/500, Loss:0.0010298581814284107\n",
            "Epoch 67/500, Loss:0.0007788015097288401\n",
            "Epoch 68/500, Loss:0.0005367618090856719\n",
            "Epoch 69/500, Loss:0.00030777459385483675\n",
            "Epoch 70/500, Loss:9.521803932131122e-05\n",
            "Epoch 71/500, Loss:9.819262062719442e-05\n",
            "Epoch 72/500, Loss:0.00027039675314133267\n",
            "Epoch 73/500, Loss:0.00041996745208714776\n",
            "Epoch 74/500, Loss:0.0005460796681192353\n",
            "Epoch 75/500, Loss:0.0006484689797391688\n",
            "Epoch 76/500, Loss:0.0007273827054714775\n",
            "Epoch 77/500, Loss:0.0007835250829921322\n",
            "Epoch 78/500, Loss:0.0008179982267271146\n",
            "Epoch 79/500, Loss:0.0008322405207438801\n",
            "Epoch 80/500, Loss:0.0008279640107817789\n",
            "Epoch 81/500, Loss:0.0008070922327417176\n",
            "Epoch 82/500, Loss:0.000771699761869412\n",
            "Epoch 83/500, Loss:0.0007239545955883712\n",
            "Epoch 84/500, Loss:0.0006660643020970048\n",
            "Epoch 85/500, Loss:0.0006002266842897069\n",
            "Epoch 86/500, Loss:0.0005285855306611237\n",
            "Epoch 87/500, Loss:0.0004531918562121458\n",
            "Epoch 88/500, Loss:0.0003759708799392161\n",
            "Epoch 89/500, Loss:0.0002986948428654584\n",
            "Epoch 90/500, Loss:0.00022296164244719905\n",
            "Epoch 91/500, Loss:0.0001501791456906977\n",
            "Epoch 92/500, Loss:8.155494429145639e-05\n",
            "Epoch 93/500, Loss:1.8091230307539668e-05\n",
            "Epoch 94/500, Loss:3.941559997456344e-05\n",
            "Epoch 95/500, Loss:9.037106349283282e-05\n",
            "Epoch 96/500, Loss:0.00013437492244537465\n",
            "Epoch 97/500, Loss:0.00017121025480042602\n",
            "Epoch 98/500, Loss:0.00020082980502852447\n",
            "Epoch 99/500, Loss:0.0002233401623466049\n",
            "Epoch 100/500, Loss:0.00023898430760030618\n",
            "Epoch 101/500, Loss:0.00024812305285322255\n",
            "Epoch 102/500, Loss:0.0002512158708545482\n",
            "Epoch 103/500, Loss:0.00024880157613255057\n",
            "Epoch 104/500, Loss:0.00024147927705561723\n",
            "Epoch 105/500, Loss:0.00022988997052177573\n",
            "Epoch 106/500, Loss:0.0002146990997449971\n",
            "Epoch 107/500, Loss:0.00019658034260837732\n",
            "Epoch 108/500, Loss:0.0001762008448001449\n",
            "Epoch 109/500, Loss:0.00015420805976758893\n",
            "Epoch 110/500, Loss:0.00013121830749888465\n",
            "Epoch 111/500, Loss:0.00010780711711707524\n",
            "Epoch 112/500, Loss:8.450137487549578e-05\n",
            "Epoch 113/500, Loss:6.177325983456286e-05\n",
            "Epoch 114/500, Loss:4.00359145869099e-05\n",
            "Epoch 115/500, Loss:1.964076807570701e-05\n",
            "Epoch 116/500, Loss:8.764019166393945e-07\n",
            "Epoch 117/500, Loss:1.6031169298885047e-05\n",
            "Epoch 118/500, Loss:3.091694953393019e-05\n",
            "Epoch 119/500, Loss:4.367430226355616e-05\n",
            "Epoch 120/500, Loss:5.4251345908782296e-05\n",
            "Epoch 121/500, Loss:6.2646572172018e-05\n",
            "Epoch 122/500, Loss:6.890385529102464e-05\n",
            "Epoch 123/500, Loss:7.31070163784596e-05\n",
            "Epoch 124/500, Loss:7.537410010520668e-05\n",
            "Epoch 125/500, Loss:7.585151143728521e-05\n",
            "Epoch 126/500, Loss:7.470814839495994e-05\n",
            "Epoch 127/500, Loss:7.212965333271365e-05\n",
            "Epoch 128/500, Loss:6.831289050687263e-05\n",
            "Epoch 129/500, Loss:6.346074215868469e-05\n",
            "Epoch 130/500, Loss:5.777729942325559e-05\n",
            "Epoch 131/500, Loss:5.146350846887265e-05\n",
            "Epoch 132/500, Loss:4.471331672145619e-05\n",
            "Epoch 133/500, Loss:3.7710349129724855e-05\n",
            "Epoch 134/500, Loss:3.0625130422495793e-05\n",
            "Epoch 135/500, Loss:2.3612856397148824e-05\n",
            "Epoch 136/500, Loss:1.681170560842592e-05\n",
            "Epoch 137/500, Loss:1.0341672509311783e-05\n",
            "Epoch 138/500, Loss:4.303894201867443e-06\n",
            "Epoch 139/500, Loss:1.2195644781063713e-06\n",
            "Epoch 140/500, Loss:6.165508788226397e-06\n",
            "Epoch 141/500, Loss:1.0489040593355121e-05\n",
            "Epoch 142/500, Loss:1.4162662479351485e-05\n",
            "Epoch 143/500, Loss:1.7175110199597454e-05\n",
            "Epoch 144/500, Loss:1.9529964778441726e-05\n",
            "Epoch 145/500, Loss:2.1244095065751622e-05\n",
            "Epoch 146/500, Loss:2.234598000439351e-05\n",
            "Epoch 147/500, Loss:2.2873957466528967e-05\n",
            "Epoch 148/500, Loss:2.287444337024929e-05\n",
            "Epoch 149/500, Loss:2.2400161043233384e-05\n",
            "Epoch 150/500, Loss:2.1508416593398824e-05\n",
            "Epoch 151/500, Loss:2.0259451512577733e-05\n",
            "Epoch 152/500, Loss:1.8714899007359805e-05\n",
            "Epoch 153/500, Loss:1.6936365741334813e-05\n",
            "Epoch 154/500, Loss:1.4984155894631e-05\n",
            "Epoch 155/500, Loss:1.2916149797001715e-05\n",
            "Epoch 156/500, Loss:1.0786844953934208e-05\n",
            "Epoch 157/500, Loss:8.646563132128143e-06\n",
            "Epoch 158/500, Loss:6.540823358459584e-06\n",
            "Epoch 159/500, Loss:4.509877259743124e-06\n",
            "Epoch 160/500, Loss:2.588400160878454e-06\n",
            "Epoch 161/500, Loss:8.053287867375798e-07\n",
            "Epoch 162/500, Loss:8.161657127215199e-07\n",
            "Epoch 163/500, Loss:2.258582374673684e-06\n",
            "Epoch 164/500, Loss:3.509886877680544e-06\n",
            "Epoch 165/500, Loss:4.563214868570758e-06\n",
            "Epoch 166/500, Loss:5.416497624617106e-06\n",
            "Epoch 167/500, Loss:6.072025582102263e-06\n",
            "Epoch 168/500, Loss:6.535965001600587e-06\n",
            "Epoch 169/500, Loss:6.817842488551106e-06\n",
            "Epoch 170/500, Loss:6.930011285027113e-06\n",
            "Epoch 171/500, Loss:6.887112236776709e-06\n",
            "Epoch 172/500, Loss:6.705541160427105e-06\n",
            "Epoch 173/500, Loss:6.402933030472706e-06\n",
            "Epoch 174/500, Loss:5.997672014237732e-06\n",
            "Epoch 175/500, Loss:5.5084349430570924e-06\n",
            "Epoch 176/500, Loss:4.953774354328848e-06\n",
            "Epoch 177/500, Loss:4.351745803660515e-06\n",
            "Epoch 178/500, Loss:3.719582757059213e-06\n",
            "Epoch 179/500, Loss:3.0734210541640013e-06\n",
            "Epoch 180/500, Loss:2.428073705487738e-06\n",
            "Epoch 181/500, Loss:1.796855665500637e-06\n",
            "Epoch 182/500, Loss:1.1914572220669742e-06\n",
            "Epoch 183/500, Loss:6.218637696289809e-07\n",
            "Epoch 184/500, Loss:9.631899419324885e-08\n",
            "Epoch 185/500, Loss:3.786721058000758e-07\n",
            "Epoch 186/500, Loss:7.983044078424261e-07\n",
            "Epoch 187/500, Loss:1.1594024467632138e-06\n",
            "Epoch 188/500, Loss:1.4603233865850591e-06\n",
            "Epoch 189/500, Loss:1.7008378773536938e-06\n",
            "Epoch 190/500, Loss:1.8819934812796855e-06\n",
            "Epoch 191/500, Loss:2.005965246230699e-06\n",
            "Epoch 192/500, Loss:2.0758978137823386e-06\n",
            "Epoch 193/500, Loss:2.0957431862623188e-06\n",
            "Epoch 194/500, Loss:2.0700979549698213e-06\n",
            "Epoch 195/500, Loss:2.0040434227944453e-06\n",
            "Epoch 196/500, Loss:1.9029916510210061e-06\n",
            "Epoch 197/500, Loss:1.7725400341320882e-06\n",
            "Epoch 198/500, Loss:1.6183365689425132e-06\n",
            "Epoch 199/500, Loss:1.4459575457108722e-06\n",
            "Epoch 200/500, Loss:1.2607989581592165e-06\n",
            "Epoch 201/500, Loss:1.0679825148547368e-06\n",
            "Epoch 202/500, Loss:8.722767429964089e-07\n",
            "Epoch 203/500, Loss:6.780333132634574e-07\n",
            "Epoch 204/500, Loss:4.891383855062416e-07\n",
            "Epoch 205/500, Loss:3.08978483333894e-07\n",
            "Epoch 206/500, Loss:1.4042015318896522e-07\n",
            "Epoch 207/500, Loss:1.4197548152536026e-08\n",
            "Epoch 208/500, Loss:1.530588644594413e-07\n",
            "Epoch 209/500, Loss:2.748567063173818e-07\n",
            "Epoch 210/500, Loss:3.7876892095687866e-07\n",
            "Epoch 211/500, Loss:4.644268678444738e-07\n",
            "Epoch 212/500, Loss:5.318777217585841e-07\n",
            "Epoch 213/500, Loss:5.815419131366472e-07\n",
            "Epoch 214/500, Loss:6.141670754982193e-07\n",
            "Epoch 215/500, Loss:6.307798052631582e-07\n",
            "Epoch 216/500, Loss:6.326364541904209e-07\n",
            "Epoch 217/500, Loss:6.21174072759054e-07\n",
            "Epoch 218/500, Loss:5.979625077955868e-07\n",
            "Epoch 219/500, Loss:5.64658533422421e-07\n",
            "Epoch 220/500, Loss:5.22962764215795e-07\n",
            "Epoch 221/500, Loss:4.745799668680985e-07\n",
            "Epoch 222/500, Loss:4.211832544230534e-07\n",
            "Epoch 223/500, Loss:3.6438251815975424e-07\n",
            "Epoch 224/500, Loss:3.0569732870247914e-07\n",
            "Epoch 225/500, Loss:2.4653442202524685e-07\n",
            "Epoch 226/500, Loss:1.881697792311709e-07\n",
            "Epoch 227/500, Loss:1.317352127710885e-07\n",
            "Epoch 228/500, Loss:7.82092869258237e-08\n",
            "Epoch 229/500, Loss:2.8412327613899746e-08\n",
            "Epoch 230/500, Loss:1.6994783782510452e-08\n",
            "Epoch 231/500, Loss:5.750838593765262e-08\n",
            "Epoch 232/500, Loss:9.277677169962603e-08\n",
            "Epoch 233/500, Loss:1.2259229421277444e-07\n",
            "Epoch 234/500, Loss:1.4688127177356444e-07\n",
            "Epoch 235/500, Loss:1.6569212105674564e-07\n",
            "Epoch 236/500, Loss:1.79182142267081e-07\n",
            "Epoch 237/500, Loss:1.8760336534359004e-07\n",
            "Epoch 238/500, Loss:1.9128784494326407e-07\n",
            "Epoch 239/500, Loss:1.9063276461040355e-07\n",
            "Epoch 240/500, Loss:1.8608567844230664e-07\n",
            "Epoch 241/500, Loss:1.7813018288311286e-07\n",
            "Epoch 242/500, Loss:1.672722731045826e-07\n",
            "Epoch 243/500, Loss:1.5402759873995134e-07\n",
            "Epoch 244/500, Loss:1.3890979360785597e-07\n",
            "Epoch 245/500, Loss:1.2242001431561205e-07\n",
            "Epoch 246/500, Loss:1.0503778401509142e-07\n",
            "Epoch 247/500, Loss:8.721320088724263e-08\n",
            "Epoch 248/500, Loss:6.936053664188421e-08\n",
            "Epoch 249/500, Loss:5.185321890334246e-08\n",
            "Epoch 250/500, Loss:3.502016329378661e-08\n",
            "Epoch 251/500, Loss:1.9143396345986514e-08\n",
            "Epoch 252/500, Loss:4.456889563355174e-09\n",
            "Epoch 253/500, Loss:8.853492332924066e-09\n",
            "Epoch 254/500, Loss:2.064903927402952e-08\n",
            "Epoch 255/500, Loss:3.0836360004728477e-08\n",
            "Epoch 256/500, Loss:3.9364789013087484e-08\n",
            "Epoch 257/500, Loss:4.622316577889891e-08\n",
            "Epoch 258/500, Loss:5.143611598017859e-08\n",
            "Epoch 259/500, Loss:5.5059961654455165e-08\n",
            "Epoch 260/500, Loss:5.717838233660111e-08\n",
            "Epoch 261/500, Loss:5.789794211793495e-08\n",
            "Epoch 262/500, Loss:5.734358894822984e-08\n",
            "Epoch 263/500, Loss:5.565422233960356e-08\n",
            "Epoch 264/500, Loss:5.29784146574041e-08\n",
            "Epoch 265/500, Loss:4.947035950331175e-08\n",
            "Epoch 266/500, Loss:4.528610852896202e-08\n",
            "Epoch 267/500, Loss:4.058014601866074e-08\n",
            "Epoch 268/500, Loss:3.550233853666194e-08\n",
            "Epoch 269/500, Loss:3.019528548335242e-08\n",
            "Epoch 270/500, Loss:2.479208545597999e-08\n",
            "Epoch 271/500, Loss:1.9414523211953503e-08\n",
            "Epoch 272/500, Loss:1.417167280401421e-08\n",
            "Epoch 273/500, Loss:9.158904280392314e-09\n",
            "Epoch 274/500, Loss:4.457274210096715e-09\n",
            "Epoch 275/500, Loss:1.332742984982288e-10\n",
            "Epoch 276/500, Loss:3.761092492136908e-09\n",
            "Epoch 277/500, Loss:7.187942646690384e-09\n",
            "Epoch 278/500, Loss:1.0122886617272215e-08\n",
            "Epoch 279/500, Loss:1.2554185747177742e-08\n",
            "Epoch 280/500, Loss:1.4481730882184846e-08\n",
            "Epoch 281/500, Loss:1.5915881683122364e-08\n",
            "Epoch 282/500, Loss:1.687620468479762e-08\n",
            "Epoch 283/500, Loss:1.73901464036244e-08\n",
            "Epoch 284/500, Loss:1.749167555568052e-08\n",
            "Epoch 285/500, Loss:1.7219925644044655e-08\n",
            "Epoch 286/500, Loss:1.661786607908173e-08\n",
            "Epoch 287/500, Loss:1.5731026535873917e-08\n",
            "Epoch 288/500, Loss:1.4606295742347614e-08\n",
            "Epoch 289/500, Loss:1.3290812165460752e-08\n",
            "Epoch 290/500, Loss:1.1830960436375392e-08\n",
            "Epoch 291/500, Loss:1.0271483807814187e-08\n",
            "Epoch 292/500, Loss:8.65471941075696e-09\n",
            "Epoch 293/500, Loss:7.019959958167245e-09\n",
            "Epoch 294/500, Loss:5.402942464954874e-09\n",
            "Epoch 295/500, Loss:3.835461925925238e-09\n",
            "Epoch 296/500, Loss:2.3451054553119688e-09\n",
            "Epoch 297/500, Loss:9.55100361646477e-10\n",
            "Epoch 298/500, Loss:3.157320618060039e-10\n",
            "Epoch 299/500, Loss:1.4529266728537799e-09\n",
            "Epoch 300/500, Loss:2.4462391546003354e-09\n",
            "Epoch 301/500, Loss:3.2894366065652075e-09\n",
            "Epoch 302/500, Loss:3.980025796565187e-09\n",
            "Epoch 303/500, Loss:4.518930956428496e-09\n",
            "Epoch 304/500, Loss:4.91013284610424e-09\n",
            "Epoch 305/500, Loss:5.160280461182637e-09\n",
            "Epoch 306/500, Loss:5.278286176841826e-09\n",
            "Epoch 307/500, Loss:5.274914391568963e-09\n",
            "Epoch 308/500, Loss:5.1623728503785715e-09\n",
            "Epoch 309/500, Loss:4.953914884428881e-09\n",
            "Epoch 310/500, Loss:4.6634597104106e-09\n",
            "Epoch 311/500, Loss:4.305236884008523e-09\n",
            "Epoch 312/500, Loss:3.8934598568188955e-09\n",
            "Epoch 313/500, Loss:3.4420325125284618e-09\n",
            "Epoch 314/500, Loss:2.9642914960766736e-09\n",
            "Epoch 315/500, Loss:2.4727860481899705e-09\n",
            "Epoch 316/500, Loss:1.9790962380202004e-09\n",
            "Epoch 317/500, Loss:1.4936894778775495e-09\n",
            "Epoch 318/500, Loss:1.0258144780665723e-09\n",
            "Epoch 319/500, Loss:5.834311239422824e-10\n",
            "Epoch 320/500, Loss:1.7317410504723296e-10\n",
            "Epoch 321/500, Loss:1.9965229708200638e-10\n",
            "Epoch 322/500, Loss:5.310512890977642e-10\n",
            "Epoch 323/500, Loss:8.182857874367705e-10\n",
            "Epoch 324/500, Loss:1.059809267255074e-09\n",
            "Epoch 325/500, Loss:1.2551788159190225e-09\n",
            "Epoch 326/500, Loss:1.4049539643658238e-09\n",
            "Epoch 327/500, Loss:1.5105848301822516e-09\n",
            "Epoch 328/500, Loss:1.5742929477396972e-09\n",
            "Epoch 329/500, Loss:1.5989480169585624e-09\n",
            "Epoch 330/500, Loss:1.5879435043362339e-09\n",
            "Epoch 331/500, Loss:1.5450738206218562e-09\n",
            "Epoch 332/500, Loss:1.474415454311151e-09\n",
            "Epoch 333/500, Loss:1.3802141380758054e-09\n",
            "Epoch 334/500, Loss:1.2667797826340643e-09\n",
            "Epoch 335/500, Loss:1.1383905818874995e-09\n",
            "Epoch 336/500, Loss:9.99207372658767e-10\n",
            "Epoch 337/500, Loss:8.531989912751586e-10\n",
            "Epoch 338/500, Loss:7.040790838807459e-10\n",
            "Epoch 339/500, Loss:5.552545233496187e-10\n",
            "Epoch 340/500, Loss:4.0978536232708085e-10\n",
            "Epoch 341/500, Loss:2.7035599605394756e-10\n",
            "Epoch 342/500, Loss:1.3925698918657026e-10\n",
            "Epoch 343/500, Loss:1.837691587444623e-11\n",
            "Epoch 344/500, Loss:9.07966531821236e-11\n",
            "Epoch 345/500, Loss:1.8716793371252594e-10\n",
            "Epoch 346/500, Loss:2.7001648725093197e-10\n",
            "Epoch 347/500, Loss:3.389746593227688e-10\n",
            "Epoch 348/500, Loss:3.939999186283738e-10\n",
            "Epoch 349/500, Loss:4.3534323390138907e-10\n",
            "Epoch 350/500, Loss:4.635145149645137e-10\n",
            "Epoch 351/500, Loss:4.792461312779633e-10\n",
            "Epoch 352/500, Loss:4.834554622953902e-10\n",
            "Epoch 353/500, Loss:4.772073493742451e-10\n",
            "Epoch 354/500, Loss:4.6167724938178667e-10\n",
            "Epoch 355/500, Loss:4.3811576415364595e-10\n",
            "Epoch 356/500, Loss:4.0781516813699625e-10\n",
            "Epoch 357/500, Loss:3.7207840953255855e-10\n",
            "Epoch 358/500, Loss:3.321909973659498e-10\n",
            "Epoch 359/500, Loss:2.893960602840667e-10\n",
            "Epoch 360/500, Loss:2.4487277049817235e-10\n",
            "Epoch 361/500, Loss:1.997182618937443e-10\n",
            "Epoch 362/500, Loss:1.5493306594269174e-10\n",
            "Epoch 363/500, Loss:1.1140998627118259e-10\n",
            "Epoch 364/500, Loss:6.992634899935513e-11\n",
            "Epoch 365/500, Loss:3.1139408326191775e-11\n",
            "Epoch 366/500, Loss:4.415289965239988e-12\n",
            "Epoch 367/500, Loss:3.632285813272296e-11\n",
            "Epoch 368/500, Loss:6.428560960468344e-11\n",
            "Epoch 369/500, Loss:8.811751892393027e-11\n",
            "Epoch 370/500, Loss:1.0773692289862968e-10\n",
            "Epoch 371/500, Loss:1.2315777304233722e-10\n",
            "Epoch 372/500, Loss:1.3447979015090983e-10\n",
            "Epoch 373/500, Loss:1.418778117993208e-10\n",
            "Epoch 374/500, Loss:1.4559065411590133e-10\n",
            "Epoch 375/500, Loss:1.4590976018559465e-10\n",
            "Epoch 376/500, Loss:1.431678751245835e-10\n",
            "Epoch 377/500, Loss:1.377280214789195e-10\n",
            "Epoch 378/500, Loss:1.2997292738597954e-10\n",
            "Epoch 379/500, Loss:1.2029511132875814e-10\n",
            "Epoch 380/500, Loss:1.0908775597250064e-10\n",
            "Epoch 381/500, Loss:9.67364803712567e-11\n",
            "Epoch 382/500, Loss:8.361208882375071e-11\n",
            "Epoch 383/500, Loss:7.006435427496516e-11\n",
            "Epoch 384/500, Loss:5.641686120008704e-11\n",
            "Epoch 385/500, Loss:4.296290493505106e-11\n",
            "Epoch 386/500, Loss:2.996243791270059e-11\n",
            "Epoch 387/500, Loss:1.763999796927651e-11\n",
            "Epoch 388/500, Loss:6.183597254030837e-12\n",
            "Epoch 389/500, Loss:4.255534509847725e-12\n",
            "Epoch 390/500, Loss:1.3562409875711445e-11\n",
            "Epoch 391/500, Loss:2.1657041394604337e-11\n",
            "Epoch 392/500, Loss:2.8492610614633263e-11\n",
            "Epoch 393/500, Loss:3.405311839801972e-11\n",
            "Epoch 394/500, Loss:3.835063451702869e-11\n",
            "Epoch 395/500, Loss:4.1422240854363523e-11\n",
            "Epoch 396/500, Loss:4.332676470202024e-11\n",
            "Epoch 397/500, Loss:4.414137306542332e-11\n",
            "Epoch 398/500, Loss:4.3958148752965e-11\n",
            "Epoch 399/500, Loss:4.288067795808759e-11\n",
            "Epoch 400/500, Loss:4.102075493520041e-11\n",
            "Epoch 401/500, Loss:3.8495235644975256e-11\n",
            "Epoch 402/500, Loss:3.542310650535285e-11\n",
            "Epoch 403/500, Loss:3.1922809664783336e-11\n",
            "Epoch 404/500, Loss:2.8109827182945524e-11\n",
            "Epoch 405/500, Loss:2.4094585484787934e-11\n",
            "Epoch 406/500, Loss:1.99806633911781e-11\n",
            "Epoch 407/500, Loss:1.5863336469065792e-11\n",
            "Epoch 408/500, Loss:1.1828421488807583e-11\n",
            "Epoch 409/500, Loss:7.951440721132297e-12\n",
            "Epoch 410/500, Loss:4.297085907586928e-12\n",
            "Epoch 411/500, Loss:9.189578351748162e-13\n",
            "Epoch 412/500, Loss:2.140433663644359e-12\n",
            "Epoch 413/500, Loss:4.849449834753994e-12\n",
            "Epoch 414/500, Loss:7.18689127307548e-12\n",
            "Epoch 415/500, Loss:9.141389468309002e-12\n",
            "Epoch 416/500, Loss:1.0710662825402784e-11\n",
            "Epoch 417/500, Loss:1.1900655374347258e-11\n",
            "Epoch 418/500, Loss:1.2724592646923272e-11\n",
            "Epoch 419/500, Loss:1.3201973368681674e-11\n",
            "Epoch 420/500, Loss:1.3357544671049881e-11\n",
            "Epoch 421/500, Loss:1.3220256053075863e-11\n",
            "Epoch 422/500, Loss:1.2822245435556434e-11\n",
            "Epoch 423/500, Loss:1.2197844730804652e-11\n",
            "Epoch 424/500, Loss:1.1382659138164941e-11\n",
            "Epoch 425/500, Loss:1.0412689157093813e-11\n",
            "Epoch 426/500, Loss:9.323548443712637e-12\n",
            "Epoch 427/500, Loss:8.149764500406387e-12\n",
            "Epoch 428/500, Loss:6.924171522607048e-12\n",
            "Epoch 429/500, Loss:5.677392583836038e-12\n",
            "Epoch 430/500, Loss:4.4374291567617075e-12\n",
            "Epoch 431/500, Loss:3.229341490398885e-12\n",
            "Epoch 432/500, Loss:2.075021795014398e-12\n",
            "Epoch 433/500, Loss:9.93055030568124e-13\n",
            "Epoch 434/500, Loss:1.3298823847707197e-15\n",
            "Epoch 435/500, Loss:8.962474859486314e-13\n",
            "Epoch 436/500, Loss:1.6830699160752527e-12\n",
            "Epoch 437/500, Loss:2.356267524844524e-12\n",
            "Epoch 438/500, Loss:2.913230637627273e-12\n",
            "Epoch 439/500, Loss:3.354017801340814e-12\n",
            "Epoch 440/500, Loss:3.681097744331896e-12\n",
            "Epoch 441/500, Loss:3.899050136577098e-12\n",
            "Epoch 442/500, Loss:4.014263097276705e-12\n",
            "Epoch 443/500, Loss:4.0346187762246855e-12\n",
            "Epoch 444/500, Loss:3.969180886742585e-12\n",
            "Epoch 445/500, Loss:3.827886792262536e-12\n",
            "Epoch 446/500, Loss:3.621254554900255e-12\n",
            "Epoch 447/500, Loss:3.360109499667141e-12\n",
            "Epoch 448/500, Loss:3.0553218375445335e-12\n",
            "Epoch 449/500, Loss:2.717585921921395e-12\n",
            "Epoch 450/500, Loss:2.3572064439258966e-12\n",
            "Epoch 451/500, Loss:1.9839297305673798e-12\n",
            "Epoch 452/500, Loss:1.6067932574748145e-12\n",
            "Epoch 453/500, Loss:1.234014409753903e-12\n",
            "Epoch 454/500, Loss:8.728942048341626e-13\n",
            "Epoch 455/500, Loss:5.297661181263846e-13\n",
            "Epoch 456/500, Loss:2.0995293177616947e-13\n",
            "Epoch 457/500, Loss:8.2237818985198e-14\n",
            "Epoch 458/500, Loss:3.4350105225511296e-13\n",
            "Epoch 459/500, Loss:5.7150855228838e-13\n",
            "Epoch 460/500, Loss:7.648499888990301e-13\n",
            "Epoch 461/500, Loss:9.22977406309089e-13\n",
            "Epoch 462/500, Loss:1.0461336658051934e-12\n",
            "Epoch 463/500, Loss:1.1352561685856744e-12\n",
            "Epoch 464/500, Loss:1.191899226885007e-12\n",
            "Epoch 465/500, Loss:1.2181327994909008e-12\n",
            "Epoch 466/500, Loss:1.2164551050491967e-12\n",
            "Epoch 467/500, Loss:1.1896922249426956e-12\n",
            "Epoch 468/500, Loss:1.1409081145108413e-12\n",
            "Epoch 469/500, Loss:1.0733161321524465e-12\n",
            "Epoch 470/500, Loss:9.902018441310112e-13\n",
            "Epoch 471/500, Loss:8.948423599330901e-13\n",
            "Epoch 472/500, Loss:7.90441063297509e-13\n",
            "Epoch 473/500, Loss:6.800708000215261e-13\n",
            "Epoch 474/500, Loss:5.666196678522084e-13\n",
            "Epoch 475/500, Loss:4.5274959996344233e-13\n",
            "Epoch 476/500, Loss:3.4086817569944916e-13\n",
            "Epoch 477/500, Loss:2.33098913435259e-13\n",
            "Epoch 478/500, Loss:1.3126782646977198e-13\n",
            "Epoch 479/500, Loss:3.689453256794373e-14\n",
            "Epoch 480/500, Loss:4.8808179720083444e-14\n",
            "Epoch 481/500, Loss:1.2492719532464225e-13\n",
            "Epoch 482/500, Loss:1.90840615879595e-13\n",
            "Epoch 483/500, Loss:2.4620127984931184e-13\n",
            "Epoch 484/500, Loss:2.9091334376174505e-13\n",
            "Epoch 485/500, Loss:3.2511666969714526e-13\n",
            "Epoch 486/500, Loss:3.4914822769072096e-13\n",
            "Epoch 487/500, Loss:3.635295189874377e-13\n",
            "Epoch 488/500, Loss:3.6892646056163603e-13\n",
            "Epoch 489/500, Loss:3.661227137435885e-13\n",
            "Epoch 490/500, Loss:3.5600060226126384e-13\n",
            "Epoch 491/500, Loss:3.3949991255777245e-13\n",
            "Epoch 492/500, Loss:3.1760119707791024e-13\n",
            "Epoch 493/500, Loss:2.912984523734119e-13\n",
            "Epoch 494/500, Loss:2.6157916978297724e-13\n",
            "Epoch 495/500, Loss:2.2940221770795244e-13\n",
            "Epoch 496/500, Loss:1.956798438562668e-13\n",
            "Epoch 497/500, Loss:1.6126791742288038e-13\n",
            "Epoch 498/500, Loss:1.2694814817415523e-13\n",
            "Epoch 499/500, Loss:9.342526752220692e-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X, Y)\n",
        "mod.evaluate(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz2Q6eSC82cI",
        "outputId": "941597b6-434b-42a9-bb12-2f1a23a74310"
      },
      "id": "Iz2Q6eSC82cI",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1]\n",
            " [1 1]\n",
            " [1 1]\n",
            " [1 1]] [[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.86850535e-13],\n",
              "       [-1.86850535e-13],\n",
              "       [-1.86850535e-13],\n",
              "       [-1.86850535e-13]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Flatten images to 1D vector of 784 features (28*28)\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "def test(model, X, Y, batch_size):\n",
        "  k = int(len(X)/batch_size)\n",
        "  for i in range(k):\n",
        "    Y_hat=model.evaluate(X[i*batch_size:(i+1)*batch_size])\n",
        "    wrong=0\n",
        "    for j in range(batch_size):\n",
        "      max1,max2,id1,id2=-999,-999,-1,-1\n",
        "      for l in range(10):\n",
        "        if max1 < Y_hat[j][l]:\n",
        "          max1,id1=Y_hat[j][l],l\n",
        "        if max2 < Y[i*batch_size+j][l]:\n",
        "          max2,id2=Y[i*batch_size+j][l],l\n",
        "      if id1 != id2: wrong+=1\n",
        "    print(f\"batch: {i}, accuracy: {(batch_size-wrong)/batch_size*100}%\")\n",
        "mod2 = Model(784, 10)\n",
        "mod2.addLayer(32, mod2.Input_layer, mod2.Output_layer)"
      ],
      "metadata": {
        "id": "lcOYUuYAjXPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944beeb1-f823-46ee-d161-52549b354d0c"
      },
      "id": "lcOYUuYAjXPG",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[794,\n",
              " 795,\n",
              " 796,\n",
              " 797,\n",
              " 798,\n",
              " 799,\n",
              " 800,\n",
              " 801,\n",
              " 802,\n",
              " 803,\n",
              " 804,\n",
              " 805,\n",
              " 806,\n",
              " 807,\n",
              " 808,\n",
              " 809,\n",
              " 810,\n",
              " 811,\n",
              " 812,\n",
              " 813,\n",
              " 814,\n",
              " 815,\n",
              " 816,\n",
              " 817,\n",
              " 818,\n",
              " 819,\n",
              " 820,\n",
              " 821,\n",
              " 822,\n",
              " 823,\n",
              " 824,\n",
              " 825]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(mod2.Input_layer))\n",
        "mod2.parallel_train(x_train, y_train, 2048, 5, 0.01)\n",
        "\"\"\"\n",
        "for i in range(1,9):\n",
        "  plt.subplot(330+i)\n",
        "  plt.imshow(x_test[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
        "print(mod2.evaluate(np.array([x_test[5]])))\n",
        "print(y_test[5])\n",
        "\"\"\"\n",
        "test(mod2, x_test, y_test, 500)"
      ],
      "metadata": {
        "id": "ClmATYl3Qcb7",
        "outputId": "c2f2afa0-0af8-434a-8a76-1b1cc8340df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "id": "ClmATYl3Qcb7",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n",
            "<Future at 0x7cfb35dd5890 state=finished raised TypeError>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'Future' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-b26c3796ca9e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmod2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m330\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-8d49c48e95c6>\u001b[0m in \u001b[0;36mparallel_train\u001b[0;34m(self, X, Y, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    179\u001b[0m           batch_size, learning_rate, 0.9, 0.9) for b in range(k)])\n\u001b[1;32m    180\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0mdelta_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Future' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(mod2, x_test, y_test, 5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhe7AJ7MHY5E",
        "outputId": "b81f74d0-33ed-437d-f846-9b568278f352"
      },
      "id": "Bhe7AJ7MHY5E",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch: 0, accuracy: 57.98%\n",
            "batch: 1, accuracy: 66.96%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}