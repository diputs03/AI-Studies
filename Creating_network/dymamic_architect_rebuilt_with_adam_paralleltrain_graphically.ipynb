{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diputs03/AI-Studies/blob/main/Creating_network/dymamic_architect_rebuilt_with_adam_paralleltrain_graphically.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Aiming a Dynaimic Graph-structured NeuronNetwork\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import deque\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "metadata": {
        "id": "6mXjwpToZTeV"
      },
      "id": "6mXjwpToZTeV",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Activation function in this case in \\tanh, thus\n",
        "\\dfrac{d\\tanh(x)}{dx}=1-\\tanh^2(x)\n",
        "however, for other activation funtions\n",
        "\\dfrac{d\\sigma(x)}{dx}=\\sigma(x)\\cdot\\left\\big(1-\\sigma(x)\\right\\big)\n",
        "\\dfrac{d\\mathop{\\mathrm{ReLu}}(x)}{dx}=\\begin{cases}1&x\\ge0\\\\0&\\text{else}\\end{cases}\n",
        "Loss is the Euclidean loss\n",
        "\\dfrac{d\\L}\n",
        "\"\"\"\n",
        "\n",
        "class Model:\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.idcnt = 0\n",
        "    self.prev, self.next = {}, {}\n",
        "    self.neurons = set()\n",
        "\n",
        "    self.Input_layer = [self.idcnt+i for i in range(input_size)]\n",
        "    self.neurons.update([self.idcnt+i for i in range(input_size)])\n",
        "    self.idcnt+=input_size\n",
        "\n",
        "    self.Output_layer = [self.idcnt+o for o in range(output_size)]\n",
        "    self.neurons.update([self.idcnt+o for o in range(output_size)])\n",
        "    self.idcnt+=output_size\n",
        "\n",
        "    for i in self.Input_layer: self.next[i], self.prev[i] = self.Output_layer, []\n",
        "    for o in self.Output_layer: self.prev[o], self.next[o] = self.Input_layer, []\n",
        "\n",
        "    self.weight = {}\n",
        "    self.weight_gsum, self.weight_gsqr = {}, {}\n",
        "    for u in self.Input_layer:\n",
        "      for v in self.Output_layer:\n",
        "        self.weight[(u,v)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_gsum[(u,v)], self.weight_gsqr[(u,v)] = 0, 0\n",
        "\n",
        "    self.bias = {}\n",
        "    self.bias_gsum, self.bias_gsqr = {}, {}\n",
        "    for i in self.neurons:\n",
        "      self.bias[i] = np.random.uniform(-0.1, 0.1)\n",
        "      self.bias_gsum[i], self.bias_gsqr[i] = 0, 0\n",
        "\n",
        "  def __forward(self, X, batch_size):\n",
        "    assert X.shape == (batch_size,len(self.Input_layer)), \\\n",
        "      f\"X.shape={X.shape}, where {(batch_size,len(self.Input_layer))} is expected\"\n",
        "    a = {q: np.zeros(batch_size) for q in self.neurons}\n",
        "\n",
        "    for i, n in enumerate(self.Input_layer):\n",
        "      a[n] = X[:, i].copy()\n",
        "\n",
        "    q = deque()\n",
        "    for i in self.Input_layer:\n",
        "      q.append(i)\n",
        "\n",
        "    cnt = {q: 0 for q in self.neurons}\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      a[c] = np.tanh(a[c] + self.bias[c])\n",
        "      for n in self.next[c]:\n",
        "        a[n] = a[n] + a[c] * self.weight[(c,n)]\n",
        "        cnt[n] += 1\n",
        "        if cnt[n] == len(self.prev[n]):\n",
        "          q.append(n)\n",
        "    return a\n",
        "\n",
        "  def evaluate(self, X):\n",
        "    a = self.__forward(X, len(X))\n",
        "    return np.array([a[o] for o in self.Output_layer]).T\n",
        "\n",
        "  def __backward(self, X, Y, batch_size, learning_rate, dsum, dsqr):\n",
        "    assert X.shape == (batch_size,len(self.Input_layer)), \\\n",
        "      f\"X.shape={X.shape}, where {(batch_size,len(self.Input_layer))} is expected\"\n",
        "    assert Y.shape == (batch_size,len(self.Output_layer)), \\\n",
        "      f\"X.shape={Y.shape}, where {(batch_size,len(self.Output_layer))} is expected\"\n",
        "    a = self.__forward(X, batch_size)\n",
        "\n",
        "    delta_b, delta_w = {}, {}\n",
        "\n",
        "    par_a = {q: np.zeros(batch_size) for q in self.neurons}\n",
        "    for o, n in enumerate(self.Output_layer):\n",
        "      par_a[n] = 2 * (a[n] - Y[:, o])\n",
        "\n",
        "    q = deque()\n",
        "    for o in self.Output_layer:\n",
        "      q.append(o)\n",
        "\n",
        "    cnt = {q: 0 for q in self.neurons}\n",
        "\n",
        "    while len(q) != 0:\n",
        "      c = q.popleft()\n",
        "      par_b = par_a[c] * (1-a[c]**2)\n",
        "\n",
        "      gbias = par_b\n",
        "      self.bias_gsum[c] = (1-dsum)*np.sum(gbias)/batch_size + dsum*self.bias_gsum[c]\n",
        "      self.bias_gsqr[c] = (1-dsqr)*np.sum(gbias**2)/batch_size + dsqr*self.bias_gsqr[c]\n",
        "      delta_b[c] = -learning_rate * self.bias_gsum[c] / (self.bias_gsqr[c]**(1/2)+1)\n",
        "\n",
        "      for p in self.prev[c]:\n",
        "        par_a[p] += par_a[c] * (1-a[c]**2) * self.weight[(p,c)]\n",
        "        gweight = par_a[c] * (1-a[c]**2) * a[p]\n",
        "        self.weight_gsum[(p,c)] = \\\n",
        "         (1-dsum)*np.sum(gweight)/batch_size + dsum*self.weight_gsum[(p,c)]\n",
        "        self.weight_gsqr[(p,c)] = \\\n",
        "         (1-dsqr)*np.sum(gweight**2)/batch_size + dsqr*self.weight_gsqr[(p,c)]\n",
        "        delta_w[(p,c)] = \\\n",
        "         -learning_rate * self.weight_gsum[(p,c)] / (self.weight_gsqr[(p,c)]**(1/2)+1)\n",
        "\n",
        "        cnt[p] += 1\n",
        "        if cnt[p] == len(self.next[p]):\n",
        "          q.append(p)\n",
        "\n",
        "    return delta_w, delta_b\n",
        "\n",
        "  def update(self, X, Y, batch_size, learning_rate, dsum=0.9, dsqr=0.9):\n",
        "    delta_w, delta_b = \\\n",
        "     self.__backward(X, Y, batch_size, learning_rate, dsum, dsqr)\n",
        "    for w in self.weight:\n",
        "        self.weight[w] += delta_w[w]\n",
        "    for p in self.neurons:\n",
        "        self.bias[p] += delta_b[p]\n",
        "\n",
        "  def addLayer(self, mid_size, UP, DOWN):\n",
        "    Mid_layer = [self.idcnt+m for m in range(mid_size)]\n",
        "    self.neurons.update([self.idcnt+m for m in range(mid_size)])\n",
        "    self.idcnt+=mid_size\n",
        "\n",
        "    for m in Mid_layer:\n",
        "      self.bias[m] = np.random.uniform(-0.1, 0.1)\n",
        "      self.bias_gsum[m], self.bias_gsqr[m] = 0, 0\n",
        "\n",
        "      self.prev[m] = UP\n",
        "      for u in UP:\n",
        "        self.weight[(u,m)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_gsum[(u,m)], self.weight_gsqr[(u,m)] = 0, 0\n",
        "\n",
        "      self.next[m] = DOWN\n",
        "      for v in DOWN:\n",
        "        self.weight[(m,v)] = np.random.uniform(-0.1, 0.1)\n",
        "        self.weight_gsum[(m,v)], self.weight_gsqr[(m,v)] = 0, 0\n",
        "\n",
        "    for u in UP:\n",
        "      self.next[u] = Mid_layer\n",
        "    for v in DOWN:\n",
        "      self.prev[v] = Mid_layer\n",
        "\n",
        "    for u in UP:\n",
        "      for v in DOWN:\n",
        "        self.weight.pop((u,v))\n",
        "        self.weight_gsum.pop((u,v))\n",
        "        self.weight_gsqr.pop((u,v))\n",
        "    return Mid_layer\n",
        "\n",
        "  def train(self, x, y, batch_size, epochs, learning_rate):\n",
        "    assert len(x) == len(y)\n",
        "    l = len(x)\n",
        "    for epoch in range(epochs):\n",
        "      X, Y = x.copy(), y.copy()\n",
        "      data=[(X[_], Y[_]) for _ in range(l)]\n",
        "      random.shuffle(data)\n",
        "      for _ in range(l):\n",
        "        X[_],Y[_]=data[_]\n",
        "      loss = 0\n",
        "      for batch in range(int(l / batch_size)):\n",
        "        L, R = batch * batch_size, (batch + 1) * batch_size\n",
        "        x_train, y_train = X[L:R], Y[L:R]\n",
        "        self.update(x_train, y_train, batch_size, learning_rate)\n",
        "        output = self.evaluate(x_train)\n",
        "        loss += np.sum(((y_train-output) ** 2), axis=(0,1))\n",
        "      loss = ((loss) ** 0.5) / (int(l / batch_size) * batch_size)\n",
        "      print(f\"Epoch {epoch}/{epochs}, Loss:{loss}\")\n",
        "\n",
        "  def parallel_train(self, x, y, proc, batch_size, epochs=10, learning_rate=0.1):\n",
        "    assert len(x) == len(y)\n",
        "    l = len(x)\n",
        "    for epoch in range(epochs):\n",
        "      X, Y = x.copy(), y.copy()\n",
        "      data=[(X[_], Y[_]) for _ in range(l)]\n",
        "      random.shuffle(data)\n",
        "      for _ in range(l):\n",
        "        X[_],Y[_]=data[_]\n",
        "\n",
        "      k = int(l / batch_size)\n",
        "      def train_proc(mod, X_split, Y_split):\n",
        "        tdelta_w = {w: 0 for w in mod.weight}\n",
        "        tdelta_b = {q: 0 for q in mod.neurons}\n",
        "        nonlocal proc, k, batch_size, learning_rate\n",
        "        for c in range(int(k / proc)):\n",
        "          delta_w, delta_b = mod.__backward(\n",
        "            X_split[c*batch_size:(c+1)*batch_size],\n",
        "            Y_split[c*batch_size:(c+1)*batch_size],\n",
        "            batch_size, learning_rate, 0.9, 0.9\n",
        "          )\n",
        "          for w in self.weight:\n",
        "            self.weight[w] += delta_w[w]\n",
        "            tdelta_w[w] += delta_w[w]\n",
        "          for p in self.neurons:\n",
        "            self.bias[p] += delta_b[p]\n",
        "            tdelta_b[p] += delta_b[p]\n",
        "        return tdelta_w, tdelta_b\n",
        "\n",
        "      with ThreadPoolExecutor(max_workers=proc) as executor:\n",
        "        handles = [\n",
        "          executor.submit(train_proc, self,\n",
        "            X[b*int(k/proc)*batch_size:(b+1)*int(k/proc)*batch_size],\n",
        "            Y[b*int(k/proc)*batch_size:(b+1)*int(k/proc)*batch_size])\n",
        "            for b in range(proc)\n",
        "          ]\n",
        "\n",
        "      results = [f.result() for f in handles]\n",
        "\n",
        "      delta_w = {w: np.mean([res[0][w] for res in results], axis=0) for w in self.weight}\n",
        "      delta_b = {n: np.mean([res[1][n] for res in results], axis=0) for n in self.neurons}\n",
        "      for w in self.weight:\n",
        "        self.weight[w] += delta_w[w]\n",
        "      for p in self.neurons:\n",
        "        self.bias[p] += delta_b[p]\n",
        "      output = self.evaluate(X)\n",
        "      loss = (np.sum(((Y-output)**2), axis=(0,1)) ** 0.5)\\\n",
        "       / (int(l / batch_size) * batch_size)\n",
        "      print(f\"Epoch {epoch}/{epochs}, Loss:{loss}\")"
      ],
      "metadata": {
        "id": "YUliU5Cx-oyU"
      },
      "id": "YUliU5Cx-oyU",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "Y=np.array([[0],[1],[1],[0]])\n",
        "mod=Model(2, 1)\n",
        "mid1=mod.addLayer(4, mod.Input_layer, mod.Output_layer)\n",
        "mod.addLayer(4, mid1, mod.Output_layer)\n",
        "mod.evaluate(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfiQiE5l8XOr",
        "outputId": "67355f60-69a1-43a4-f8ea-c5bb3acf6773"
      },
      "id": "KfiQiE5l8XOr",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.08765865],\n",
              "       [0.08768841],\n",
              "       [0.08731752],\n",
              "       [0.08734712]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod.parallel_train(X, Y, 1, 4, 500, 0.1)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC2n_sC_8xnP",
        "outputId": "b4aff01c-fee8-4407-8250-bf6415afa518"
      },
      "id": "aC2n_sC_8xnP",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/500, Loss:0.004844040913498024\n",
            "Epoch 1/500, Loss:0.004839749161304804\n",
            "Epoch 2/500, Loss:0.00030484857268106905\n",
            "Epoch 3/500, Loss:0.007898054370048363\n",
            "Epoch 4/500, Loss:0.006889159446078488\n",
            "Epoch 5/500, Loss:0.005630083985322584\n",
            "Epoch 6/500, Loss:0.003989998909459919\n",
            "Epoch 7/500, Loss:0.005625429936492033\n",
            "Epoch 8/500, Loss:0.006811195091298022\n",
            "Epoch 9/500, Loss:0.006805980405899401\n",
            "Epoch 10/500, Loss:0.00624102031133002\n",
            "Epoch 11/500, Loss:0.005608761583371809\n",
            "Epoch 12/500, Loss:0.00678880361414753\n",
            "Epoch 13/500, Loss:0.007850590690224332\n",
            "Epoch 14/500, Loss:0.0055911932090442484\n",
            "Epoch 15/500, Loss:0.007834162866084437\n",
            "Epoch 16/500, Loss:0.008275842114177897\n",
            "Epoch 17/500, Loss:0.003940838834759267\n",
            "Epoch 18/500, Loss:0.0010466885238167441\n",
            "Epoch 19/500, Loss:0.007804279491340792\n",
            "Epoch 20/500, Loss:0.006810893182419987\n",
            "Epoch 21/500, Loss:0.0039292455905821705\n",
            "Epoch 22/500, Loss:0.007307995952615289\n",
            "Epoch 23/500, Loss:0.0055464511233130065\n",
            "Epoch 24/500, Loss:0.0072933710182794325\n",
            "Epoch 25/500, Loss:0.005535002789998847\n",
            "Epoch 26/500, Loss:0.005529215037563459\n",
            "Epoch 27/500, Loss:0.007741881381366625\n",
            "Epoch 28/500, Loss:0.005785658880634585\n",
            "Epoch 29/500, Loss:0.0041987462898033494\n",
            "Epoch 30/500, Loss:0.0041060766980293905\n",
            "Epoch 31/500, Loss:0.008205718777919861\n",
            "Epoch 32/500, Loss:0.00477043968473461\n",
            "Epoch 33/500, Loss:0.008212644541882044\n",
            "Epoch 34/500, Loss:0.007765633783986217\n",
            "Epoch 35/500, Loss:0.003966837562733821\n",
            "Epoch 36/500, Loss:0.007764238574827809\n",
            "Epoch 37/500, Loss:0.004763109731840825\n",
            "Epoch 38/500, Loss:0.005542440584308633\n",
            "Epoch 39/500, Loss:0.007756152686463202\n",
            "Epoch 40/500, Loss:0.0047359428467758655\n",
            "Epoch 41/500, Loss:0.0077467900053118655\n",
            "Epoch 42/500, Loss:0.007269304250927716\n",
            "Epoch 43/500, Loss:0.0004299285918402788\n",
            "Epoch 44/500, Loss:0.0039053829939479493\n",
            "Epoch 45/500, Loss:0.005534187000384414\n",
            "Epoch 46/500, Loss:0.007257466053501371\n",
            "Epoch 47/500, Loss:0.00551337512583634\n",
            "Epoch 48/500, Loss:0.00018950391700419626\n",
            "Epoch 49/500, Loss:0.008155707358595665\n",
            "Epoch 50/500, Loss:0.004705590054308018\n",
            "Epoch 51/500, Loss:0.0072321945734733885\n",
            "Epoch 52/500, Loss:0.004696461020029903\n",
            "Epoch 53/500, Loss:0.0054872131703776385\n",
            "Epoch 54/500, Loss:0.007679016238537301\n",
            "Epoch 55/500, Loss:0.007204867789033191\n",
            "Epoch 56/500, Loss:0.008099041073173344\n",
            "Epoch 57/500, Loss:0.006151917433565088\n",
            "Epoch 58/500, Loss:0.007647834618442398\n",
            "Epoch 59/500, Loss:0.006596512840850507\n",
            "Epoch 60/500, Loss:0.004663968911952545\n",
            "Epoch 61/500, Loss:0.00558853804062973\n",
            "Epoch 62/500, Loss:0.003860891046199995\n",
            "Epoch 63/500, Loss:0.0002714382949679717\n",
            "Epoch 64/500, Loss:0.008050366507419665\n",
            "Epoch 65/500, Loss:0.0071506940071285685\n",
            "Epoch 66/500, Loss:0.008035756392682905\n",
            "Epoch 67/500, Loss:3.674947593753653e-05\n",
            "Epoch 68/500, Loss:0.004631338101984577\n",
            "Epoch 69/500, Loss:0.007582793224303373\n",
            "Epoch 70/500, Loss:0.004905933583221857\n",
            "Epoch 71/500, Loss:0.0040908092667048\n",
            "Epoch 72/500, Loss:0.008017075732645753\n",
            "Epoch 73/500, Loss:0.00552346771200796\n",
            "Epoch 74/500, Loss:0.008026466072136824\n",
            "Epoch 75/500, Loss:0.006026187013771363\n",
            "Epoch 76/500, Loss:0.0004601761866876573\n",
            "Epoch 77/500, Loss:0.006039816730639097\n",
            "Epoch 78/500, Loss:0.008030727134022303\n",
            "Epoch 79/500, Loss:0.005439638236267442\n",
            "Epoch 80/500, Loss:0.0003082948277419143\n",
            "Epoch 81/500, Loss:0.005432625366705125\n",
            "Epoch 82/500, Loss:0.007590972251610343\n",
            "Epoch 83/500, Loss:0.005426772130075354\n",
            "Epoch 84/500, Loss:0.007582675041052665\n",
            "Epoch 85/500, Loss:0.005421642531721939\n",
            "Epoch 86/500, Loss:0.007118739619222603\n",
            "Epoch 87/500, Loss:0.006019359592796747\n",
            "Epoch 88/500, Loss:0.007109177473541766\n",
            "Epoch 89/500, Loss:0.0066203144516427245\n",
            "Epoch 90/500, Loss:0.00598314985139365\n",
            "Epoch 91/500, Loss:0.005396635373634653\n",
            "Epoch 92/500, Loss:0.005390850061857807\n",
            "Epoch 93/500, Loss:0.0075255150124211275\n",
            "Epoch 94/500, Loss:0.005378790946025912\n",
            "Epoch 95/500, Loss:0.007508645702288576\n",
            "Epoch 96/500, Loss:6.052003889124738e-05\n",
            "Epoch 97/500, Loss:0.006462839867823645\n",
            "Epoch 98/500, Loss:0.004741436978444074\n",
            "Epoch 99/500, Loss:0.005481561300493398\n",
            "Epoch 100/500, Loss:0.0079113725437081\n",
            "Epoch 101/500, Loss:0.006563959395050091\n",
            "Epoch 102/500, Loss:0.006462157041370426\n",
            "Epoch 103/500, Loss:0.005362810204833644\n",
            "Epoch 104/500, Loss:0.00789536600874053\n",
            "Epoch 105/500, Loss:0.005923961591249722\n",
            "Epoch 106/500, Loss:0.006541383220365821\n",
            "Epoch 107/500, Loss:0.00653471279911945\n",
            "Epoch 108/500, Loss:0.005331449819592166\n",
            "Epoch 109/500, Loss:0.0065212077443411965\n",
            "Epoch 110/500, Loss:0.005524769800060414\n",
            "Epoch 111/500, Loss:0.006989069700565773\n",
            "Epoch 112/500, Loss:0.003995023338067182\n",
            "Epoch 113/500, Loss:0.0064124907958316895\n",
            "Epoch 114/500, Loss:0.0001724413770761839\n",
            "Epoch 115/500, Loss:0.0037644430258215247\n",
            "Epoch 116/500, Loss:0.005320290521231228\n",
            "Epoch 117/500, Loss:0.007430493608170644\n",
            "Epoch 118/500, Loss:0.005315837617812955\n",
            "Epoch 119/500, Loss:0.005313181732294261\n",
            "Epoch 120/500, Loss:0.006976602341228427\n",
            "Epoch 121/500, Loss:0.00783180451241351\n",
            "Epoch 122/500, Loss:0.006493077483596808\n",
            "Epoch 123/500, Loss:0.007400684789444378\n",
            "Epoch 124/500, Loss:0.0012686413946964162\n",
            "Epoch 125/500, Loss:0.005861082536828074\n",
            "Epoch 126/500, Loss:0.005408791420759465\n",
            "Epoch 127/500, Loss:0.007396283562438839\n",
            "Epoch 128/500, Loss:0.006481972858742023\n",
            "Epoch 129/500, Loss:0.003838818598832951\n",
            "Epoch 130/500, Loss:0.0007313113853045011\n",
            "Epoch 131/500, Loss:0.007404146493226247\n",
            "Epoch 132/500, Loss:0.0005351385177467684\n",
            "Epoch 133/500, Loss:0.00740893101132132\n",
            "Epoch 134/500, Loss:0.006493416537000291\n",
            "Epoch 135/500, Loss:0.0074068699066847385\n",
            "Epoch 136/500, Loss:0.005300208044753345\n",
            "Epoch 137/500, Loss:0.007401076709462843\n",
            "Epoch 138/500, Loss:0.006954378793624986\n",
            "Epoch 139/500, Loss:0.0037430043128049385\n",
            "Epoch 140/500, Loss:0.007386461177403706\n",
            "Epoch 141/500, Loss:0.005281675467882476\n",
            "Epoch 142/500, Loss:0.005277873570762287\n",
            "Epoch 143/500, Loss:0.006356691594644948\n",
            "Epoch 144/500, Loss:0.007362659097546742\n",
            "Epoch 145/500, Loss:0.005265467769988832\n",
            "Epoch 146/500, Loss:0.007347956460027627\n",
            "Epoch 147/500, Loss:0.005311112338783482\n",
            "Epoch 148/500, Loss:0.005304308483984946\n",
            "Epoch 149/500, Loss:0.007747096824539208\n",
            "Epoch 150/500, Loss:0.007328419761710736\n",
            "Epoch 151/500, Loss:0.006419230958620809\n",
            "Epoch 152/500, Loss:0.005237357051119816\n",
            "Epoch 153/500, Loss:0.006873707121139973\n",
            "Epoch 154/500, Loss:0.006298346511040588\n",
            "Epoch 155/500, Loss:0.005223476082735403\n",
            "Epoch 156/500, Loss:0.005327419748916745\n",
            "Epoch 157/500, Loss:0.007696550455158725\n",
            "Epoch 158/500, Loss:0.0010386263527832464\n",
            "Epoch 159/500, Loss:0.0045323997665929865\n",
            "Epoch 160/500, Loss:0.005225389154240847\n",
            "Epoch 161/500, Loss:0.004457763707296484\n",
            "Epoch 162/500, Loss:0.005223074657214129\n",
            "Epoch 163/500, Loss:0.00769639790247793\n",
            "Epoch 164/500, Loss:0.005216635990359312\n",
            "Epoch 165/500, Loss:0.005242919681260239\n",
            "Epoch 166/500, Loss:0.004440979725892878\n",
            "Epoch 167/500, Loss:5.640391489147361e-05\n",
            "Epoch 168/500, Loss:0.00638056787133841\n",
            "Epoch 169/500, Loss:0.006840368586113954\n",
            "Epoch 170/500, Loss:0.006374475924890373\n",
            "Epoch 171/500, Loss:0.006832419270129617\n",
            "Epoch 172/500, Loss:0.005198965231712539\n",
            "Epoch 173/500, Loss:0.005195226915077905\n",
            "Epoch 174/500, Loss:0.006817143271149189\n",
            "Epoch 175/500, Loss:0.006810813181221196\n",
            "Epoch 176/500, Loss:0.005261304379224797\n",
            "Epoch 177/500, Loss:0.006802333254916622\n",
            "Epoch 178/500, Loss:0.0051745540585288186\n",
            "Epoch 179/500, Loss:0.005170912200961183\n",
            "Epoch 180/500, Loss:0.007217744690673623\n",
            "Epoch 181/500, Loss:0.006322270859811205\n",
            "Epoch 182/500, Loss:0.0010730568581478646\n",
            "Epoch 183/500, Loss:0.006777122511287009\n",
            "Epoch 184/500, Loss:0.0002691728585917219\n",
            "Epoch 185/500, Loss:0.00677092028799031\n",
            "Epoch 186/500, Loss:0.006308026029924854\n",
            "Epoch 187/500, Loss:0.007190625334940103\n",
            "Epoch 188/500, Loss:0.006297739832619511\n",
            "Epoch 189/500, Loss:0.006291925744681074\n",
            "Epoch 190/500, Loss:0.00456266689261766\n",
            "Epoch 191/500, Loss:0.003810283542135162\n",
            "Epoch 192/500, Loss:0.0002840430063919739\n",
            "Epoch 193/500, Loss:0.006293735024965692\n",
            "Epoch 194/500, Loss:0.0071808744972638805\n",
            "Epoch 195/500, Loss:0.005195564731532882\n",
            "Epoch 196/500, Loss:0.004387405726559569\n",
            "Epoch 197/500, Loss:0.006752533769403634\n",
            "Epoch 198/500, Loss:0.005165894251470649\n",
            "Epoch 199/500, Loss:0.007588275880843288\n",
            "Epoch 200/500, Loss:0.0043835457719171205\n",
            "Epoch 201/500, Loss:0.00014641760570084455\n",
            "Epoch 202/500, Loss:0.005687552453998046\n",
            "Epoch 203/500, Loss:0.005147384547370757\n",
            "Epoch 204/500, Loss:0.0071737422868146175\n",
            "Epoch 205/500, Loss:0.006185604419942702\n",
            "Epoch 206/500, Loss:0.005127269933795004\n",
            "Epoch 207/500, Loss:0.00617894823438929\n",
            "Epoch 208/500, Loss:0.007157552213281176\n",
            "Epoch 209/500, Loss:0.007151396545961382\n",
            "Epoch 210/500, Loss:0.006163017794927753\n",
            "Epoch 211/500, Loss:0.005153787353828993\n",
            "Epoch 212/500, Loss:0.007536412850750697\n",
            "Epoch 213/500, Loss:0.005150408556977526\n",
            "Epoch 214/500, Loss:0.005101106591108807\n",
            "Epoch 215/500, Loss:0.004391938305727491\n",
            "Epoch 216/500, Loss:0.007123112668610656\n",
            "Epoch 217/500, Loss:0.0043792222022300065\n",
            "Epoch 218/500, Loss:0.006243031669738427\n",
            "Epoch 219/500, Loss:0.007117907898307055\n",
            "Epoch 220/500, Loss:0.005097996716177917\n",
            "Epoch 221/500, Loss:0.004344691633129313\n",
            "Epoch 222/500, Loss:0.006681805271525863\n",
            "Epoch 223/500, Loss:0.006676471509260306\n",
            "Epoch 224/500, Loss:0.004327655871831965\n",
            "Epoch 225/500, Loss:0.0036580354850828203\n",
            "Epoch 226/500, Loss:0.006664767402402733\n",
            "Epoch 227/500, Loss:0.0036539340391393248\n",
            "Epoch 228/500, Loss:0.007480333906224962\n",
            "Epoch 229/500, Loss:0.0035845321465841603\n",
            "Epoch 230/500, Loss:0.005608517493897264\n",
            "Epoch 231/500, Loss:0.007071885559714272\n",
            "Epoch 232/500, Loss:0.005060960933924727\n",
            "Epoch 233/500, Loss:0.005597615214153206\n",
            "Epoch 234/500, Loss:0.005654618093624237\n",
            "Epoch 235/500, Loss:0.005592983441820281\n",
            "Epoch 236/500, Loss:0.007052156982670589\n",
            "Epoch 237/500, Loss:0.005048006252643238\n",
            "Epoch 238/500, Loss:0.007042414183844598\n",
            "Epoch 239/500, Loss:0.004381880410537977\n",
            "Epoch 240/500, Loss:0.005042053506336259\n",
            "Epoch 241/500, Loss:0.005576712357893402\n",
            "Epoch 242/500, Loss:0.007029853391118575\n",
            "Epoch 243/500, Loss:0.004285866000579146\n",
            "Epoch 244/500, Loss:0.005110575862147859\n",
            "Epoch 245/500, Loss:0.0036466902597843973\n",
            "Epoch 246/500, Loss:0.0002548297437886715\n",
            "Epoch 247/500, Loss:0.003564205407394513\n",
            "Epoch 248/500, Loss:0.0061642453960565995\n",
            "Epoch 249/500, Loss:0.00503278026659968\n",
            "Epoch 250/500, Loss:0.00359307882197881\n",
            "Epoch 251/500, Loss:0.00503201917565279\n",
            "Epoch 252/500, Loss:0.005031028148809241\n",
            "Epoch 253/500, Loss:0.0050297553006202625\n",
            "Epoch 254/500, Loss:0.004280591059930562\n",
            "Epoch 255/500, Loss:0.005026804055719633\n",
            "Epoch 256/500, Loss:0.005560953018116499\n",
            "Epoch 257/500, Loss:0.005022631085933478\n",
            "Epoch 258/500, Loss:0.0065923748730016335\n",
            "Epoch 259/500, Loss:0.007004063896300496\n",
            "Epoch 260/500, Loss:0.0035452023320281644\n",
            "Epoch 261/500, Loss:0.003595567276667342\n",
            "Epoch 262/500, Loss:0.0005466778824683152\n",
            "Epoch 263/500, Loss:0.0069945668724565166\n",
            "Epoch 264/500, Loss:0.0069919085161471595\n",
            "Epoch 265/500, Loss:0.005027232668950178\n",
            "Epoch 266/500, Loss:0.006573120904545913\n",
            "Epoch 267/500, Loss:0.005020058455021361\n",
            "Epoch 268/500, Loss:0.0050157138104743905\n",
            "Epoch 269/500, Loss:0.006985889315284986\n",
            "Epoch 270/500, Loss:0.006983068561297064\n",
            "Epoch 271/500, Loss:0.0042743794221302145\n",
            "Epoch 272/500, Loss:0.007367988090478777\n",
            "Epoch 273/500, Loss:0.0035428976674535917\n",
            "Epoch 274/500, Loss:0.00041668934683328516\n",
            "Epoch 275/500, Loss:0.0035463542586991804\n",
            "Epoch 276/500, Loss:0.004998645431441459\n",
            "Epoch 277/500, Loss:0.0042558991923258935\n",
            "Epoch 278/500, Loss:0.004988477676987284\n",
            "Epoch 279/500, Loss:0.005523986444319876\n",
            "Epoch 280/500, Loss:0.006962972798789209\n",
            "Epoch 281/500, Loss:0.006959980117678513\n",
            "Epoch 282/500, Loss:0.006101883226531322\n",
            "Epoch 283/500, Loss:0.004979767855215466\n",
            "Epoch 284/500, Loss:9.183189490574049e-05\n",
            "Epoch 285/500, Loss:0.007333564930421852\n",
            "Epoch 286/500, Loss:0.003515203979239456\n",
            "Epoch 287/500, Loss:0.004228512177028739\n",
            "Epoch 288/500, Loss:0.0065186698961777825\n",
            "Epoch 289/500, Loss:5.845610361887695e-06\n",
            "Epoch 290/500, Loss:0.004957531024201794\n",
            "Epoch 291/500, Loss:0.004954163070844495\n",
            "Epoch 292/500, Loss:0.003563834272654629\n",
            "Epoch 293/500, Loss:0.006499845321529847\n",
            "Epoch 294/500, Loss:0.00013877189442391334\n",
            "Epoch 295/500, Loss:0.007288805933642516\n",
            "Epoch 296/500, Loss:0.00648861013379735\n",
            "Epoch 297/500, Loss:0.004938697790037479\n",
            "Epoch 298/500, Loss:0.006886748689204105\n",
            "Epoch 299/500, Loss:0.004195854231588337\n",
            "Epoch 300/500, Loss:0.006033039150089574\n",
            "Epoch 301/500, Loss:4.568406086335517e-05\n",
            "Epoch 302/500, Loss:0.00368220258725657\n",
            "Epoch 303/500, Loss:0.007249652676895569\n",
            "Epoch 304/500, Loss:0.006862284681341399\n",
            "Epoch 305/500, Loss:0.005556003434848349\n",
            "Epoch 306/500, Loss:0.004918531541330786\n",
            "Epoch 307/500, Loss:0.006021088958066637\n",
            "Epoch 308/500, Loss:0.00645231788549112\n",
            "Epoch 309/500, Loss:0.0002435791683024776\n",
            "Epoch 310/500, Loss:0.004912281660042382\n",
            "Epoch 311/500, Loss:0.003600965130857375\n",
            "Epoch 312/500, Loss:0.003566837585651806\n",
            "Epoch 313/500, Loss:0.0064524290949320605\n",
            "Epoch 314/500, Loss:0.006859033227861944\n",
            "Epoch 315/500, Loss:0.006451958423548285\n",
            "Epoch 316/500, Loss:0.004210411251715796\n",
            "Epoch 317/500, Loss:0.00021634119318008737\n",
            "Epoch 318/500, Loss:0.004914810463031522\n",
            "Epoch 319/500, Loss:0.006857053469424886\n",
            "Epoch 320/500, Loss:0.00644835044611102\n",
            "Epoch 321/500, Loss:0.007234403761122507\n",
            "Epoch 322/500, Loss:0.0068468063571697465\n",
            "Epoch 323/500, Loss:0.006004294162799675\n",
            "Epoch 324/500, Loss:0.004899349849028856\n",
            "Epoch 325/500, Loss:0.000541919227627762\n",
            "Epoch 326/500, Loss:0.00683080549029573\n",
            "Epoch 327/500, Loss:0.006422320879496122\n",
            "Epoch 328/500, Loss:0.0048885189149442745\n",
            "Epoch 329/500, Loss:0.004885223354021954\n",
            "Epoch 330/500, Loss:0.004928959199421561\n",
            "Epoch 331/500, Loss:0.0034521672937246023\n",
            "Epoch 332/500, Loss:0.006808482525953096\n",
            "Epoch 333/500, Loss:0.003449610554866536\n",
            "Epoch 334/500, Loss:0.00596885117612922\n",
            "Epoch 335/500, Loss:0.006796275788395472\n",
            "Epoch 336/500, Loss:0.00538399003007908\n",
            "Epoch 337/500, Loss:0.0067854057518966495\n",
            "Epoch 338/500, Loss:0.0067789977592906375\n",
            "Epoch 339/500, Loss:0.0067720403786911895\n",
            "Epoch 340/500, Loss:0.005486100947322154\n",
            "Epoch 341/500, Loss:0.006764500126264725\n",
            "Epoch 342/500, Loss:0.0010949402033099446\n",
            "Epoch 343/500, Loss:0.004850676579947745\n",
            "Epoch 344/500, Loss:0.005936746718255049\n",
            "Epoch 345/500, Loss:0.005829804345784862\n",
            "Epoch 346/500, Loss:0.004846091528838392\n",
            "Epoch 347/500, Loss:0.00535572674728136\n",
            "Epoch 348/500, Loss:0.006749593361235256\n",
            "Epoch 349/500, Loss:0.00355970039616259\n",
            "Epoch 350/500, Loss:0.003527390337293269\n",
            "Epoch 351/500, Loss:0.005354142823729957\n",
            "Epoch 352/500, Loss:0.006751911736097223\n",
            "Epoch 353/500, Loss:0.004873766017149462\n",
            "Epoch 354/500, Loss:0.0067535032991351465\n",
            "Epoch 355/500, Loss:0.005355060626428443\n",
            "Epoch 356/500, Loss:0.0067507853943868865\n",
            "Epoch 357/500, Loss:0.006349505218750771\n",
            "Epoch 358/500, Loss:0.004130497668661965\n",
            "Epoch 359/500, Loss:0.00483539312339748\n",
            "Epoch 360/500, Loss:0.00534479342015192\n",
            "Epoch 361/500, Loss:0.006737785444086603\n",
            "Epoch 362/500, Loss:0.005353584148854556\n",
            "Epoch 363/500, Loss:0.006731489511619526\n",
            "Epoch 364/500, Loss:0.007101511958645121\n",
            "Epoch 365/500, Loss:0.006721660062631706\n",
            "Epoch 366/500, Loss:0.0034457236914938933\n",
            "Epoch 367/500, Loss:0.00671390984677433\n",
            "Epoch 368/500, Loss:0.004841908191138582\n",
            "Epoch 369/500, Loss:0.004835357258868799\n",
            "Epoch 370/500, Loss:0.004827900391632847\n",
            "Epoch 371/500, Loss:0.003418187448448631\n",
            "Epoch 372/500, Loss:0.004817519101239305\n",
            "Epoch 373/500, Loss:0.005788038563068821\n",
            "Epoch 374/500, Loss:0.006711053668119247\n",
            "Epoch 375/500, Loss:0.006708758629421584\n",
            "Epoch 376/500, Loss:0.0067053942587569386\n",
            "Epoch 377/500, Loss:0.005314597545715163\n",
            "Epoch 378/500, Loss:0.00480579890774977\n",
            "Epoch 379/500, Loss:0.006693272994173919\n",
            "Epoch 380/500, Loss:0.003400043392422989\n",
            "Epoch 381/500, Loss:0.006291716941976681\n",
            "Epoch 382/500, Loss:0.004071650870751529\n",
            "Epoch 383/500, Loss:0.004800454113051208\n",
            "Epoch 384/500, Loss:0.004080907084481768\n",
            "Epoch 385/500, Loss:0.0047853945901532545\n",
            "Epoch 386/500, Loss:0.006666776467371563\n",
            "Epoch 387/500, Loss:0.00479829612584475\n",
            "Epoch 388/500, Loss:0.006269574911832906\n",
            "Epoch 389/500, Loss:0.0047759905836129764\n",
            "Epoch 390/500, Loss:0.0066525688747587535\n",
            "Epoch 391/500, Loss:0.004083335255616775\n",
            "Epoch 392/500, Loss:0.00476876665314952\n",
            "Epoch 393/500, Loss:0.0033707278893953317\n",
            "Epoch 394/500, Loss:0.0004975361342357466\n",
            "Epoch 395/500, Loss:0.004784591175014863\n",
            "Epoch 396/500, Loss:0.004766368310987989\n",
            "Epoch 397/500, Loss:0.006250790206993467\n",
            "Epoch 398/500, Loss:0.004059887133260515\n",
            "Epoch 399/500, Loss:0.00021270946006586065\n",
            "Epoch 400/500, Loss:0.006247789203324742\n",
            "Epoch 401/500, Loss:0.005829951326823917\n",
            "Epoch 402/500, Loss:0.006999451637644686\n",
            "Epoch 403/500, Loss:0.0058236004281563775\n",
            "Epoch 404/500, Loss:0.004036960584567778\n",
            "Epoch 405/500, Loss:0.005248850893933355\n",
            "Epoch 406/500, Loss:0.003356286891908096\n",
            "Epoch 407/500, Loss:3.5241735856627656e-05\n",
            "Epoch 408/500, Loss:0.006608183793372785\n",
            "Epoch 409/500, Loss:0.004024696779477025\n",
            "Epoch 410/500, Loss:0.005799351336765128\n",
            "Epoch 411/500, Loss:0.006594183932444976\n",
            "Epoch 412/500, Loss:0.004728289037512277\n",
            "Epoch 413/500, Loss:0.004724498025447083\n",
            "Epoch 414/500, Loss:0.004008834571264227\n",
            "Epoch 415/500, Loss:0.006936370298321852\n",
            "Epoch 416/500, Loss:0.004170971372428096\n",
            "Epoch 417/500, Loss:0.004713740708917357\n",
            "Epoch 418/500, Loss:0.005312397793279936\n",
            "Epoch 419/500, Loss:0.006184642596741308\n",
            "Epoch 420/500, Loss:0.004787167100662114\n",
            "Epoch 421/500, Loss:0.004762738710225738\n",
            "Epoch 422/500, Loss:0.0065826291759540825\n",
            "Epoch 423/500, Loss:0.006584964664864192\n",
            "Epoch 424/500, Loss:0.006200618109174807\n",
            "Epoch 425/500, Loss:0.0006415480251737354\n",
            "Epoch 426/500, Loss:0.00619500551896619\n",
            "Epoch 427/500, Loss:0.005220122158481307\n",
            "Epoch 428/500, Loss:0.00471905671434379\n",
            "Epoch 429/500, Loss:0.006183883091456483\n",
            "Epoch 430/500, Loss:0.006565291901669797\n",
            "Epoch 431/500, Loss:0.006924441983340708\n",
            "Epoch 432/500, Loss:0.004707870048681054\n",
            "Epoch 433/500, Loss:0.00691544454789304\n",
            "Epoch 434/500, Loss:0.0039987663047399\n",
            "Epoch 435/500, Loss:0.004749724468364006\n",
            "Epoch 436/500, Loss:0.005638145404421023\n",
            "Epoch 437/500, Loss:0.0065362611379070085\n",
            "Epoch 438/500, Loss:0.00468981968842844\n",
            "Epoch 439/500, Loss:0.00522826376002663\n",
            "Epoch 440/500, Loss:0.0046846271833657\n",
            "Epoch 441/500, Loss:0.004683007832599362\n",
            "Epoch 442/500, Loss:0.006138651243685154\n",
            "Epoch 443/500, Loss:0.004026112705140096\n",
            "Epoch 444/500, Loss:0.0046825700085868826\n",
            "Epoch 445/500, Loss:0.006515070594217598\n",
            "Epoch 446/500, Loss:0.0003301808389585004\n",
            "Epoch 447/500, Loss:0.0051646585253895035\n",
            "Epoch 448/500, Loss:0.0065051231057230985\n",
            "Epoch 449/500, Loss:0.004669074117041278\n",
            "Epoch 450/500, Loss:0.005206722344172965\n",
            "Epoch 451/500, Loss:0.004714799612337688\n",
            "Epoch 452/500, Loss:0.004666809047690138\n",
            "Epoch 453/500, Loss:0.006496849121349061\n",
            "Epoch 454/500, Loss:0.003957924928133486\n",
            "Epoch 455/500, Loss:0.004662151370660562\n",
            "Epoch 456/500, Loss:0.004694702020189662\n",
            "Epoch 457/500, Loss:0.004685955673510458\n",
            "Epoch 458/500, Loss:0.004662013281230375\n",
            "Epoch 459/500, Loss:0.005708459364797801\n",
            "Epoch 460/500, Loss:0.0003187247026928921\n",
            "Epoch 461/500, Loss:0.00466133951414028\n",
            "Epoch 462/500, Loss:0.003956132669964095\n",
            "Epoch 463/500, Loss:7.585374095153169e-05\n",
            "Epoch 464/500, Loss:0.005705842694496333\n",
            "Epoch 465/500, Loss:0.004660818842878676\n",
            "Epoch 466/500, Loss:0.005703091629971564\n",
            "Epoch 467/500, Loss:0.005142841202461401\n",
            "Epoch 468/500, Loss:0.00465680008852094\n",
            "Epoch 469/500, Loss:0.005142808630451685\n",
            "Epoch 470/500, Loss:0.006479264154304066\n",
            "Epoch 471/500, Loss:0.0032866178727346\n",
            "Epoch 472/500, Loss:0.003944488862945379\n",
            "Epoch 473/500, Loss:0.004643762438987762\n",
            "Epoch 474/500, Loss:0.00557276225079217\n",
            "Epoch 475/500, Loss:0.003951973414880706\n",
            "Epoch 476/500, Loss:0.006082958018023655\n",
            "Epoch 477/500, Loss:0.004648995052263649\n",
            "Epoch 478/500, Loss:0.006077918071142446\n",
            "Epoch 479/500, Loss:0.003932625007564907\n",
            "Epoch 480/500, Loss:0.005557473271486903\n",
            "Epoch 481/500, Loss:0.0033027614606604538\n",
            "Epoch 482/500, Loss:0.00606709137220844\n",
            "Epoch 483/500, Loss:0.004623651027395995\n",
            "Epoch 484/500, Loss:0.003953607942003797\n",
            "Epoch 485/500, Loss:0.006437105395521204\n",
            "Epoch 486/500, Loss:0.005102686248183397\n",
            "Epoch 487/500, Loss:0.004616477194813459\n",
            "Epoch 488/500, Loss:0.006051152812889987\n",
            "Epoch 489/500, Loss:0.006422853954351754\n",
            "Epoch 490/500, Loss:0.00553006276103904\n",
            "Epoch 491/500, Loss:0.006412927381498093\n",
            "Epoch 492/500, Loss:0.00460006253733591\n",
            "Epoch 493/500, Loss:0.005077224012638547\n",
            "Epoch 494/500, Loss:0.0051676995859163445\n",
            "Epoch 495/500, Loss:0.005152232703007991\n",
            "Epoch 496/500, Loss:0.005627006810218525\n",
            "Epoch 497/500, Loss:0.006024394004179037\n",
            "Epoch 498/500, Loss:0.006395921637099515\n",
            "Epoch 499/500, Loss:0.003324910919195852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X, Y)\n",
        "mod.evaluate(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz2Q6eSC82cI",
        "outputId": "38f9415b-3383-4aab-e79e-46227fb5d035"
      },
      "id": "Iz2Q6eSC82cI",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]] [[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.76240638e-03],\n",
              "       [9.84417306e-01],\n",
              "       [9.87014795e-01],\n",
              "       [5.63063583e-04]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Flatten images to 1D vector of 784 features (28*28)\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "def test(model, X, Y, batch_size):\n",
        "  k = int(len(X)/batch_size)\n",
        "  for i in range(k):\n",
        "    Y_hat=model.evaluate(X[i*batch_size:(i+1)*batch_size])\n",
        "    wrong=0\n",
        "    for j in range(batch_size):\n",
        "      max1,max2,id1,id2=-999,-999,-1,-1\n",
        "      for l in range(10):\n",
        "        if max1 < Y_hat[j][l]:\n",
        "          max1,id1=Y_hat[j][l],l\n",
        "        if max2 < Y[i*batch_size+j][l]:\n",
        "          max2,id2=Y[i*batch_size+j][l],l\n",
        "      if id1 != id2: wrong+=1\n",
        "    print(f\"batch: {i}, accuracy: {(batch_size-wrong)/batch_size*100}%\")\n",
        "mod2 = Model(784, 10)\n",
        "#mod2.addLayer(32, mod2.Input_layer, mod2.Output_layer)"
      ],
      "metadata": {
        "id": "lcOYUuYAjXPG",
        "collapsed": true
      },
      "id": "lcOYUuYAjXPG",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(mod2.Input_layer))\n",
        "mod2.parallel_train(x_train, y_train, 1, 512, 5, 0.01)\n",
        "\"\"\"\n",
        "for i in range(1,9):\n",
        "  plt.subplot(330+i)\n",
        "  plt.imshow(x_test[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
        "print(mod2.evaluate(np.array([x_test[5]])))\n",
        "print(y_test[5])\n",
        "\"\"\"\n",
        "test(mod2, x_test, y_test, 500)"
      ],
      "metadata": {
        "id": "ClmATYl3Qcb7",
        "outputId": "0d886d41-4302-4b0a-cd20-3cc6a3f6dd78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ClmATYl3Qcb7",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n",
            "Epoch 0/5, Loss:0.003301105296252907\n",
            "Epoch 1/5, Loss:0.003130849243302807\n",
            "Epoch 2/5, Loss:0.0031157990038879913\n",
            "Epoch 3/5, Loss:0.0034394485536595166\n",
            "Epoch 4/5, Loss:0.003034747843717671\n",
            "batch: 0, accuracy: 77.0%\n",
            "batch: 1, accuracy: 76.6%\n",
            "batch: 2, accuracy: 70.8%\n",
            "batch: 3, accuracy: 76.4%\n",
            "batch: 4, accuracy: 73.2%\n",
            "batch: 5, accuracy: 75.8%\n",
            "batch: 6, accuracy: 77.8%\n",
            "batch: 7, accuracy: 72.39999999999999%\n",
            "batch: 8, accuracy: 71.8%\n",
            "batch: 9, accuracy: 78.8%\n",
            "batch: 10, accuracy: 89.60000000000001%\n",
            "batch: 11, accuracy: 78.8%\n",
            "batch: 12, accuracy: 88.2%\n",
            "batch: 13, accuracy: 82.6%\n",
            "batch: 14, accuracy: 85.39999999999999%\n",
            "batch: 15, accuracy: 85.8%\n",
            "batch: 16, accuracy: 86.6%\n",
            "batch: 17, accuracy: 93.2%\n",
            "batch: 18, accuracy: 84.8%\n",
            "batch: 19, accuracy: 76.4%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(mod2, x_test, y_test, 5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhe7AJ7MHY5E",
        "outputId": "a6bea7ca-a51c-45ef-ac53-6673cb51bd59"
      },
      "id": "Bhe7AJ7MHY5E",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch: 0, accuracy: 75.06%\n",
            "batch: 1, accuracy: 85.14%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}